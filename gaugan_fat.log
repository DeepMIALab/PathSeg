GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100-SXM-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name  | Type         | Params
---------------------------------------
0 | model | UnetPlusPlus | 128 M 
---------------------------------------
128 M     Trainable params
0         Non-trainable params
128 M     Total params
513.657   Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/truba/home/isahin/miniconda3/envs/gaugan_pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/truba/home/isahin/miniconda3/envs/gaugan_pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('Validation MCC', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/truba/home/isahin/miniconda3/envs/gaugan_pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 128 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/truba/home/isahin/miniconda3/envs/gaugan_pytorch/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=31.0625). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Metric Validation Loss improved. New best score: 0.429
Metric Validation Loss improved by 0.186 >= min_delta = 0.0. New best score: 0.243
Metric Validation Loss improved by 0.073 >= min_delta = 0.0. New best score: 0.171
Metric Validation Loss improved by 0.038 >= min_delta = 0.0. New best score: 0.133
Metric Validation Loss improved by 0.022 >= min_delta = 0.0. New best score: 0.111
Metric Validation Loss improved by 0.004 >= min_delta = 0.0. New best score: 0.107
Metric Validation Loss improved by 0.012 >= min_delta = 0.0. New best score: 0.094
Metric Validation Loss improved by 0.010 >= min_delta = 0.0. New best score: 0.084
Metric Validation Loss improved by 0.004 >= min_delta = 0.0. New best score: 0.080
Monitored metric Validation Loss did not improve in the last 20 records. Best score: 0.080. Signaling Trainer to stop.
