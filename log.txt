Data Distribution For Training Phase
Images 812
Masks 812
Data Distribution For Validation Phase
Images 91
Masks 91
Feature info [{'num_chs': 64, 'reduction': 2, 'module': 'stem'}, {'num_chs': 128, 'reduction': 4, 'module': 'stage1'}, {'num_chs': 256, 'reduction': 8, 'module': 'stage2'}, {'num_chs': 512, 'reduction': 16, 'module': 'stage3'}, {'num_chs': 1024, 'reduction': 32, 'module': 'stage4'}]
Feature info [{'num_chs': 64, 'reduction': 2, 'module': 'stem'}, {'num_chs': 128, 'reduction': 4, 'module': 'stage1'}, {'num_chs': 256, 'reduction': 8, 'module': 'stage2'}, {'num_chs': 512, 'reduction': 16, 'module': 'stage3'}, {'num_chs': 1024, 'reduction': 32, 'module': 'stage4'}]
Model UnetPlusPlus(
  (encoder): HighResolutionNetFeatures(
    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act1): ReLU(inplace=True)
    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (act2): ReLU(inplace=True)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (drop_block): Identity()
        (act2): ReLU(inplace=True)
        (aa): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (cbam): CBAM(
          (channel_attention): Channel_Attention(
            (shared_mlp): Sequential(
              (0): Flatten(start_dim=1, end_dim=-1)
              (1): Linear(in_features=256, out_features=16, bias=True)
              (2): ReLU(inplace=True)
              (3): Linear(in_features=16, out_features=256, bias=True)
            )
          )
          (spatial_attention): Spatial_Attention(
            (compress): ChannelPool()
            (spatial_attention): Sequential(
              (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
              (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (drop_block): Identity()
        (act2): ReLU(inplace=True)
        (aa): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (cbam): CBAM(
          (channel_attention): Channel_Attention(
            (shared_mlp): Sequential(
              (0): Flatten(start_dim=1, end_dim=-1)
              (1): Linear(in_features=256, out_features=16, bias=True)
              (2): ReLU(inplace=True)
              (3): Linear(in_features=16, out_features=256, bias=True)
            )
          )
          (spatial_attention): Spatial_Attention(
            (compress): ChannelPool()
            (spatial_attention): Sequential(
              (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
              (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (drop_block): Identity()
        (act2): ReLU(inplace=True)
        (aa): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (cbam): CBAM(
          (channel_attention): Channel_Attention(
            (shared_mlp): Sequential(
              (0): Flatten(start_dim=1, end_dim=-1)
              (1): Linear(in_features=256, out_features=16, bias=True)
              (2): ReLU(inplace=True)
              (3): Linear(in_features=16, out_features=256, bias=True)
            )
          )
          (spatial_attention): Spatial_Attention(
            (compress): ChannelPool()
            (spatial_attention): Sequential(
              (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
              (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (3): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act1): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (drop_block): Identity()
        (act2): ReLU(inplace=True)
        (aa): Identity()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (act3): ReLU(inplace=True)
        (cbam): CBAM(
          (channel_attention): Channel_Attention(
            (shared_mlp): Sequential(
              (0): Flatten(start_dim=1, end_dim=-1)
              (1): Linear(in_features=256, out_features=16, bias=True)
              (2): ReLU(inplace=True)
              (3): Linear(in_features=16, out_features=256, bias=True)
            )
          )
          (spatial_attention): Spatial_Attention(
            (compress): ChannelPool()
            (spatial_attention): Sequential(
              (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
              (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
            )
          )
        )
      )
    )
    (transition1): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Sequential(
          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
      )
    )
    (stage2): Sequential(
      (0): HighResolutionModule(
        (branches): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
        )
        (fuse_layers): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Identity()
          )
        )
        (fuse_act): ReLU()
      )
    )
    (transition2): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Sequential(
        (0): Sequential(
          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
      )
    )
    (stage3): Sequential(
      (0): HighResolutionModule(
        (branches): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
        )
        (fuse_layers): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
            (2): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=nearest)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): Identity()
          )
        )
        (fuse_act): ReLU()
      )
      (1): HighResolutionModule(
        (branches): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
        )
        (fuse_layers): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
            (2): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=nearest)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): Identity()
          )
        )
        (fuse_act): ReLU()
      )
      (2): HighResolutionModule(
        (branches): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
        )
        (fuse_layers): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
            (2): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=nearest)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): Identity()
          )
        )
        (fuse_act): ReLU()
      )
      (3): HighResolutionModule(
        (branches): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
        )
        (fuse_layers): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
            (2): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=nearest)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): Identity()
          )
        )
        (fuse_act): ReLU()
      )
    )
    (transition3): ModuleList(
      (0): Identity()
      (1): Identity()
      (2): Identity()
      (3): Sequential(
        (0): Sequential(
          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
      )
    )
    (stage4): Sequential(
      (0): HighResolutionModule(
        (branches): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=512, out_features=32, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=32, out_features=512, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=512, out_features=32, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=32, out_features=512, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=512, out_features=32, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=32, out_features=512, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=512, out_features=32, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=32, out_features=512, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
        )
        (fuse_layers): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
            (2): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=nearest)
            )
            (3): Sequential(
              (0): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=nearest)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
            (3): Sequential(
              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=nearest)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (2): Sequential(
                (0): Conv2d(64, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): Sequential(
                (0): Conv2d(128, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (3): Identity()
          )
        )
        (fuse_act): ReLU()
      )
      (1): HighResolutionModule(
        (branches): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=512, out_features=32, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=32, out_features=512, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=512, out_features=32, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=32, out_features=512, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=512, out_features=32, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=32, out_features=512, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=512, out_features=32, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=32, out_features=512, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
        )
        (fuse_layers): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
            (2): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=nearest)
            )
            (3): Sequential(
              (0): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=nearest)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
            (3): Sequential(
              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=nearest)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (2): Sequential(
                (0): Conv2d(64, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): Sequential(
                (0): Conv2d(128, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (3): Identity()
          )
        )
        (fuse_act): ReLU()
      )
      (2): HighResolutionModule(
        (branches): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=64, out_features=4, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=4, out_features=64, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=128, out_features=8, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=8, out_features=128, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=256, out_features=16, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=16, out_features=256, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
          (3): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=512, out_features=32, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=32, out_features=512, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=512, out_features=32, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=32, out_features=512, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (2): BasicBlock(
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=512, out_features=32, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=32, out_features=512, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
            (3): BasicBlock(
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (drop_block): Identity()
              (act1): ReLU(inplace=True)
              (aa): Identity()
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (act2): ReLU(inplace=True)
              (cbam): CBAM(
                (channel_attention): Channel_Attention(
                  (shared_mlp): Sequential(
                    (0): Flatten(start_dim=1, end_dim=-1)
                    (1): Linear(in_features=512, out_features=32, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Linear(in_features=32, out_features=512, bias=True)
                  )
                )
                (spatial_attention): Spatial_Attention(
                  (compress): ChannelPool()
                  (spatial_attention): Sequential(
                    (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                    (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
                  )
                )
              )
            )
          )
        )
        (fuse_layers): ModuleList(
          (0): ModuleList(
            (0): Identity()
            (1): Sequential(
              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
            (2): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=nearest)
            )
            (3): Sequential(
              (0): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=nearest)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Identity()
            (2): Sequential(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
            (3): Sequential(
              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=nearest)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): Sequential(
                (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): Identity()
            (3): Sequential(
              (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (2): Sequential(
                (0): Conv2d(64, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): Sequential(
                (0): Conv2d(128, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (3): Identity()
          )
        )
        (fuse_act): ReLU()
      )
    )
    (incre_modules): ModuleList(
      (0): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): ReLU(inplace=True)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_block): Identity()
          (act2): ReLU(inplace=True)
          (aa): Identity()
          (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act3): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (cbam): CBAM(
            (channel_attention): Channel_Attention(
              (shared_mlp): Sequential(
                (0): Flatten(start_dim=1, end_dim=-1)
                (1): Linear(in_features=128, out_features=8, bias=True)
                (2): ReLU(inplace=True)
                (3): Linear(in_features=8, out_features=128, bias=True)
              )
            )
            (spatial_attention): Spatial_Attention(
              (compress): ChannelPool()
              (spatial_attention): Sequential(
                (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
      )
      (1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_block): Identity()
          (act2): ReLU(inplace=True)
          (aa): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act3): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (cbam): CBAM(
            (channel_attention): Channel_Attention(
              (shared_mlp): Sequential(
                (0): Flatten(start_dim=1, end_dim=-1)
                (1): Linear(in_features=256, out_features=16, bias=True)
                (2): ReLU(inplace=True)
                (3): Linear(in_features=16, out_features=256, bias=True)
              )
            )
            (spatial_attention): Spatial_Attention(
              (compress): ChannelPool()
              (spatial_attention): Sequential(
                (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
      )
      (2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_block): Identity()
          (act2): ReLU(inplace=True)
          (aa): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act3): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (cbam): CBAM(
            (channel_attention): Channel_Attention(
              (shared_mlp): Sequential(
                (0): Flatten(start_dim=1, end_dim=-1)
                (1): Linear(in_features=512, out_features=32, bias=True)
                (2): ReLU(inplace=True)
                (3): Linear(in_features=32, out_features=512, bias=True)
              )
            )
            (spatial_attention): Spatial_Attention(
              (compress): ChannelPool()
              (spatial_attention): Sequential(
                (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
      )
      (3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act1): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (drop_block): Identity()
          (act2): ReLU(inplace=True)
          (aa): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act3): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (cbam): CBAM(
            (channel_attention): Channel_Attention(
              (shared_mlp): Sequential(
                (0): Flatten(start_dim=1, end_dim=-1)
                (1): Linear(in_features=1024, out_features=64, bias=True)
                (2): ReLU(inplace=True)
                (3): Linear(in_features=64, out_features=1024, bias=True)
              )
            )
            (spatial_attention): Spatial_Attention(
              (compress): ChannelPool()
              (spatial_attention): Sequential(
                (0): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
                (1): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
      )
    )
  )
  (decoder): UnetPlusPlusDecoder(
    (center): Identity()
    (blocks): ModuleDict(
      (x_0_0): DecoderBlock(
        (conv1): Conv2dReLU(
          (0): Conv2d(1536, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention1): Attention(
          (attention): Identity()
        )
        (conv2): Conv2dReLU(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention2): Attention(
          (attention): Identity()
        )
      )
      (x_0_1): DecoderBlock(
        (conv1): Conv2dReLU(
          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention1): Attention(
          (attention): Identity()
        )
        (conv2): Conv2dReLU(
          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention2): Attention(
          (attention): Identity()
        )
      )
      (x_1_1): DecoderBlock(
        (conv1): Conv2dReLU(
          (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention1): Attention(
          (attention): Identity()
        )
        (conv2): Conv2dReLU(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention2): Attention(
          (attention): Identity()
        )
      )
      (x_0_2): DecoderBlock(
        (conv1): Conv2dReLU(
          (0): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention1): Attention(
          (attention): Identity()
        )
        (conv2): Conv2dReLU(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention2): Attention(
          (attention): Identity()
        )
      )
      (x_1_2): DecoderBlock(
        (conv1): Conv2dReLU(
          (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention1): Attention(
          (attention): Identity()
        )
        (conv2): Conv2dReLU(
          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention2): Attention(
          (attention): Identity()
        )
      )
      (x_2_2): DecoderBlock(
        (conv1): Conv2dReLU(
          (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention1): Attention(
          (attention): Identity()
        )
        (conv2): Conv2dReLU(
          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention2): Attention(
          (attention): Identity()
        )
      )
      (x_0_3): DecoderBlock(
        (conv1): Conv2dReLU(
          (0): Conv2d(320, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention1): Attention(
          (attention): Identity()
        )
        (conv2): Conv2dReLU(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention2): Attention(
          (attention): Identity()
        )
      )
      (x_1_3): DecoderBlock(
        (conv1): Conv2dReLU(
          (0): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention1): Attention(
          (attention): Identity()
        )
        (conv2): Conv2dReLU(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention2): Attention(
          (attention): Identity()
        )
      )
      (x_2_3): DecoderBlock(
        (conv1): Conv2dReLU(
          (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention1): Attention(
          (attention): Identity()
        )
        (conv2): Conv2dReLU(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention2): Attention(
          (attention): Identity()
        )
      )
      (x_3_3): DecoderBlock(
        (conv1): Conv2dReLU(
          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention1): Attention(
          (attention): Identity()
        )
        (conv2): Conv2dReLU(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention2): Attention(
          (attention): Identity()
        )
      )
      (x_0_4): DecoderBlock(
        (conv1): Conv2dReLU(
          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention1): Attention(
          (attention): Identity()
        )
        (conv2): Conv2dReLU(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
        (attention2): Attention(
          (attention): Identity()
        )
      )
    )
  )
  (segmentation_head): SegmentationHead(
    (0): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Identity()
    (2): Activation(
      (activation): Identity()
    )
  )
)
Number of train batches: 407
Number of val batches: 92
Adjusting learning rate of group 0 to 1.0000e-03.
Sanity Checking: 0it [00:00, ?it/s]Sanity Checking:   0%|                                                                                                                               | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|                                                                                                                  | 0/2 [00:00<?, ?it/s]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])
Sanity Checking DataLoader 0:  50%|█████████████████████████████████████████████████████                                                     | 1/2 [00:00<00:00,  1.28it/s]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])
Sanity Checking DataLoader 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.87it/s]                                                                                                                                                                           Training: 0it [00:00, ?it/s]Training:   0%|                                                                                                                                    | 0/497 [00:00<?, ?it/s]Epoch 0:   0%|                                                                                                                                     | 0/497 [00:00<?, ?it/s]Epoch 0:   0%|▎                                                                                                                            | 1/497 [00:01<10:18,  1.25s/it]Epoch 0:   0%|▏                                                                                                       | 1/497 [00:01<10:18,  1.25s/it, loss=1.54, v_num=75]Epoch 0:   0%|▍                                                                                                       | 2/497 [00:02<09:51,  1.19s/it, loss=1.54, v_num=75]Epoch 0:   0%|▍                                                                                                       | 2/497 [00:02<09:51,  1.19s/it, loss=1.22, v_num=75]Epoch 0:   1%|▋                                                                                                       | 3/497 [00:03<09:41,  1.18s/it, loss=1.22, v_num=75]Epoch 0:   1%|▋                                                                                                       | 3/497 [00:03<09:41,  1.18s/it, loss=1.19, v_num=75]Epoch 0:   1%|▊                                                                                                       | 4/497 [00:04<09:34,  1.17s/it, loss=1.19, v_num=75]Epoch 0:   1%|▊                                                                                                       | 4/497 [00:04<09:34,  1.17s/it, loss=1.05, v_num=75]Epoch 0:   1%|█                                                                                                       | 5/497 [00:05<09:30,  1.16s/it, loss=1.05, v_num=75]Epoch 0:   1%|█                                                                                                       | 5/497 [00:05<09:30,  1.16s/it, loss=1.03, v_num=75]Epoch 0:   1%|█▎                                                                                                      | 6/497 [00:06<09:27,  1.16s/it, loss=1.03, v_num=75]Epoch 0:   1%|█▏                                                                                                     | 6/497 [00:06<09:27,  1.16s/it, loss=0.982, v_num=75]Epoch 0:   1%|█▍                                                                                                     | 7/497 [00:08<09:25,  1.15s/it, loss=0.982, v_num=75]Epoch 0:   1%|█▍                                                                                                     | 7/497 [00:08<09:25,  1.15s/it, loss=0.953, v_num=75]Epoch 0:   2%|█▋                                                                                                     | 8/497 [00:09<09:23,  1.15s/it, loss=0.953, v_num=75]Epoch 0:   2%|█▋                                                                                                     | 8/497 [00:09<09:23,  1.15s/it, loss=0.911, v_num=75]Epoch 0:   2%|█▊                                                                                                     | 9/497 [00:10<09:21,  1.15s/it, loss=0.911, v_num=75]Epoch 0:   2%|█▊                                                                                                     | 9/497 [00:10<09:21,  1.15s/it, loss=0.873, v_num=75]Epoch 0:   2%|██                                                                                                    | 10/497 [00:11<09:20,  1.15s/it, loss=0.873, v_num=75]Epoch 0:   2%|██                                                                                                    | 10/497 [00:11<09:20,  1.15s/it, loss=0.835, v_num=75]Epoch 0:   2%|██▎                                                                                                   | 11/497 [00:12<09:18,  1.15s/it, loss=0.835, v_num=75]Epoch 0:   2%|██▎                                                                                                   | 11/497 [00:12<09:18,  1.15s/it, loss=0.807, v_num=75]Epoch 0:   2%|██▍                                                                                                   | 12/497 [00:13<09:17,  1.15s/it, loss=0.807, v_num=75]Epoch 0:   2%|██▍                                                                                                    | 12/497 [00:13<09:17,  1.15s/it, loss=0.78, v_num=75]Epoch 0:   3%|██▋                                                                                                    | 13/497 [00:14<09:15,  1.15s/it, loss=0.78, v_num=75]Epoch 0:   3%|██▋                                                                                                   | 13/497 [00:14<09:15,  1.15s/it, loss=0.752, v_num=75]Epoch 0:   3%|██▊                                                                                                   | 14/497 [00:16<09:14,  1.15s/it, loss=0.752, v_num=75]Epoch 0:   3%|██▊                                                                                                   | 14/497 [00:16<09:14,  1.15s/it, loss=0.729, v_num=75]Epoch 0:   3%|███                                                                                                   | 15/497 [00:17<09:12,  1.15s/it, loss=0.729, v_num=75]Epoch 0:   3%|███                                                                                                   | 15/497 [00:17<09:12,  1.15s/it, loss=0.711, v_num=75]Epoch 0:   3%|███▎                                                                                                  | 16/497 [00:18<09:11,  1.15s/it, loss=0.711, v_num=75]Epoch 0:   3%|███▎                                                                                                  | 16/497 [00:18<09:11,  1.15s/it, loss=0.687, v_num=75]Epoch 0:   3%|███▍                                                                                                  | 17/497 [00:19<09:10,  1.15s/it, loss=0.687, v_num=75]Epoch 0:   3%|███▍                                                                                                  | 17/497 [00:19<09:10,  1.15s/it, loss=0.666, v_num=75]Epoch 0:   4%|███▋                                                                                                  | 18/497 [00:20<09:09,  1.15s/it, loss=0.666, v_num=75]Epoch 0:   4%|███▋                                                                                                  | 18/497 [00:20<09:09,  1.15s/it, loss=0.649, v_num=75]Epoch 0:   4%|███▉                                                                                                  | 19/497 [00:21<09:08,  1.15s/it, loss=0.649, v_num=75]Epoch 0:   4%|███▉                                                                                                   | 19/497 [00:21<09:08,  1.15s/it, loss=0.63, v_num=75]Epoch 0:   4%|████▏                                                                                                  | 20/497 [00:22<09:06,  1.15s/it, loss=0.63, v_num=75]Epoch 0:   4%|████                                                                                                  | 20/497 [00:22<09:06,  1.15s/it, loss=0.613, v_num=75]Epoch 0:   4%|████▎                                                                                                 | 21/497 [00:24<09:05,  1.15s/it, loss=0.613, v_num=75]Epoch 0:   4%|████▎                                                                                                 | 21/497 [00:24<09:05,  1.15s/it, loss=0.549, v_num=75]Epoch 0:   4%|████▌                                                                                                 | 22/497 [00:25<09:04,  1.15s/it, loss=0.549, v_num=75]Epoch 0:   4%|████▌                                                                                                 | 22/497 [00:25<09:04,  1.15s/it, loss=0.517, v_num=75]Epoch 0:   5%|████▋                                                                                                 | 23/497 [00:26<09:03,  1.15s/it, loss=0.517, v_num=75]Epoch 0:   5%|████▋                                                                                                 | 23/497 [00:26<09:03,  1.15s/it, loss=0.471, v_num=75]Epoch 0:   5%|████▉                                                                                                 | 24/497 [00:27<09:02,  1.15s/it, loss=0.471, v_num=75]Epoch 0:   5%|████▉                                                                                                 | 24/497 [00:27<09:02,  1.15s/it, loss=0.458, v_num=75]Epoch 0:   5%|█████▏                                                                                                | 25/497 [00:28<09:01,  1.15s/it, loss=0.458, v_num=75]Epoch 0:   5%|█████▏                                                                                                | 25/497 [00:28<09:01,  1.15s/it, loss=0.421, v_num=75]Epoch 0:   5%|█████▎                                                                                                | 26/497 [00:29<09:00,  1.15s/it, loss=0.421, v_num=75]Epoch 0:   5%|█████▎                                                                                                | 26/497 [00:29<09:00,  1.15s/it, loss=0.396, v_num=75]Epoch 0:   5%|█████▌                                                                                                | 27/497 [00:30<08:59,  1.15s/it, loss=0.396, v_num=75]Epoch 0:   5%|█████▌                                                                                                | 27/497 [00:30<08:59,  1.15s/it, loss=0.366, v_num=75]Epoch 0:   6%|█████▋                                                                                                | 28/497 [00:32<08:58,  1.15s/it, loss=0.366, v_num=75]Epoch 0:   6%|█████▋                                                                                                | 28/497 [00:32<08:58,  1.15s/it, loss=0.346, v_num=75]Epoch 0:   6%|█████▉                                                                                                | 29/497 [00:33<08:56,  1.15s/it, loss=0.346, v_num=75]Epoch 0:   6%|█████▉                                                                                                | 29/497 [00:33<08:56,  1.15s/it, loss=0.328, v_num=75]Epoch 0:   6%|██████▏                                                                                               | 30/497 [00:34<08:55,  1.15s/it, loss=0.328, v_num=75]Epoch 0:   6%|██████▏                                                                                               | 30/497 [00:34<08:55,  1.15s/it, loss=0.318, v_num=75]Epoch 0:   6%|██████▎                                                                                               | 31/497 [00:35<08:54,  1.15s/it, loss=0.318, v_num=75]Epoch 0:   6%|██████▍                                                                                                 | 31/497 [00:35<08:54,  1.15s/it, loss=0.3, v_num=75]Epoch 0:   6%|██████▋                                                                                                 | 32/497 [00:36<08:53,  1.15s/it, loss=0.3, v_num=75]Epoch 0:   6%|██████▌                                                                                               | 32/497 [00:36<08:53,  1.15s/it, loss=0.285, v_num=75]Epoch 0:   7%|██████▊                                                                                               | 33/497 [00:37<08:52,  1.15s/it, loss=0.285, v_num=75]Epoch 0:   7%|██████▊                                                                                               | 33/497 [00:37<08:52,  1.15s/it, loss=0.271, v_num=75]Epoch 0:   7%|██████▉                                                                                               | 34/497 [00:39<08:51,  1.15s/it, loss=0.271, v_num=75]Epoch 0:   7%|██████▉                                                                                               | 34/497 [00:39<08:51,  1.15s/it, loss=0.258, v_num=75]Epoch 0:   7%|███████▏                                                                                              | 35/497 [00:40<08:50,  1.15s/it, loss=0.258, v_num=75]Epoch 0:   7%|███████▎                                                                                               | 35/497 [00:40<08:50,  1.15s/it, loss=0.25, v_num=75]Epoch 0:   7%|███████▍                                                                                               | 36/497 [00:41<08:49,  1.15s/it, loss=0.25, v_num=75]Epoch 0:   7%|███████▍                                                                                              | 36/497 [00:41<08:49,  1.15s/it, loss=0.257, v_num=75]Epoch 0:   7%|███████▌                                                                                              | 37/497 [00:42<08:48,  1.15s/it, loss=0.257, v_num=75]Epoch 0:   7%|███████▌                                                                                              | 37/497 [00:42<08:48,  1.15s/it, loss=0.262, v_num=75]Epoch 0:   8%|███████▊                                                                                              | 38/497 [00:43<08:47,  1.15s/it, loss=0.262, v_num=75]Epoch 0:   8%|███████▊                                                                                              | 38/497 [00:43<08:47,  1.15s/it, loss=0.249, v_num=75]Epoch 0:   8%|████████                                                                                              | 39/497 [00:44<08:46,  1.15s/it, loss=0.249, v_num=75]Epoch 0:   8%|████████                                                                                              | 39/497 [00:44<08:46,  1.15s/it, loss=0.245, v_num=75]Epoch 0:   8%|████████▏                                                                                             | 40/497 [00:45<08:45,  1.15s/it, loss=0.245, v_num=75]Epoch 0:   8%|████████▏                                                                                             | 40/497 [00:45<08:45,  1.15s/it, loss=0.239, v_num=75]Epoch 0:   8%|████████▍                                                                                             | 41/497 [00:47<08:44,  1.15s/it, loss=0.239, v_num=75]Epoch 0:   8%|████████▍                                                                                              | 41/497 [00:47<08:44,  1.15s/it, loss=0.24, v_num=75]Epoch 0:   8%|████████▋                                                                                              | 42/497 [00:48<08:43,  1.15s/it, loss=0.24, v_num=75]Epoch 0:   8%|████████▌                                                                                             | 42/497 [00:48<08:43,  1.15s/it, loss=0.251, v_num=75]Epoch 0:   9%|████████▊                                                                                             | 43/497 [00:49<08:42,  1.15s/it, loss=0.251, v_num=75]Epoch 0:   9%|████████▉                                                                                              | 43/497 [00:49<08:42,  1.15s/it, loss=0.25, v_num=75]Epoch 0:   9%|█████████                                                                                              | 44/497 [00:50<08:41,  1.15s/it, loss=0.25, v_num=75]Epoch 0:   9%|█████████                                                                                             | 44/497 [00:50<08:41,  1.15s/it, loss=0.239, v_num=75]Epoch 0:   9%|█████████▏                                                                                            | 45/497 [00:51<08:40,  1.15s/it, loss=0.239, v_num=75]Epoch 0:   9%|█████████▏                                                                                            | 45/497 [00:51<08:40,  1.15s/it, loss=0.236, v_num=75]Epoch 0:   9%|█████████▍                                                                                            | 46/497 [00:52<08:39,  1.15s/it, loss=0.236, v_num=75]Epoch 0:   9%|█████████▌                                                                                             | 46/497 [00:52<08:39,  1.15s/it, loss=0.23, v_num=75]Epoch 0:   9%|█████████▋                                                                                             | 47/497 [00:54<08:37,  1.15s/it, loss=0.23, v_num=75]Epoch 0:   9%|█████████▋                                                                                             | 47/497 [00:54<08:38,  1.15s/it, loss=0.23, v_num=75]Epoch 0:  10%|█████████▉                                                                                             | 48/497 [00:55<08:36,  1.15s/it, loss=0.23, v_num=75]Epoch 0:  10%|█████████▊                                                                                            | 48/497 [00:55<08:36,  1.15s/it, loss=0.229, v_num=75]Epoch 0:  10%|██████████                                                                                            | 49/497 [00:56<08:35,  1.15s/it, loss=0.229, v_num=75]Epoch 0:  10%|██████████                                                                                            | 49/497 [00:56<08:35,  1.15s/it, loss=0.233, v_num=75]Epoch 0:  10%|██████████▎                                                                                           | 50/497 [00:57<08:34,  1.15s/it, loss=0.233, v_num=75]Epoch 0:  10%|██████████▎                                                                                           | 50/497 [00:57<08:34,  1.15s/it, loss=0.226, v_num=75]Epoch 0:  10%|██████████▍                                                                                           | 51/497 [00:58<08:33,  1.15s/it, loss=0.226, v_num=75]Epoch 0:  10%|██████████▍                                                                                           | 51/497 [00:58<08:33,  1.15s/it, loss=0.227, v_num=75]Epoch 0:  10%|██████████▋                                                                                           | 52/497 [00:59<08:32,  1.15s/it, loss=0.227, v_num=75]Epoch 0:  10%|██████████▋                                                                                           | 52/497 [00:59<08:32,  1.15s/it, loss=0.229, v_num=75]Epoch 0:  11%|██████████▉                                                                                           | 53/497 [01:01<08:31,  1.15s/it, loss=0.229, v_num=75]Epoch 0:  11%|██████████▉                                                                                            | 53/497 [01:01<08:31,  1.15s/it, loss=0.24, v_num=75]Epoch 0:  11%|███████████▏                                                                                           | 54/497 [01:02<08:30,  1.15s/it, loss=0.24, v_num=75]Epoch 0:  11%|███████████                                                                                           | 54/497 [01:02<08:30,  1.15s/it, loss=0.239, v_num=75]Epoch 0:  11%|███████████▎                                                                                          | 55/497 [01:03<08:29,  1.15s/it, loss=0.239, v_num=75]Epoch 0:  11%|███████████▎                                                                                          | 55/497 [01:03<08:29,  1.15s/it, loss=0.233, v_num=75]Epoch 0:  11%|███████████▍                                                                                          | 56/497 [01:04<08:28,  1.15s/it, loss=0.233, v_num=75]Epoch 0:  11%|███████████▍                                                                                          | 56/497 [01:04<08:28,  1.15s/it, loss=0.218, v_num=75]Epoch 0:  11%|███████████▋                                                                                          | 57/497 [01:05<08:27,  1.15s/it, loss=0.218, v_num=75]Epoch 0:  11%|███████████▋                                                                                          | 57/497 [01:05<08:27,  1.15s/it, loss=0.207, v_num=75]Epoch 0:  12%|███████████▉                                                                                          | 58/497 [01:06<08:26,  1.15s/it, loss=0.207, v_num=75]Epoch 0:  12%|███████████▉                                                                                          | 58/497 [01:06<08:26,  1.15s/it, loss=0.209, v_num=75]Epoch 0:  12%|████████████                                                                                          | 59/497 [01:08<08:25,  1.15s/it, loss=0.209, v_num=75]Epoch 0:  12%|████████████                                                                                          | 59/497 [01:08<08:25,  1.15s/it, loss=0.209, v_num=75]Epoch 0:  12%|████████████▎                                                                                         | 60/497 [01:09<08:24,  1.15s/it, loss=0.209, v_num=75]Epoch 0:  12%|████████████▎                                                                                         | 60/497 [01:09<08:24,  1.15s/it, loss=0.211, v_num=75]Epoch 0:  12%|████████████▌                                                                                         | 61/497 [01:10<08:23,  1.15s/it, loss=0.211, v_num=75]Epoch 0:  12%|████████████▌                                                                                         | 61/497 [01:10<08:23,  1.15s/it, loss=0.202, v_num=75]Epoch 0:  12%|████████████▋                                                                                         | 62/497 [01:11<08:22,  1.16s/it, loss=0.202, v_num=75]Epoch 0:  12%|████████████▋                                                                                         | 62/497 [01:11<08:22,  1.16s/it, loss=0.184, v_num=75]Epoch 0:  13%|████████████▉                                                                                         | 63/497 [01:12<08:21,  1.16s/it, loss=0.184, v_num=75]Epoch 0:  13%|████████████▉                                                                                         | 63/497 [01:12<08:21,  1.16s/it, loss=0.178, v_num=75]Epoch 0:  13%|█████████████▏                                                                                        | 64/497 [01:13<08:20,  1.16s/it, loss=0.178, v_num=75]Epoch 0:  13%|█████████████▏                                                                                        | 64/497 [01:13<08:20,  1.16s/it, loss=0.174, v_num=75]Epoch 0:  13%|█████████████▎                                                                                        | 65/497 [01:15<08:19,  1.16s/it, loss=0.174, v_num=75]Epoch 0:  13%|█████████████▎                                                                                        | 65/497 [01:15<08:19,  1.16s/it, loss=0.172, v_num=75]Epoch 0:  13%|█████████████▌                                                                                        | 66/497 [01:16<08:18,  1.16s/it, loss=0.172, v_num=75]Epoch 0:  13%|█████████████▌                                                                                        | 66/497 [01:16<08:18,  1.16s/it, loss=0.171, v_num=75]Epoch 0:  13%|█████████████▊                                                                                        | 67/497 [01:17<08:17,  1.16s/it, loss=0.171, v_num=75]Epoch 0:  13%|█████████████▊                                                                                        | 67/497 [01:17<08:17,  1.16s/it, loss=0.166, v_num=75]Epoch 0:  14%|█████████████▉                                                                                        | 68/497 [01:18<08:16,  1.16s/it, loss=0.166, v_num=75]Epoch 0:  14%|█████████████▉                                                                                        | 68/497 [01:18<08:16,  1.16s/it, loss=0.185, v_num=75]Epoch 0:  14%|██████████████▏                                                                                       | 69/497 [01:19<08:15,  1.16s/it, loss=0.185, v_num=75]Epoch 0:  14%|██████████████▏                                                                                       | 69/497 [01:19<08:15,  1.16s/it, loss=0.179, v_num=75]Epoch 0:  14%|██████████████▎                                                                                       | 70/497 [01:21<08:14,  1.16s/it, loss=0.179, v_num=75]Epoch 0:  14%|██████████████▎                                                                                       | 70/497 [01:21<08:14,  1.16s/it, loss=0.174, v_num=75]Epoch 0:  14%|██████████████▌                                                                                       | 71/497 [01:22<08:13,  1.16s/it, loss=0.174, v_num=75]Epoch 0:  14%|██████████████▌                                                                                       | 71/497 [01:22<08:13,  1.16s/it, loss=0.186, v_num=75]Epoch 0:  14%|██████████████▊                                                                                       | 72/497 [01:23<08:12,  1.16s/it, loss=0.186, v_num=75]Epoch 0:  14%|██████████████▉                                                                                        | 72/497 [01:23<08:12,  1.16s/it, loss=0.18, v_num=75]Epoch 0:  15%|███████████████▏                                                                                       | 73/497 [01:24<08:11,  1.16s/it, loss=0.18, v_num=75]Epoch 0:  15%|██████████████▉                                                                                       | 73/497 [01:24<08:11,  1.16s/it, loss=0.176, v_num=75]Epoch 0:  15%|███████████████▏                                                                                      | 74/497 [01:25<08:10,  1.16s/it, loss=0.176, v_num=75]Epoch 0:  15%|███████████████▏                                                                                      | 74/497 [01:25<08:10,  1.16s/it, loss=0.173, v_num=75]Epoch 0:  15%|███████████████▍                                                                                      | 75/497 [01:26<08:09,  1.16s/it, loss=0.173, v_num=75]Epoch 0:  15%|███████████████▍                                                                                      | 75/497 [01:26<08:09,  1.16s/it, loss=0.174, v_num=75]Epoch 0:  15%|███████████████▌                                                                                      | 76/497 [01:28<08:08,  1.16s/it, loss=0.174, v_num=75]Epoch 0:  15%|███████████████▌                                                                                      | 76/497 [01:28<08:08,  1.16s/it, loss=0.176, v_num=75]Epoch 0:  15%|███████████████▊                                                                                      | 77/497 [01:29<08:07,  1.16s/it, loss=0.176, v_num=75]Epoch 0:  15%|███████████████▊                                                                                      | 77/497 [01:29<08:07,  1.16s/it, loss=0.175, v_num=75]Epoch 0:  16%|████████████████                                                                                      | 78/497 [01:30<08:06,  1.16s/it, loss=0.175, v_num=75]Epoch 0:  16%|████████████████                                                                                      | 78/497 [01:30<08:06,  1.16s/it, loss=0.173, v_num=75]Epoch 0:  16%|████████████████▏                                                                                     | 79/497 [01:31<08:05,  1.16s/it, loss=0.173, v_num=75]Epoch 0:  16%|████████████████▏                                                                                     | 79/497 [01:31<08:05,  1.16s/it, loss=0.171, v_num=75]Epoch 0:  16%|████████████████▍                                                                                     | 80/497 [01:32<08:04,  1.16s/it, loss=0.171, v_num=75]Epoch 0:  16%|████████████████▍                                                                                     | 80/497 [01:32<08:04,  1.16s/it, loss=0.167, v_num=75]Epoch 0:  16%|████████████████▌                                                                                     | 81/497 [01:34<08:03,  1.16s/it, loss=0.167, v_num=75]Epoch 0:  16%|████████████████▊                                                                                      | 81/497 [01:34<08:03,  1.16s/it, loss=0.17, v_num=75]Epoch 0:  16%|████████████████▉                                                                                      | 82/497 [01:35<08:02,  1.16s/it, loss=0.17, v_num=75]Epoch 0:  16%|████████████████▊                                                                                     | 82/497 [01:35<08:02,  1.16s/it, loss=0.172, v_num=75]Epoch 0:  17%|█████████████████                                                                                     | 83/497 [01:36<08:01,  1.16s/it, loss=0.172, v_num=75]Epoch 0:  17%|█████████████████                                                                                     | 83/497 [01:36<08:01,  1.16s/it, loss=0.175, v_num=75]Epoch 0:  17%|█████████████████▏                                                                                    | 84/497 [01:37<08:00,  1.16s/it, loss=0.175, v_num=75]Epoch 0:  17%|█████████████████▏                                                                                    | 84/497 [01:37<08:00,  1.16s/it, loss=0.178, v_num=75]Epoch 0:  17%|█████████████████▍                                                                                    | 85/497 [01:38<07:58,  1.16s/it, loss=0.178, v_num=75]Epoch 0:  17%|█████████████████▍                                                                                    | 85/497 [01:38<07:58,  1.16s/it, loss=0.184, v_num=75]Epoch 0:  17%|█████████████████▋                                                                                    | 86/497 [01:40<07:57,  1.16s/it, loss=0.184, v_num=75]Epoch 0:  17%|█████████████████▋                                                                                    | 86/497 [01:40<07:57,  1.16s/it, loss=0.186, v_num=75]Epoch 0:  18%|█████████████████▊                                                                                    | 87/497 [01:41<07:56,  1.16s/it, loss=0.186, v_num=75]Epoch 0:  18%|█████████████████▊                                                                                    | 87/497 [01:41<07:56,  1.16s/it, loss=0.189, v_num=75]Epoch 0:  18%|██████████████████                                                                                    | 88/497 [01:42<07:55,  1.16s/it, loss=0.189, v_num=75]Epoch 0:  18%|██████████████████                                                                                    | 88/497 [01:42<07:55,  1.16s/it, loss=0.165, v_num=75]Epoch 0:  18%|██████████████████▎                                                                                   | 89/497 [01:43<07:54,  1.16s/it, loss=0.165, v_num=75]Epoch 0:  18%|██████████████████▎                                                                                   | 89/497 [01:43<07:54,  1.16s/it, loss=0.168, v_num=75]Epoch 0:  18%|██████████████████▍                                                                                   | 90/497 [01:44<07:53,  1.16s/it, loss=0.168, v_num=75]Epoch 0:  18%|██████████████████▋                                                                                    | 90/497 [01:44<07:53,  1.16s/it, loss=0.17, v_num=75]Epoch 0:  18%|██████████████████▊                                                                                    | 91/497 [01:45<07:52,  1.16s/it, loss=0.17, v_num=75]Epoch 0:  18%|██████████████████▋                                                                                   | 91/497 [01:45<07:52,  1.16s/it, loss=0.153, v_num=75]Epoch 0:  19%|██████████████████▉                                                                                   | 92/497 [01:47<07:51,  1.16s/it, loss=0.153, v_num=75]Epoch 0:  19%|██████████████████▉                                                                                   | 92/497 [01:47<07:51,  1.16s/it, loss=0.153, v_num=75]Epoch 0:  19%|███████████████████                                                                                   | 93/497 [01:48<07:50,  1.17s/it, loss=0.153, v_num=75]Epoch 0:  19%|███████████████████                                                                                   | 93/497 [01:48<07:50,  1.17s/it, loss=0.163, v_num=75]Epoch 0:  19%|███████████████████▎                                                                                  | 94/497 [01:49<07:49,  1.17s/it, loss=0.163, v_num=75]Epoch 0:  19%|███████████████████▎                                                                                  | 94/497 [01:49<07:49,  1.17s/it, loss=0.165, v_num=75]Epoch 0:  19%|███████████████████▍                                                                                  | 95/497 [01:50<07:48,  1.17s/it, loss=0.165, v_num=75]Epoch 0:  19%|███████████████████▍                                                                                  | 95/497 [01:50<07:48,  1.17s/it, loss=0.159, v_num=75]Epoch 0:  19%|███████████████████▋                                                                                  | 96/497 [01:51<07:47,  1.17s/it, loss=0.159, v_num=75]Epoch 0:  19%|███████████████████▋                                                                                  | 96/497 [01:51<07:47,  1.17s/it, loss=0.155, v_num=75]Epoch 0:  20%|███████████████████▉                                                                                  | 97/497 [01:53<07:46,  1.17s/it, loss=0.155, v_num=75]Epoch 0:  20%|███████████████████▉                                                                                  | 97/497 [01:53<07:46,  1.17s/it, loss=0.153, v_num=75]Epoch 0:  20%|████████████████████                                                                                  | 98/497 [01:54<07:45,  1.17s/it, loss=0.153, v_num=75]Epoch 0:  20%|████████████████████                                                                                  | 98/497 [01:54<07:45,  1.17s/it, loss=0.156, v_num=75]Epoch 0:  20%|████████████████████▎                                                                                 | 99/497 [01:55<07:44,  1.17s/it, loss=0.156, v_num=75]Epoch 0:  20%|████████████████████▎                                                                                 | 99/497 [01:55<07:44,  1.17s/it, loss=0.164, v_num=75]Epoch 0:  20%|████████████████████▎                                                                                | 100/497 [01:56<07:43,  1.17s/it, loss=0.164, v_num=75]Epoch 0:  20%|████████████████████▎                                                                                | 100/497 [01:56<07:43,  1.17s/it, loss=0.171, v_num=75]Epoch 0:  20%|████████████████████▌                                                                                | 101/497 [01:57<07:42,  1.17s/it, loss=0.171, v_num=75]Epoch 0:  20%|████████████████████▌                                                                                | 101/497 [01:57<07:42,  1.17s/it, loss=0.174, v_num=75]Epoch 0:  21%|████████████████████▋                                                                                | 102/497 [01:59<07:41,  1.17s/it, loss=0.174, v_num=75]Epoch 0:  21%|████████████████████▋                                                                                | 102/497 [01:59<07:41,  1.17s/it, loss=0.175, v_num=75]Epoch 0:  21%|████████████████████▉                                                                                | 103/497 [02:00<07:40,  1.17s/it, loss=0.175, v_num=75]Epoch 0:  21%|████████████████████▉                                                                                | 103/497 [02:00<07:40,  1.17s/it, loss=0.176, v_num=75]Epoch 0:  21%|█████████████████████▏                                                                               | 104/497 [02:01<07:39,  1.17s/it, loss=0.176, v_num=75]Epoch 0:  21%|█████████████████████▏                                                                               | 104/497 [02:01<07:39,  1.17s/it, loss=0.176, v_num=75]Epoch 0:  21%|█████████████████████▎                                                                               | 105/497 [02:02<07:38,  1.17s/it, loss=0.176, v_num=75]Epoch 0:  21%|█████████████████████▎                                                                               | 105/497 [02:02<07:38,  1.17s/it, loss=0.172, v_num=75]Epoch 0:  21%|█████████████████████▌                                                                               | 106/497 [02:03<07:37,  1.17s/it, loss=0.172, v_num=75]Epoch 0:  21%|█████████████████████▌                                                                               | 106/497 [02:03<07:37,  1.17s/it, loss=0.173, v_num=75]Epoch 0:  22%|█████████████████████▋                                                                               | 107/497 [02:05<07:36,  1.17s/it, loss=0.173, v_num=75]Epoch 0:  22%|█████████████████████▋                                                                               | 107/497 [02:05<07:36,  1.17s/it, loss=0.173, v_num=75]Epoch 0:  22%|█████████████████████▉                                                                               | 108/497 [02:06<07:35,  1.17s/it, loss=0.173, v_num=75]Epoch 0:  22%|█████████████████████▉                                                                               | 108/497 [02:06<07:35,  1.17s/it, loss=0.176, v_num=75]Epoch 0:  22%|██████████████████████▏                                                                              | 109/497 [02:07<07:34,  1.17s/it, loss=0.176, v_num=75]Epoch 0:  22%|██████████████████████▏                                                                              | 109/497 [02:07<07:34,  1.17s/it, loss=0.172, v_num=75]Epoch 0:  22%|██████████████████████▎                                                                              | 110/497 [02:08<07:33,  1.17s/it, loss=0.172, v_num=75]Epoch 0:  22%|██████████████████████▎                                                                              | 110/497 [02:08<07:33,  1.17s/it, loss=0.172, v_num=75]Epoch 0:  22%|██████████████████████▌                                                                              | 111/497 [02:09<07:31,  1.17s/it, loss=0.172, v_num=75]Epoch 0:  22%|██████████████████████▌                                                                              | 111/497 [02:09<07:31,  1.17s/it, loss=0.173, v_num=75]Epoch 0:  23%|██████████████████████▊                                                                              | 112/497 [02:11<07:30,  1.17s/it, loss=0.173, v_num=75]Epoch 0:  23%|██████████████████████▊                                                                              | 112/497 [02:11<07:30,  1.17s/it, loss=0.176, v_num=75]Epoch 0:  23%|██████████████████████▉                                                                              | 113/497 [02:12<07:29,  1.17s/it, loss=0.176, v_num=75]Epoch 0:  23%|██████████████████████▉                                                                              | 113/497 [02:12<07:29,  1.17s/it, loss=0.157, v_num=75]Epoch 0:  23%|███████████████████████▏                                                                             | 114/497 [02:13<07:28,  1.17s/it, loss=0.157, v_num=75]Epoch 0:  23%|███████████████████████▏                                                                             | 114/497 [02:13<07:28,  1.17s/it, loss=0.154, v_num=75]Epoch 0:  23%|███████████████████████▎                                                                             | 115/497 [02:14<07:27,  1.17s/it, loss=0.154, v_num=75]Epoch 0:  23%|███████████████████████▎                                                                             | 115/497 [02:14<07:27,  1.17s/it, loss=0.159, v_num=75]Epoch 0:  23%|███████████████████████▌                                                                             | 116/497 [02:15<07:26,  1.17s/it, loss=0.159, v_num=75]Epoch 0:  23%|███████████████████████▌                                                                             | 116/497 [02:15<07:26,  1.17s/it, loss=0.158, v_num=75]Epoch 0:  24%|███████████████████████▊                                                                             | 117/497 [02:17<07:25,  1.17s/it, loss=0.158, v_num=75]Epoch 0:  24%|███████████████████████▊                                                                             | 117/497 [02:17<07:25,  1.17s/it, loss=0.158, v_num=75]Epoch 0:  24%|███████████████████████▉                                                                             | 118/497 [02:18<07:24,  1.17s/it, loss=0.158, v_num=75]Epoch 0:  24%|███████████████████████▉                                                                             | 118/497 [02:18<07:24,  1.17s/it, loss=0.151, v_num=75]Epoch 0:  24%|████████████████████████▏                                                                            | 119/497 [02:19<07:23,  1.17s/it, loss=0.151, v_num=75]Epoch 0:  24%|████████████████████████▏                                                                            | 119/497 [02:19<07:23,  1.17s/it, loss=0.141, v_num=75]Epoch 0:  24%|████████████████████████▍                                                                            | 120/497 [02:20<07:22,  1.17s/it, loss=0.141, v_num=75]Epoch 0:  24%|████████████████████████▍                                                                            | 120/497 [02:20<07:22,  1.17s/it, loss=0.129, v_num=75]Epoch 0:  24%|████████████████████████▌                                                                            | 121/497 [02:22<07:21,  1.17s/it, loss=0.129, v_num=75]Epoch 0:  24%|████████████████████████▌                                                                            | 121/497 [02:22<07:21,  1.17s/it, loss=0.123, v_num=75]Epoch 0:  25%|████████████████████████▊                                                                            | 122/497 [02:23<07:20,  1.17s/it, loss=0.123, v_num=75]Epoch 0:  25%|████████████████████████▊                                                                            | 122/497 [02:23<07:20,  1.17s/it, loss=0.131, v_num=75]Epoch 0:  25%|████████████████████████▉                                                                            | 123/497 [02:24<07:19,  1.17s/it, loss=0.131, v_num=75]Epoch 0:  25%|████████████████████████▉                                                                            | 123/497 [02:24<07:19,  1.17s/it, loss=0.126, v_num=75]Epoch 0:  25%|█████████████████████████▏                                                                           | 124/497 [02:25<07:18,  1.17s/it, loss=0.126, v_num=75]Epoch 0:  25%|█████████████████████████▏                                                                           | 124/497 [02:25<07:18,  1.17s/it, loss=0.128, v_num=75]Epoch 0:  25%|█████████████████████████▍                                                                           | 125/497 [02:26<07:16,  1.17s/it, loss=0.128, v_num=75]Epoch 0:  25%|█████████████████████████▍                                                                           | 125/497 [02:26<07:16,  1.17s/it, loss=0.126, v_num=75]Epoch 0:  25%|█████████████████████████▌                                                                           | 126/497 [02:28<07:15,  1.17s/it, loss=0.126, v_num=75]Epoch 0:  25%|█████████████████████████▌                                                                           | 126/497 [02:28<07:15,  1.17s/it, loss=0.121, v_num=75]Epoch 0:  26%|█████████████████████████▊                                                                           | 127/497 [02:29<07:14,  1.18s/it, loss=0.121, v_num=75]Epoch 0:  26%|█████████████████████████▊                                                                           | 127/497 [02:29<07:14,  1.18s/it, loss=0.116, v_num=75]Epoch 0:  26%|██████████████████████████                                                                           | 128/497 [02:30<07:13,  1.18s/it, loss=0.116, v_num=75]Epoch 0:  26%|██████████████████████████                                                                           | 128/497 [02:30<07:13,  1.18s/it, loss=0.123, v_num=75]Epoch 0:  26%|██████████████████████████▏                                                                          | 129/497 [02:31<07:12,  1.18s/it, loss=0.123, v_num=75]Epoch 0:  26%|██████████████████████████▏                                                                          | 129/497 [02:31<07:12,  1.18s/it, loss=0.129, v_num=75]Epoch 0:  26%|██████████████████████████▍                                                                          | 130/497 [02:32<07:11,  1.18s/it, loss=0.129, v_num=75]Epoch 0:  26%|██████████████████████████▍                                                                          | 130/497 [02:32<07:11,  1.18s/it, loss=0.126, v_num=75]Epoch 0:  26%|██████████████████████████▌                                                                          | 131/497 [02:34<07:10,  1.18s/it, loss=0.126, v_num=75]Epoch 0:  26%|██████████████████████████▌                                                                          | 131/497 [02:34<07:10,  1.18s/it, loss=0.127, v_num=75]Epoch 0:  27%|██████████████████████████▊                                                                          | 132/497 [02:35<07:09,  1.18s/it, loss=0.127, v_num=75]Epoch 0:  27%|██████████████████████████▊                                                                          | 132/497 [02:35<07:09,  1.18s/it, loss=0.126, v_num=75]Epoch 0:  27%|███████████████████████████                                                                          | 133/497 [02:36<07:08,  1.18s/it, loss=0.126, v_num=75]Epoch 0:  27%|███████████████████████████                                                                          | 133/497 [02:36<07:08,  1.18s/it, loss=0.127, v_num=75]Epoch 0:  27%|███████████████████████████▏                                                                         | 134/497 [02:37<07:07,  1.18s/it, loss=0.127, v_num=75]Epoch 0:  27%|███████████████████████████▏                                                                         | 134/497 [02:37<07:07,  1.18s/it, loss=0.129, v_num=75]Epoch 0:  27%|███████████████████████████▍                                                                         | 135/497 [02:38<07:05,  1.18s/it, loss=0.129, v_num=75]Epoch 0:  27%|███████████████████████████▍                                                                         | 135/497 [02:38<07:05,  1.18s/it, loss=0.123, v_num=75]Epoch 0:  27%|███████████████████████████▋                                                                         | 136/497 [02:40<07:04,  1.18s/it, loss=0.123, v_num=75]Epoch 0:  27%|███████████████████████████▋                                                                         | 136/497 [02:40<07:04,  1.18s/it, loss=0.121, v_num=75]Epoch 0:  28%|███████████████████████████▊                                                                         | 137/497 [02:41<07:03,  1.18s/it, loss=0.121, v_num=75]Epoch 0:  28%|███████████████████████████▊                                                                         | 137/497 [02:41<07:03,  1.18s/it, loss=0.129, v_num=75]Epoch 0:  28%|████████████████████████████                                                                         | 138/497 [02:42<07:02,  1.18s/it, loss=0.129, v_num=75]Epoch 0:  28%|████████████████████████████                                                                         | 138/497 [02:42<07:02,  1.18s/it, loss=0.134, v_num=75]Epoch 0:  28%|████████████████████████████▏                                                                        | 139/497 [02:43<07:01,  1.18s/it, loss=0.134, v_num=75]Epoch 0:  28%|████████████████████████████▏                                                                        | 139/497 [02:43<07:01,  1.18s/it, loss=0.134, v_num=75]Epoch 0:  28%|████████████████████████████▍                                                                        | 140/497 [02:44<07:00,  1.18s/it, loss=0.134, v_num=75]Epoch 0:  28%|████████████████████████████▍                                                                        | 140/497 [02:44<07:00,  1.18s/it, loss=0.144, v_num=75]Epoch 0:  28%|████████████████████████████▋                                                                        | 141/497 [02:46<06:59,  1.18s/it, loss=0.144, v_num=75]Epoch 0:  28%|████████████████████████████▉                                                                         | 141/497 [02:46<06:59,  1.18s/it, loss=0.15, v_num=75]Epoch 0:  29%|█████████████████████████████▏                                                                        | 142/497 [02:47<06:58,  1.18s/it, loss=0.15, v_num=75]Epoch 0:  29%|█████████████████████████████▏                                                                        | 142/497 [02:47<06:58,  1.18s/it, loss=0.14, v_num=75]Epoch 0:  29%|█████████████████████████████▎                                                                        | 143/497 [02:48<06:57,  1.18s/it, loss=0.14, v_num=75]Epoch 0:  29%|█████████████████████████████                                                                        | 143/497 [02:48<06:57,  1.18s/it, loss=0.142, v_num=75]Epoch 0:  29%|█████████████████████████████▎                                                                       | 144/497 [02:49<06:56,  1.18s/it, loss=0.142, v_num=75]Epoch 0:  29%|█████████████████████████████▎                                                                       | 144/497 [02:49<06:56,  1.18s/it, loss=0.139, v_num=75]Epoch 0:  29%|█████████████████████████████▍                                                                       | 145/497 [02:50<06:54,  1.18s/it, loss=0.139, v_num=75]Epoch 0:  29%|█████████████████████████████▍                                                                       | 145/497 [02:50<06:54,  1.18s/it, loss=0.139, v_num=75]Epoch 0:  29%|█████████████████████████████▋                                                                       | 146/497 [02:52<06:53,  1.18s/it, loss=0.139, v_num=75]Epoch 0:  29%|█████████████████████████████▋                                                                       | 146/497 [02:52<06:53,  1.18s/it, loss=0.141, v_num=75]Epoch 0:  30%|█████████████████████████████▊                                                                       | 147/497 [02:53<06:52,  1.18s/it, loss=0.141, v_num=75]Epoch 0:  30%|█████████████████████████████▊                                                                       | 147/497 [02:53<06:52,  1.18s/it, loss=0.146, v_num=75]Epoch 0:  30%|██████████████████████████████                                                                       | 148/497 [02:54<06:51,  1.18s/it, loss=0.146, v_num=75]Epoch 0:  30%|██████████████████████████████                                                                       | 148/497 [02:54<06:51,  1.18s/it, loss=0.141, v_num=75]Epoch 0:  30%|██████████████████████████████▎                                                                      | 149/497 [02:55<06:50,  1.18s/it, loss=0.141, v_num=75]Epoch 0:  30%|██████████████████████████████▎                                                                      | 149/497 [02:55<06:50,  1.18s/it, loss=0.169, v_num=75]Epoch 0:  30%|██████████████████████████████▍                                                                      | 150/497 [02:56<06:49,  1.18s/it, loss=0.169, v_num=75]Epoch 0:  30%|██████████████████████████████▍                                                                      | 150/497 [02:56<06:49,  1.18s/it, loss=0.173, v_num=75]Epoch 0:  30%|██████████████████████████████▋                                                                      | 151/497 [02:58<06:48,  1.18s/it, loss=0.173, v_num=75]Epoch 0:  30%|██████████████████████████████▋                                                                      | 151/497 [02:58<06:48,  1.18s/it, loss=0.172, v_num=75]Epoch 0:  31%|██████████████████████████████▉                                                                      | 152/497 [02:59<06:47,  1.18s/it, loss=0.172, v_num=75]Epoch 0:  31%|███████████████████████████████▏                                                                      | 152/497 [02:59<06:47,  1.18s/it, loss=0.17, v_num=75]Epoch 0:  31%|███████████████████████████████▍                                                                      | 153/497 [03:00<06:46,  1.18s/it, loss=0.17, v_num=75]Epoch 0:  31%|███████████████████████████████                                                                      | 153/497 [03:00<06:46,  1.18s/it, loss=0.173, v_num=75]Epoch 0:  31%|███████████████████████████████▎                                                                     | 154/497 [03:01<06:44,  1.18s/it, loss=0.173, v_num=75]Epoch 0:  31%|███████████████████████████████▎                                                                     | 154/497 [03:01<06:44,  1.18s/it, loss=0.175, v_num=75]Epoch 0:  31%|███████████████████████████████▍                                                                     | 155/497 [03:03<06:43,  1.18s/it, loss=0.175, v_num=75]Epoch 0:  31%|███████████████████████████████▍                                                                     | 155/497 [03:03<06:43,  1.18s/it, loss=0.177, v_num=75]Epoch 0:  31%|███████████████████████████████▋                                                                     | 156/497 [03:04<06:42,  1.18s/it, loss=0.177, v_num=75]Epoch 0:  31%|████████████████████████████████                                                                      | 156/497 [03:04<06:42,  1.18s/it, loss=0.18, v_num=75]Epoch 0:  32%|████████████████████████████████▏                                                                     | 157/497 [03:05<06:41,  1.18s/it, loss=0.18, v_num=75]Epoch 0:  32%|███████████████████████████████▉                                                                     | 157/497 [03:05<06:41,  1.18s/it, loss=0.173, v_num=75]Epoch 0:  32%|████████████████████████████████                                                                     | 158/497 [03:06<06:40,  1.18s/it, loss=0.173, v_num=75]Epoch 0:  32%|████████████████████████████████                                                                     | 158/497 [03:06<06:40,  1.18s/it, loss=0.177, v_num=75]Epoch 0:  32%|████████████████████████████████▎                                                                    | 159/497 [03:07<06:39,  1.18s/it, loss=0.177, v_num=75]Epoch 0:  32%|████████████████████████████████▎                                                                    | 159/497 [03:07<06:39,  1.18s/it, loss=0.177, v_num=75]Epoch 0:  32%|████████████████████████████████▌                                                                    | 160/497 [03:09<06:38,  1.18s/it, loss=0.177, v_num=75]Epoch 0:  32%|████████████████████████████████▊                                                                     | 160/497 [03:09<06:38,  1.18s/it, loss=0.17, v_num=75]Epoch 0:  32%|█████████████████████████████████                                                                     | 161/497 [03:10<06:37,  1.18s/it, loss=0.17, v_num=75]Epoch 0:  32%|████████████████████████████████▋                                                                    | 161/497 [03:10<06:37,  1.18s/it, loss=0.165, v_num=75]Epoch 0:  33%|████████████████████████████████▉                                                                    | 162/497 [03:11<06:35,  1.18s/it, loss=0.165, v_num=75]Epoch 0:  33%|████████████████████████████████▉                                                                    | 162/497 [03:11<06:35,  1.18s/it, loss=0.162, v_num=75]Epoch 0:  33%|█████████████████████████████████                                                                    | 163/497 [03:12<06:34,  1.18s/it, loss=0.162, v_num=75]Epoch 0:  33%|█████████████████████████████████                                                                    | 163/497 [03:12<06:34,  1.18s/it, loss=0.166, v_num=75]Epoch 0:  33%|█████████████████████████████████▎                                                                   | 164/497 [03:13<06:33,  1.18s/it, loss=0.166, v_num=75]Epoch 0:  33%|█████████████████████████████████▎                                                                   | 164/497 [03:13<06:33,  1.18s/it, loss=0.165, v_num=75]Epoch 0:  33%|█████████████████████████████████▌                                                                   | 165/497 [03:15<06:32,  1.18s/it, loss=0.165, v_num=75]Epoch 0:  33%|█████████████████████████████████▌                                                                   | 165/497 [03:15<06:32,  1.18s/it, loss=0.165, v_num=75]Epoch 0:  33%|█████████████████████████████████▋                                                                   | 166/497 [03:16<06:31,  1.18s/it, loss=0.165, v_num=75]Epoch 0:  33%|█████████████████████████████████▋                                                                   | 166/497 [03:16<06:31,  1.18s/it, loss=0.164, v_num=75]Epoch 0:  34%|█████████████████████████████████▉                                                                   | 167/497 [03:17<06:30,  1.18s/it, loss=0.164, v_num=75]Epoch 0:  34%|█████████████████████████████████▉                                                                   | 167/497 [03:17<06:30,  1.18s/it, loss=0.169, v_num=75]Epoch 0:  34%|██████████████████████████████████▏                                                                  | 168/497 [03:18<06:29,  1.18s/it, loss=0.169, v_num=75]Epoch 0:  34%|██████████████████████████████████▏                                                                  | 168/497 [03:18<06:29,  1.18s/it, loss=0.164, v_num=75]Epoch 0:  34%|██████████████████████████████████▎                                                                  | 169/497 [03:19<06:27,  1.18s/it, loss=0.164, v_num=75]Epoch 0:  34%|██████████████████████████████████▎                                                                  | 169/497 [03:19<06:27,  1.18s/it, loss=0.142, v_num=75]Epoch 0:  34%|██████████████████████████████████▌                                                                  | 170/497 [03:21<06:26,  1.18s/it, loss=0.142, v_num=75]Epoch 0:  34%|██████████████████████████████████▌                                                                  | 170/497 [03:21<06:26,  1.18s/it, loss=0.142, v_num=75]Epoch 0:  34%|██████████████████████████████████▊                                                                  | 171/497 [03:22<06:25,  1.18s/it, loss=0.142, v_num=75]Epoch 0:  34%|██████████████████████████████████▊                                                                  | 171/497 [03:22<06:25,  1.18s/it, loss=0.145, v_num=75]Epoch 0:  35%|██████████████████████████████████▉                                                                  | 172/497 [03:23<06:24,  1.18s/it, loss=0.145, v_num=75]Epoch 0:  35%|██████████████████████████████████▉                                                                  | 172/497 [03:23<06:24,  1.18s/it, loss=0.144, v_num=75]Epoch 0:  35%|███████████████████████████████████▏                                                                 | 173/497 [03:24<06:23,  1.18s/it, loss=0.144, v_num=75]Epoch 0:  35%|███████████████████████████████████▏                                                                 | 173/497 [03:24<06:23,  1.18s/it, loss=0.143, v_num=75]Epoch 0:  35%|███████████████████████████████████▎                                                                 | 174/497 [03:25<06:22,  1.18s/it, loss=0.143, v_num=75]Epoch 0:  35%|███████████████████████████████████▎                                                                 | 174/497 [03:25<06:22,  1.18s/it, loss=0.146, v_num=75]Epoch 0:  35%|███████████████████████████████████▌                                                                 | 175/497 [03:27<06:21,  1.18s/it, loss=0.146, v_num=75]Epoch 0:  35%|███████████████████████████████████▌                                                                 | 175/497 [03:27<06:21,  1.18s/it, loss=0.145, v_num=75]Epoch 0:  35%|███████████████████████████████████▊                                                                 | 176/497 [03:28<06:19,  1.18s/it, loss=0.145, v_num=75]Epoch 0:  35%|███████████████████████████████████▊                                                                 | 176/497 [03:28<06:19,  1.18s/it, loss=0.144, v_num=75]Epoch 0:  36%|███████████████████████████████████▉                                                                 | 177/497 [03:29<06:18,  1.18s/it, loss=0.144, v_num=75]Epoch 0:  36%|███████████████████████████████████▉                                                                 | 177/497 [03:29<06:18,  1.18s/it, loss=0.144, v_num=75]Epoch 0:  36%|████████████████████████████████████▏                                                                | 178/497 [03:30<06:17,  1.18s/it, loss=0.144, v_num=75]Epoch 0:  36%|████████████████████████████████████▏                                                                | 178/497 [03:30<06:17,  1.18s/it, loss=0.138, v_num=75]Epoch 0:  36%|████████████████████████████████████▍                                                                | 179/497 [03:31<06:16,  1.18s/it, loss=0.138, v_num=75]Epoch 0:  36%|████████████████████████████████████▍                                                                | 179/497 [03:31<06:16,  1.18s/it, loss=0.136, v_num=75]Epoch 0:  36%|████████████████████████████████████▌                                                                | 180/497 [03:33<06:15,  1.18s/it, loss=0.136, v_num=75]Epoch 0:  36%|████████████████████████████████████▌                                                                | 180/497 [03:33<06:15,  1.18s/it, loss=0.149, v_num=75]Epoch 0:  36%|████████████████████████████████████▊                                                                | 181/497 [03:34<06:14,  1.18s/it, loss=0.149, v_num=75]Epoch 0:  36%|████████████████████████████████████▊                                                                | 181/497 [03:34<06:14,  1.18s/it, loss=0.152, v_num=75]Epoch 0:  37%|████████████████████████████████████▉                                                                | 182/497 [03:35<06:13,  1.18s/it, loss=0.152, v_num=75]Epoch 0:  37%|████████████████████████████████████▉                                                                | 182/497 [03:35<06:13,  1.18s/it, loss=0.153, v_num=75]Epoch 0:  37%|█████████████████████████████████████▏                                                               | 183/497 [03:36<06:11,  1.18s/it, loss=0.153, v_num=75]Epoch 0:  37%|█████████████████████████████████████▏                                                               | 183/497 [03:36<06:11,  1.18s/it, loss=0.155, v_num=75]Epoch 0:  37%|█████████████████████████████████████▍                                                               | 184/497 [03:37<06:10,  1.18s/it, loss=0.155, v_num=75]Epoch 0:  37%|█████████████████████████████████████▍                                                               | 184/497 [03:37<06:10,  1.18s/it, loss=0.155, v_num=75]Epoch 0:  37%|█████████████████████████████████████▌                                                               | 185/497 [03:39<06:09,  1.18s/it, loss=0.155, v_num=75]Epoch 0:  37%|█████████████████████████████████████▌                                                               | 185/497 [03:39<06:09,  1.18s/it, loss=0.154, v_num=75]Epoch 0:  37%|█████████████████████████████████████▊                                                               | 186/497 [03:40<06:08,  1.18s/it, loss=0.154, v_num=75]Epoch 0:  37%|█████████████████████████████████████▊                                                               | 186/497 [03:40<06:08,  1.18s/it, loss=0.155, v_num=75]Epoch 0:  38%|██████████████████████████████████████                                                               | 187/497 [03:41<06:07,  1.18s/it, loss=0.155, v_num=75]Epoch 0:  38%|██████████████████████████████████████                                                               | 187/497 [03:41<06:07,  1.18s/it, loss=0.147, v_num=75]Epoch 0:  38%|██████████████████████████████████████▏                                                              | 188/497 [03:42<06:06,  1.18s/it, loss=0.147, v_num=75]Epoch 0:  38%|██████████████████████████████████████▏                                                              | 188/497 [03:42<06:06,  1.18s/it, loss=0.155, v_num=75]Epoch 0:  38%|██████████████████████████████████████▍                                                              | 189/497 [03:43<06:05,  1.19s/it, loss=0.155, v_num=75]Epoch 0:  38%|██████████████████████████████████████▍                                                              | 189/497 [03:43<06:05,  1.19s/it, loss=0.149, v_num=75]Epoch 0:  38%|██████████████████████████████████████▌                                                              | 190/497 [03:45<06:03,  1.19s/it, loss=0.149, v_num=75]Epoch 0:  38%|██████████████████████████████████████▌                                                              | 190/497 [03:45<06:03,  1.19s/it, loss=0.146, v_num=75]Epoch 0:  38%|██████████████████████████████████████▊                                                              | 191/497 [03:46<06:02,  1.19s/it, loss=0.146, v_num=75]Epoch 0:  38%|██████████████████████████████████████▊                                                              | 191/497 [03:46<06:02,  1.19s/it, loss=0.144, v_num=75]Epoch 0:  39%|███████████████████████████████████████                                                              | 192/497 [03:47<06:01,  1.19s/it, loss=0.144, v_num=75]Epoch 0:  39%|███████████████████████████████████████                                                              | 192/497 [03:47<06:01,  1.19s/it, loss=0.144, v_num=75]Epoch 0:  39%|███████████████████████████████████████▏                                                             | 193/497 [03:48<06:00,  1.19s/it, loss=0.144, v_num=75]Epoch 0:  39%|███████████████████████████████████████▏                                                             | 193/497 [03:48<06:00,  1.19s/it, loss=0.143, v_num=75]Epoch 0:  39%|███████████████████████████████████████▍                                                             | 194/497 [03:49<05:59,  1.19s/it, loss=0.143, v_num=75]Epoch 0:  39%|███████████████████████████████████████▍                                                             | 194/497 [03:50<05:59,  1.19s/it, loss=0.153, v_num=75]Epoch 0:  39%|███████████████████████████████████████▋                                                             | 195/497 [03:51<05:58,  1.19s/it, loss=0.153, v_num=75]Epoch 0:  39%|███████████████████████████████████████▋                                                             | 195/497 [03:51<05:58,  1.19s/it, loss=0.151, v_num=75]Epoch 0:  39%|███████████████████████████████████████▊                                                             | 196/497 [03:52<05:56,  1.19s/it, loss=0.151, v_num=75]Epoch 0:  39%|███████████████████████████████████████▊                                                             | 196/497 [03:52<05:56,  1.19s/it, loss=0.148, v_num=75]Epoch 0:  40%|████████████████████████████████████████                                                             | 197/497 [03:53<05:55,  1.19s/it, loss=0.148, v_num=75]Epoch 0:  40%|████████████████████████████████████████                                                             | 197/497 [03:53<05:55,  1.19s/it, loss=0.145, v_num=75]Epoch 0:  40%|████████████████████████████████████████▏                                                            | 198/497 [03:54<05:54,  1.19s/it, loss=0.145, v_num=75]Epoch 0:  40%|████████████████████████████████████████▏                                                            | 198/497 [03:54<05:54,  1.19s/it, loss=0.153, v_num=75]Epoch 0:  40%|████████████████████████████████████████▍                                                            | 199/497 [03:56<05:53,  1.19s/it, loss=0.153, v_num=75]Epoch 0:  40%|████████████████████████████████████████▍                                                            | 199/497 [03:56<05:53,  1.19s/it, loss=0.155, v_num=75]Epoch 0:  40%|████████████████████████████████████████▋                                                            | 200/497 [03:57<05:52,  1.19s/it, loss=0.155, v_num=75]Epoch 0:  40%|████████████████████████████████████████▋                                                            | 200/497 [03:57<05:52,  1.19s/it, loss=0.138, v_num=75]Epoch 0:  40%|████████████████████████████████████████▊                                                            | 201/497 [03:58<05:51,  1.19s/it, loss=0.138, v_num=75]Epoch 0:  40%|████████████████████████████████████████▊                                                            | 201/497 [03:58<05:51,  1.19s/it, loss=0.139, v_num=75]Epoch 0:  41%|█████████████████████████████████████████                                                            | 202/497 [03:59<05:49,  1.19s/it, loss=0.139, v_num=75]Epoch 0:  41%|█████████████████████████████████████████                                                            | 202/497 [03:59<05:49,  1.19s/it, loss=0.138, v_num=75]Epoch 0:  41%|█████████████████████████████████████████▎                                                           | 203/497 [04:00<05:48,  1.19s/it, loss=0.138, v_num=75]Epoch 0:  41%|█████████████████████████████████████████▋                                                            | 203/497 [04:00<05:48,  1.19s/it, loss=0.13, v_num=75]Epoch 0:  41%|█████████████████████████████████████████▊                                                            | 204/497 [04:02<05:47,  1.19s/it, loss=0.13, v_num=75]Epoch 0:  41%|█████████████████████████████████████████▍                                                           | 204/497 [04:02<05:47,  1.19s/it, loss=0.142, v_num=75]Epoch 0:  41%|█████████████████████████████████████████▋                                                           | 205/497 [04:03<05:46,  1.19s/it, loss=0.142, v_num=75]Epoch 0:  41%|█████████████████████████████████████████▋                                                           | 205/497 [04:03<05:46,  1.19s/it, loss=0.152, v_num=75]Epoch 0:  41%|█████████████████████████████████████████▊                                                           | 206/497 [04:04<05:45,  1.19s/it, loss=0.152, v_num=75]Epoch 0:  41%|█████████████████████████████████████████▊                                                           | 206/497 [04:04<05:45,  1.19s/it, loss=0.153, v_num=75]Epoch 0:  42%|██████████████████████████████████████████                                                           | 207/497 [04:05<05:44,  1.19s/it, loss=0.153, v_num=75]Epoch 0:  42%|██████████████████████████████████████████                                                           | 207/497 [04:05<05:44,  1.19s/it, loss=0.152, v_num=75]Epoch 0:  42%|██████████████████████████████████████████▎                                                          | 208/497 [04:06<05:42,  1.19s/it, loss=0.152, v_num=75]Epoch 0:  42%|██████████████████████████████████████████▎                                                          | 208/497 [04:06<05:42,  1.19s/it, loss=0.151, v_num=75]Epoch 0:  42%|██████████████████████████████████████████▍                                                          | 209/497 [04:08<05:41,  1.19s/it, loss=0.151, v_num=75]Epoch 0:  42%|██████████████████████████████████████████▍                                                          | 209/497 [04:08<05:41,  1.19s/it, loss=0.145, v_num=75]Epoch 0:  42%|██████████████████████████████████████████▋                                                          | 210/497 [04:09<05:40,  1.19s/it, loss=0.145, v_num=75]Epoch 0:  42%|██████████████████████████████████████████▋                                                          | 210/497 [04:09<05:40,  1.19s/it, loss=0.147, v_num=75]Epoch 0:  42%|██████████████████████████████████████████▉                                                          | 211/497 [04:10<05:39,  1.19s/it, loss=0.147, v_num=75]Epoch 0:  42%|██████████████████████████████████████████▉                                                          | 211/497 [04:10<05:39,  1.19s/it, loss=0.146, v_num=75]Epoch 0:  43%|███████████████████████████████████████████                                                          | 212/497 [04:11<05:38,  1.19s/it, loss=0.146, v_num=75]Epoch 0:  43%|███████████████████████████████████████████                                                          | 212/497 [04:11<05:38,  1.19s/it, loss=0.148, v_num=75]Epoch 0:  43%|███████████████████████████████████████████▎                                                         | 213/497 [04:12<05:37,  1.19s/it, loss=0.148, v_num=75]Epoch 0:  43%|███████████████████████████████████████████▎                                                         | 213/497 [04:12<05:37,  1.19s/it, loss=0.147, v_num=75]Epoch 0:  43%|███████████████████████████████████████████▍                                                         | 214/497 [04:14<05:35,  1.19s/it, loss=0.147, v_num=75]Epoch 0:  43%|███████████████████████████████████████████▍                                                         | 214/497 [04:14<05:35,  1.19s/it, loss=0.133, v_num=75]Epoch 0:  43%|███████████████████████████████████████████▋                                                         | 215/497 [04:15<05:34,  1.19s/it, loss=0.133, v_num=75]Epoch 0:  43%|███████████████████████████████████████████▋                                                         | 215/497 [04:15<05:34,  1.19s/it, loss=0.139, v_num=75]Epoch 0:  43%|███████████████████████████████████████████▉                                                         | 216/497 [04:16<05:33,  1.19s/it, loss=0.139, v_num=75]Epoch 0:  43%|████████████████████████████████████████████▎                                                         | 216/497 [04:16<05:33,  1.19s/it, loss=0.14, v_num=75]Epoch 0:  44%|████████████████████████████████████████████▌                                                         | 217/497 [04:17<05:32,  1.19s/it, loss=0.14, v_num=75]Epoch 0:  44%|████████████████████████████████████████████▌                                                         | 217/497 [04:17<05:32,  1.19s/it, loss=0.14, v_num=75]Epoch 0:  44%|████████████████████████████████████████████▋                                                         | 218/497 [04:18<05:31,  1.19s/it, loss=0.14, v_num=75]Epoch 0:  44%|████████████████████████████████████████████▎                                                        | 218/497 [04:18<05:31,  1.19s/it, loss=0.131, v_num=75]Epoch 0:  44%|████████████████████████████████████████████▌                                                        | 219/497 [04:20<05:30,  1.19s/it, loss=0.131, v_num=75]Epoch 0:  44%|████████████████████████████████████████████▌                                                        | 219/497 [04:20<05:30,  1.19s/it, loss=0.133, v_num=75]Epoch 0:  44%|████████████████████████████████████████████▋                                                        | 220/497 [04:21<05:28,  1.19s/it, loss=0.133, v_num=75]Epoch 0:  44%|████████████████████████████████████████████▋                                                        | 220/497 [04:21<05:28,  1.19s/it, loss=0.135, v_num=75]Epoch 0:  44%|████████████████████████████████████████████▉                                                        | 221/497 [04:22<05:27,  1.19s/it, loss=0.135, v_num=75]Epoch 0:  44%|████████████████████████████████████████████▉                                                        | 221/497 [04:22<05:27,  1.19s/it, loss=0.131, v_num=75]Epoch 0:  45%|█████████████████████████████████████████████                                                        | 222/497 [04:23<05:26,  1.19s/it, loss=0.131, v_num=75]Epoch 0:  45%|█████████████████████████████████████████████                                                        | 222/497 [04:23<05:26,  1.19s/it, loss=0.129, v_num=75]Epoch 0:  45%|█████████████████████████████████████████████▎                                                       | 223/497 [04:24<05:25,  1.19s/it, loss=0.129, v_num=75]Epoch 0:  45%|█████████████████████████████████████████████▎                                                       | 223/497 [04:24<05:25,  1.19s/it, loss=0.133, v_num=75]Epoch 0:  45%|█████████████████████████████████████████████▌                                                       | 224/497 [04:26<05:24,  1.19s/it, loss=0.133, v_num=75]Epoch 0:  45%|█████████████████████████████████████████████▌                                                       | 224/497 [04:26<05:24,  1.19s/it, loss=0.124, v_num=75]Epoch 0:  45%|█████████████████████████████████████████████▋                                                       | 225/497 [04:27<05:23,  1.19s/it, loss=0.124, v_num=75]Epoch 0:  45%|█████████████████████████████████████████████▋                                                       | 225/497 [04:27<05:23,  1.19s/it, loss=0.113, v_num=75]Epoch 0:  45%|█████████████████████████████████████████████▉                                                       | 226/497 [04:28<05:21,  1.19s/it, loss=0.113, v_num=75]Epoch 0:  45%|█████████████████████████████████████████████▉                                                       | 226/497 [04:28<05:21,  1.19s/it, loss=0.112, v_num=75]Epoch 0:  46%|██████████████████████████████████████████████▏                                                      | 227/497 [04:29<05:20,  1.19s/it, loss=0.112, v_num=75]Epoch 0:  46%|██████████████████████████████████████████████▏                                                      | 227/497 [04:29<05:20,  1.19s/it, loss=0.111, v_num=75]Epoch 0:  46%|██████████████████████████████████████████████▎                                                      | 228/497 [04:30<05:19,  1.19s/it, loss=0.111, v_num=75]Epoch 0:  46%|██████████████████████████████████████████████▎                                                      | 228/497 [04:30<05:19,  1.19s/it, loss=0.103, v_num=75]Epoch 0:  46%|██████████████████████████████████████████████▌                                                      | 229/497 [04:32<05:18,  1.19s/it, loss=0.103, v_num=75]Epoch 0:  46%|██████████████████████████████████████████████▌                                                      | 229/497 [04:32<05:18,  1.19s/it, loss=0.102, v_num=75]Epoch 0:  46%|██████████████████████████████████████████████▋                                                      | 230/497 [04:33<05:17,  1.19s/it, loss=0.102, v_num=75]Epoch 0:  46%|██████████████████████████████████████████████▎                                                     | 230/497 [04:33<05:17,  1.19s/it, loss=0.0979, v_num=75]Epoch 0:  46%|██████████████████████████████████████████████▍                                                     | 231/497 [04:34<05:16,  1.19s/it, loss=0.0979, v_num=75]Epoch 0:  46%|██████████████████████████████████████████████▍                                                     | 231/497 [04:34<05:16,  1.19s/it, loss=0.0952, v_num=75]Epoch 0:  47%|██████████████████████████████████████████████▋                                                     | 232/497 [04:35<05:14,  1.19s/it, loss=0.0952, v_num=75]Epoch 0:  47%|███████████████████████████████████████████████▏                                                     | 232/497 [04:35<05:14,  1.19s/it, loss=0.096, v_num=75]Epoch 0:  47%|███████████████████████████████████████████████▎                                                     | 233/497 [04:36<05:13,  1.19s/it, loss=0.096, v_num=75]Epoch 0:  47%|████████████████████████████████████████████████▎                                                      | 233/497 [04:36<05:13,  1.19s/it, loss=0.1, v_num=75]Epoch 0:  47%|████████████████████████████████████████████████▍                                                      | 234/497 [04:38<05:12,  1.19s/it, loss=0.1, v_num=75]Epoch 0:  47%|███████████████████████████████████████████████▌                                                     | 234/497 [04:38<05:12,  1.19s/it, loss=0.101, v_num=75]Epoch 0:  47%|███████████████████████████████████████████████▊                                                     | 235/497 [04:39<05:11,  1.19s/it, loss=0.101, v_num=75]Epoch 0:  47%|███████████████████████████████████████████████▎                                                    | 235/497 [04:39<05:11,  1.19s/it, loss=0.0976, v_num=75]Epoch 0:  47%|███████████████████████████████████████████████▍                                                    | 236/497 [04:40<05:10,  1.19s/it, loss=0.0976, v_num=75]Epoch 0:  47%|███████████████████████████████████████████████▍                                                    | 236/497 [04:40<05:10,  1.19s/it, loss=0.0984, v_num=75]Epoch 0:  48%|███████████████████████████████████████████████▋                                                    | 237/497 [04:41<05:09,  1.19s/it, loss=0.0984, v_num=75]Epoch 0:  48%|████████████████████████████████████████████████▏                                                    | 237/497 [04:41<05:09,  1.19s/it, loss=0.111, v_num=75]Epoch 0:  48%|████████████████████████████████████████████████▎                                                    | 238/497 [04:42<05:07,  1.19s/it, loss=0.111, v_num=75]Epoch 0:  48%|████████████████████████████████████████████████▎                                                    | 238/497 [04:42<05:07,  1.19s/it, loss=0.114, v_num=75]Epoch 0:  48%|████████████████████████████████████████████████▌                                                    | 239/497 [04:44<05:06,  1.19s/it, loss=0.114, v_num=75]Epoch 0:  48%|█████████████████████████████████████████████████                                                     | 239/497 [04:44<05:06,  1.19s/it, loss=0.11, v_num=75]Epoch 0:  48%|█████████████████████████████████████████████████▎                                                    | 240/497 [04:45<05:05,  1.19s/it, loss=0.11, v_num=75]Epoch 0:  48%|████████████████████████████████████████████████▊                                                    | 240/497 [04:45<05:05,  1.19s/it, loss=0.109, v_num=75]Epoch 0:  48%|████████████████████████████████████████████████▉                                                    | 241/497 [04:46<05:04,  1.19s/it, loss=0.109, v_num=75]Epoch 0:  48%|████████████████████████████████████████████████▉                                                    | 241/497 [04:46<05:04,  1.19s/it, loss=0.111, v_num=75]Epoch 0:  49%|█████████████████████████████████████████████████▏                                                   | 242/497 [04:47<05:03,  1.19s/it, loss=0.111, v_num=75]Epoch 0:  49%|█████████████████████████████████████████████████▏                                                   | 242/497 [04:47<05:03,  1.19s/it, loss=0.112, v_num=75]Epoch 0:  49%|█████████████████████████████████████████████████▍                                                   | 243/497 [04:48<05:02,  1.19s/it, loss=0.112, v_num=75]Epoch 0:  49%|█████████████████████████████████████████████████▊                                                    | 243/497 [04:48<05:02,  1.19s/it, loss=0.11, v_num=75]Epoch 0:  49%|██████████████████████████████████████████████████                                                    | 244/497 [04:50<05:00,  1.19s/it, loss=0.11, v_num=75]Epoch 0:  49%|█████████████████████████████████████████████████▌                                                   | 244/497 [04:50<05:00,  1.19s/it, loss=0.108, v_num=75]Epoch 0:  49%|█████████████████████████████████████████████████▊                                                   | 245/497 [04:51<04:59,  1.19s/it, loss=0.108, v_num=75]Epoch 0:  49%|█████████████████████████████████████████████████▊                                                   | 245/497 [04:51<04:59,  1.19s/it, loss=0.108, v_num=75]Epoch 0:  49%|█████████████████████████████████████████████████▉                                                   | 246/497 [04:52<04:58,  1.19s/it, loss=0.108, v_num=75]Epoch 0:  49%|█████████████████████████████████████████████████▉                                                   | 246/497 [04:52<04:58,  1.19s/it, loss=0.114, v_num=75]Epoch 0:  50%|██████████████████████████████████████████████████▏                                                  | 247/497 [04:53<04:57,  1.19s/it, loss=0.114, v_num=75]Epoch 0:  50%|██████████████████████████████████████████████████▏                                                  | 247/497 [04:53<04:57,  1.19s/it, loss=0.117, v_num=75]Epoch 0:  50%|██████████████████████████████████████████████████▍                                                  | 248/497 [04:54<04:56,  1.19s/it, loss=0.117, v_num=75]Epoch 0:  50%|██████████████████████████████████████████████████▍                                                  | 248/497 [04:54<04:56,  1.19s/it, loss=0.117, v_num=75]Epoch 0:  50%|██████████████████████████████████████████████████▌                                                  | 249/497 [04:56<04:54,  1.19s/it, loss=0.117, v_num=75]Epoch 0:  50%|██████████████████████████████████████████████████▌                                                  | 249/497 [04:56<04:54,  1.19s/it, loss=0.136, v_num=75]Epoch 0:  50%|██████████████████████████████████████████████████▊                                                  | 250/497 [04:57<04:53,  1.19s/it, loss=0.136, v_num=75]Epoch 0:  50%|██████████████████████████████████████████████████▊                                                  | 250/497 [04:57<04:53,  1.19s/it, loss=0.151, v_num=75]Epoch 0:  51%|███████████████████████████████████████████████████                                                  | 251/497 [04:58<04:52,  1.19s/it, loss=0.151, v_num=75]Epoch 0:  51%|███████████████████████████████████████████████████                                                  | 251/497 [04:58<04:52,  1.19s/it, loss=0.151, v_num=75]Epoch 0:  51%|███████████████████████████████████████████████████▏                                                 | 252/497 [04:59<04:51,  1.19s/it, loss=0.151, v_num=75]Epoch 0:  51%|███████████████████████████████████████████████████▏                                                 | 252/497 [04:59<04:51,  1.19s/it, loss=0.151, v_num=75]Epoch 0:  51%|███████████████████████████████████████████████████▍                                                 | 253/497 [05:01<04:50,  1.19s/it, loss=0.151, v_num=75]Epoch 0:  51%|███████████████████████████████████████████████████▍                                                 | 253/497 [05:01<04:50,  1.19s/it, loss=0.147, v_num=75]Epoch 0:  51%|███████████████████████████████████████████████████▌                                                 | 254/497 [05:02<04:49,  1.19s/it, loss=0.147, v_num=75]Epoch 0:  51%|███████████████████████████████████████████████████▌                                                 | 254/497 [05:02<04:49,  1.19s/it, loss=0.144, v_num=75]Epoch 0:  51%|███████████████████████████████████████████████████▊                                                 | 255/497 [05:03<04:47,  1.19s/it, loss=0.144, v_num=75]Epoch 0:  51%|███████████████████████████████████████████████████▊                                                 | 255/497 [05:03<04:47,  1.19s/it, loss=0.148, v_num=75]Epoch 0:  52%|████████████████████████████████████████████████████                                                 | 256/497 [05:04<04:46,  1.19s/it, loss=0.148, v_num=75]Epoch 0:  52%|████████████████████████████████████████████████████                                                 | 256/497 [05:04<04:46,  1.19s/it, loss=0.149, v_num=75]Epoch 0:  52%|████████████████████████████████████████████████████▏                                                | 257/497 [05:05<04:45,  1.19s/it, loss=0.149, v_num=75]Epoch 0:  52%|████████████████████████████████████████████████████▋                                                 | 257/497 [05:05<04:45,  1.19s/it, loss=0.14, v_num=75]Epoch 0:  52%|████████████████████████████████████████████████████▉                                                 | 258/497 [05:07<04:44,  1.19s/it, loss=0.14, v_num=75]Epoch 0:  52%|████████████████████████████████████████████████████▍                                                | 258/497 [05:07<04:44,  1.19s/it, loss=0.138, v_num=75]Epoch 0:  52%|████████████████████████████████████████████████████▋                                                | 259/497 [05:08<04:43,  1.19s/it, loss=0.138, v_num=75]Epoch 0:  52%|████████████████████████████████████████████████████▋                                                | 259/497 [05:08<04:43,  1.19s/it, loss=0.143, v_num=75]Epoch 0:  52%|████████████████████████████████████████████████████▊                                                | 260/497 [05:09<04:42,  1.19s/it, loss=0.143, v_num=75]Epoch 0:  52%|████████████████████████████████████████████████████▊                                                | 260/497 [05:09<04:42,  1.19s/it, loss=0.144, v_num=75]Epoch 0:  53%|█████████████████████████████████████████████████████                                                | 261/497 [05:10<04:40,  1.19s/it, loss=0.144, v_num=75]Epoch 0:  53%|█████████████████████████████████████████████████████                                                | 261/497 [05:10<04:40,  1.19s/it, loss=0.145, v_num=75]Epoch 0:  53%|█████████████████████████████████████████████████████▏                                               | 262/497 [05:11<04:39,  1.19s/it, loss=0.145, v_num=75]Epoch 0:  53%|█████████████████████████████████████████████████████▏                                               | 262/497 [05:11<04:39,  1.19s/it, loss=0.147, v_num=75]Epoch 0:  53%|█████████████████████████████████████████████████████▍                                               | 263/497 [05:13<04:38,  1.19s/it, loss=0.147, v_num=75]Epoch 0:  53%|█████████████████████████████████████████████████████▍                                               | 263/497 [05:13<04:38,  1.19s/it, loss=0.151, v_num=75]Epoch 0:  53%|█████████████████████████████████████████████████████▋                                               | 264/497 [05:14<04:37,  1.19s/it, loss=0.151, v_num=75]Epoch 0:  53%|█████████████████████████████████████████████████████▋                                               | 264/497 [05:14<04:37,  1.19s/it, loss=0.156, v_num=75]Epoch 0:  53%|█████████████████████████████████████████████████████▊                                               | 265/497 [05:15<04:36,  1.19s/it, loss=0.156, v_num=75]Epoch 0:  53%|█████████████████████████████████████████████████████▊                                               | 265/497 [05:15<04:36,  1.19s/it, loss=0.162, v_num=75]Epoch 0:  54%|██████████████████████████████████████████████████████                                               | 266/497 [05:16<04:34,  1.19s/it, loss=0.162, v_num=75]Epoch 0:  54%|██████████████████████████████████████████████████████                                               | 266/497 [05:16<04:34,  1.19s/it, loss=0.156, v_num=75]Epoch 0:  54%|██████████████████████████████████████████████████████▎                                              | 267/497 [05:17<04:33,  1.19s/it, loss=0.156, v_num=75]Epoch 0:  54%|██████████████████████████████████████████████████████▎                                              | 267/497 [05:17<04:33,  1.19s/it, loss=0.154, v_num=75]Epoch 0:  54%|██████████████████████████████████████████████████████▍                                              | 268/497 [05:19<04:32,  1.19s/it, loss=0.154, v_num=75]Epoch 0:  54%|██████████████████████████████████████████████████████▍                                              | 268/497 [05:19<04:32,  1.19s/it, loss=0.156, v_num=75]Epoch 0:  54%|██████████████████████████████████████████████████████▋                                              | 269/497 [05:20<04:31,  1.19s/it, loss=0.156, v_num=75]Epoch 0:  54%|██████████████████████████████████████████████████████▋                                              | 269/497 [05:20<04:31,  1.19s/it, loss=0.138, v_num=75]Epoch 0:  54%|██████████████████████████████████████████████████████▊                                              | 270/497 [05:21<04:30,  1.19s/it, loss=0.138, v_num=75]Epoch 0:  54%|██████████████████████████████████████████████████████▊                                              | 270/497 [05:21<04:30,  1.19s/it, loss=0.125, v_num=75]Epoch 0:  55%|███████████████████████████████████████████████████████                                              | 271/497 [05:22<04:29,  1.19s/it, loss=0.125, v_num=75]Epoch 0:  55%|███████████████████████████████████████████████████████                                              | 271/497 [05:22<04:29,  1.19s/it, loss=0.125, v_num=75]Epoch 0:  55%|███████████████████████████████████████████████████████▎                                             | 272/497 [05:23<04:27,  1.19s/it, loss=0.125, v_num=75]Epoch 0:  55%|███████████████████████████████████████████████████████▎                                             | 272/497 [05:23<04:27,  1.19s/it, loss=0.121, v_num=75]Epoch 0:  55%|███████████████████████████████████████████████████████▍                                             | 273/497 [05:25<04:26,  1.19s/it, loss=0.121, v_num=75]Epoch 0:  55%|███████████████████████████████████████████████████████▍                                             | 273/497 [05:25<04:26,  1.19s/it, loss=0.119, v_num=75]Epoch 0:  55%|███████████████████████████████████████████████████████▋                                             | 274/497 [05:26<04:25,  1.19s/it, loss=0.119, v_num=75]Epoch 0:  55%|███████████████████████████████████████████████████████▋                                             | 274/497 [05:26<04:25,  1.19s/it, loss=0.121, v_num=75]Epoch 0:  55%|███████████████████████████████████████████████████████▉                                             | 275/497 [05:27<04:24,  1.19s/it, loss=0.121, v_num=75]Epoch 0:  55%|███████████████████████████████████████████████████████▉                                             | 275/497 [05:27<04:24,  1.19s/it, loss=0.126, v_num=75]Epoch 0:  56%|████████████████████████████████████████████████████████                                             | 276/497 [05:28<04:23,  1.19s/it, loss=0.126, v_num=75]Epoch 0:  56%|████████████████████████████████████████████████████████                                             | 276/497 [05:28<04:23,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  56%|████████████████████████████████████████████████████████▎                                            | 277/497 [05:29<04:22,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  56%|████████████████████████████████████████████████████████▊                                             | 277/497 [05:29<04:22,  1.19s/it, loss=0.13, v_num=75]Epoch 0:  56%|█████████████████████████████████████████████████████████                                             | 278/497 [05:31<04:20,  1.19s/it, loss=0.13, v_num=75]Epoch 0:  56%|█████████████████████████████████████████████████████████                                             | 278/497 [05:31<04:20,  1.19s/it, loss=0.13, v_num=75]Epoch 0:  56%|█████████████████████████████████████████████████████████▎                                            | 279/497 [05:32<04:19,  1.19s/it, loss=0.13, v_num=75]Epoch 0:  56%|████████████████████████████████████████████████████████▋                                            | 279/497 [05:32<04:19,  1.19s/it, loss=0.124, v_num=75]Epoch 0:  56%|████████████████████████████████████████████████████████▉                                            | 280/497 [05:33<04:18,  1.19s/it, loss=0.124, v_num=75]Epoch 0:  56%|████████████████████████████████████████████████████████▉                                            | 280/497 [05:33<04:18,  1.19s/it, loss=0.123, v_num=75]Epoch 0:  57%|█████████████████████████████████████████████████████████                                            | 281/497 [05:34<04:17,  1.19s/it, loss=0.123, v_num=75]Epoch 0:  57%|█████████████████████████████████████████████████████████                                            | 281/497 [05:34<04:17,  1.19s/it, loss=0.123, v_num=75]Epoch 0:  57%|█████████████████████████████████████████████████████████▎                                           | 282/497 [05:35<04:16,  1.19s/it, loss=0.123, v_num=75]Epoch 0:  57%|█████████████████████████████████████████████████████████▎                                           | 282/497 [05:35<04:16,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  57%|█████████████████████████████████████████████████████████▌                                           | 283/497 [05:37<04:14,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  57%|██████████████████████████████████████████████████████████                                            | 283/497 [05:37<04:14,  1.19s/it, loss=0.12, v_num=75]Epoch 0:  57%|██████████████████████████████████████████████████████████▎                                           | 284/497 [05:38<04:13,  1.19s/it, loss=0.12, v_num=75]Epoch 0:  57%|█████████████████████████████████████████████████████████▋                                           | 284/497 [05:38<04:13,  1.19s/it, loss=0.114, v_num=75]Epoch 0:  57%|█████████████████████████████████████████████████████████▉                                           | 285/497 [05:39<04:12,  1.19s/it, loss=0.114, v_num=75]Epoch 0:  57%|█████████████████████████████████████████████████████████▉                                           | 285/497 [05:39<04:12,  1.19s/it, loss=0.107, v_num=75]Epoch 0:  58%|██████████████████████████████████████████████████████████                                           | 286/497 [05:40<04:11,  1.19s/it, loss=0.107, v_num=75]Epoch 0:  58%|██████████████████████████████████████████████████████████                                           | 286/497 [05:40<04:11,  1.19s/it, loss=0.107, v_num=75]Epoch 0:  58%|██████████████████████████████████████████████████████████▎                                          | 287/497 [05:41<04:10,  1.19s/it, loss=0.107, v_num=75]Epoch 0:  58%|██████████████████████████████████████████████████████████▎                                          | 287/497 [05:41<04:10,  1.19s/it, loss=0.109, v_num=75]Epoch 0:  58%|██████████████████████████████████████████████████████████▌                                          | 288/497 [05:43<04:09,  1.19s/it, loss=0.109, v_num=75]Epoch 0:  58%|██████████████████████████████████████████████████████████▌                                          | 288/497 [05:43<04:09,  1.19s/it, loss=0.111, v_num=75]Epoch 0:  58%|██████████████████████████████████████████████████████████▋                                          | 289/497 [05:44<04:07,  1.19s/it, loss=0.111, v_num=75]Epoch 0:  58%|███████████████████████████████████████████████████████████▎                                          | 289/497 [05:44<04:07,  1.19s/it, loss=0.11, v_num=75]Epoch 0:  58%|███████████████████████████████████████████████████████████▌                                          | 290/497 [05:45<04:06,  1.19s/it, loss=0.11, v_num=75]Epoch 0:  58%|██████████████████████████████████████████████████████████▉                                          | 290/497 [05:45<04:06,  1.19s/it, loss=0.117, v_num=75]Epoch 0:  59%|███████████████████████████████████████████████████████████▏                                         | 291/497 [05:46<04:05,  1.19s/it, loss=0.117, v_num=75]Epoch 0:  59%|███████████████████████████████████████████████████████████▏                                         | 291/497 [05:46<04:05,  1.19s/it, loss=0.118, v_num=75]Epoch 0:  59%|███████████████████████████████████████████████████████████▎                                         | 292/497 [05:47<04:04,  1.19s/it, loss=0.118, v_num=75]Epoch 0:  59%|███████████████████████████████████████████████████████████▎                                         | 292/497 [05:47<04:04,  1.19s/it, loss=0.119, v_num=75]Epoch 0:  59%|███████████████████████████████████████████████████████████▌                                         | 293/497 [05:49<04:03,  1.19s/it, loss=0.119, v_num=75]Epoch 0:  59%|████████████████████████████████████████████████████████████▏                                         | 293/497 [05:49<04:03,  1.19s/it, loss=0.12, v_num=75]Epoch 0:  59%|████████████████████████████████████████████████████████████▎                                         | 294/497 [05:50<04:01,  1.19s/it, loss=0.12, v_num=75]Epoch 0:  59%|███████████████████████████████████████████████████████████▋                                         | 294/497 [05:50<04:01,  1.19s/it, loss=0.117, v_num=75]Epoch 0:  59%|███████████████████████████████████████████████████████████▉                                         | 295/497 [05:51<04:00,  1.19s/it, loss=0.117, v_num=75]Epoch 0:  59%|███████████████████████████████████████████████████████████▉                                         | 295/497 [05:51<04:00,  1.19s/it, loss=0.105, v_num=75]Epoch 0:  60%|████████████████████████████████████████████████████████████▏                                        | 296/497 [05:52<03:59,  1.19s/it, loss=0.105, v_num=75]Epoch 0:  60%|████████████████████████████████████████████████████████████▏                                        | 296/497 [05:52<03:59,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  60%|████████████████████████████████████████████████████████████▎                                        | 297/497 [05:53<03:58,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  60%|████████████████████████████████████████████████████████████▉                                         | 297/497 [05:53<03:58,  1.19s/it, loss=0.11, v_num=75]Epoch 0:  60%|█████████████████████████████████████████████████████████████▏                                        | 298/497 [05:55<03:57,  1.19s/it, loss=0.11, v_num=75]Epoch 0:  60%|████████████████████████████████████████████████████████████▌                                        | 298/497 [05:55<03:57,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  60%|████████████████████████████████████████████████████████████▊                                        | 299/497 [05:56<03:56,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  60%|████████████████████████████████████████████████████████████▊                                        | 299/497 [05:56<03:56,  1.19s/it, loss=0.124, v_num=75]Epoch 0:  60%|████████████████████████████████████████████████████████████▉                                        | 300/497 [05:57<03:54,  1.19s/it, loss=0.124, v_num=75]Epoch 0:  60%|████████████████████████████████████████████████████████████▉                                        | 300/497 [05:57<03:54,  1.19s/it, loss=0.127, v_num=75]Epoch 0:  61%|█████████████████████████████████████████████████████████████▏                                       | 301/497 [05:58<03:53,  1.19s/it, loss=0.127, v_num=75]Epoch 0:  61%|█████████████████████████████████████████████████████████████▏                                       | 301/497 [05:58<03:53,  1.19s/it, loss=0.124, v_num=75]Epoch 0:  61%|█████████████████████████████████████████████████████████████▎                                       | 302/497 [06:00<03:52,  1.19s/it, loss=0.124, v_num=75]Epoch 0:  61%|█████████████████████████████████████████████████████████████▎                                       | 302/497 [06:00<03:52,  1.19s/it, loss=0.125, v_num=75]Epoch 0:  61%|█████████████████████████████████████████████████████████████▌                                       | 303/497 [06:01<03:51,  1.19s/it, loss=0.125, v_num=75]Epoch 0:  61%|█████████████████████████████████████████████████████████████▌                                       | 303/497 [06:01<03:51,  1.19s/it, loss=0.123, v_num=75]Epoch 0:  61%|█████████████████████████████████████████████████████████████▊                                       | 304/497 [06:02<03:50,  1.19s/it, loss=0.123, v_num=75]Epoch 0:  61%|█████████████████████████████████████████████████████████████▊                                       | 304/497 [06:02<03:50,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  61%|█████████████████████████████████████████████████████████████▉                                       | 305/497 [06:03<03:48,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  61%|█████████████████████████████████████████████████████████████▉                                       | 305/497 [06:03<03:48,  1.19s/it, loss=0.126, v_num=75]Epoch 0:  62%|██████████████████████████████████████████████████████████████▏                                      | 306/497 [06:04<03:47,  1.19s/it, loss=0.126, v_num=75]Epoch 0:  62%|██████████████████████████████████████████████████████████████▏                                      | 306/497 [06:04<03:47,  1.19s/it, loss=0.128, v_num=75]Epoch 0:  62%|██████████████████████████████████████████████████████████████▍                                      | 307/497 [06:06<03:46,  1.19s/it, loss=0.128, v_num=75]Epoch 0:  62%|██████████████████████████████████████████████████████████████▍                                      | 307/497 [06:06<03:46,  1.19s/it, loss=0.126, v_num=75]Epoch 0:  62%|██████████████████████████████████████████████████████████████▌                                      | 308/497 [06:07<03:45,  1.19s/it, loss=0.126, v_num=75]Epoch 0:  62%|██████████████████████████████████████████████████████████████▌                                      | 308/497 [06:07<03:45,  1.19s/it, loss=0.124, v_num=75]Epoch 0:  62%|██████████████████████████████████████████████████████████████▊                                      | 309/497 [06:08<03:44,  1.19s/it, loss=0.124, v_num=75]Epoch 0:  62%|███████████████████████████████████████████████████████████████▍                                      | 309/497 [06:08<03:44,  1.19s/it, loss=0.13, v_num=75]Epoch 0:  62%|███████████████████████████████████████████████████████████████▌                                      | 310/497 [06:09<03:42,  1.19s/it, loss=0.13, v_num=75]Epoch 0:  62%|██████████████████████████████████████████████████████████████▉                                      | 310/497 [06:09<03:42,  1.19s/it, loss=0.121, v_num=75]Epoch 0:  63%|███████████████████████████████████████████████████████████████▏                                     | 311/497 [06:10<03:41,  1.19s/it, loss=0.121, v_num=75]Epoch 0:  63%|███████████████████████████████████████████████████████████████▊                                      | 311/497 [06:10<03:41,  1.19s/it, loss=0.12, v_num=75]Epoch 0:  63%|████████████████████████████████████████████████████████████████                                      | 312/497 [06:12<03:40,  1.19s/it, loss=0.12, v_num=75]Epoch 0:  63%|███████████████████████████████████████████████████████████████▍                                     | 312/497 [06:12<03:40,  1.19s/it, loss=0.118, v_num=75]Epoch 0:  63%|███████████████████████████████████████████████████████████████▌                                     | 313/497 [06:13<03:39,  1.19s/it, loss=0.118, v_num=75]Epoch 0:  63%|███████████████████████████████████████████████████████████████▌                                     | 313/497 [06:13<03:39,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  63%|███████████████████████████████████████████████████████████████▊                                     | 314/497 [06:14<03:38,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  63%|███████████████████████████████████████████████████████████████▊                                     | 314/497 [06:14<03:38,  1.19s/it, loss=0.121, v_num=75]Epoch 0:  63%|████████████████████████████████████████████████████████████████                                     | 315/497 [06:15<03:37,  1.19s/it, loss=0.121, v_num=75]Epoch 0:  63%|████████████████████████████████████████████████████████████████                                     | 315/497 [06:15<03:37,  1.19s/it, loss=0.135, v_num=75]Epoch 0:  64%|████████████████████████████████████████████████████████████████▏                                    | 316/497 [06:16<03:35,  1.19s/it, loss=0.135, v_num=75]Epoch 0:  64%|████████████████████████████████████████████████████████████████▊                                     | 316/497 [06:16<03:35,  1.19s/it, loss=0.12, v_num=75]Epoch 0:  64%|█████████████████████████████████████████████████████████████████                                     | 317/497 [06:18<03:34,  1.19s/it, loss=0.12, v_num=75]Epoch 0:  64%|████████████████████████████████████████████████████████████████▍                                    | 317/497 [06:18<03:34,  1.19s/it, loss=0.123, v_num=75]Epoch 0:  64%|████████████████████████████████████████████████████████████████▌                                    | 318/497 [06:19<03:33,  1.19s/it, loss=0.123, v_num=75]Epoch 0:  64%|████████████████████████████████████████████████████████████████▌                                    | 318/497 [06:19<03:33,  1.19s/it, loss=0.115, v_num=75]Epoch 0:  64%|████████████████████████████████████████████████████████████████▊                                    | 319/497 [06:20<03:32,  1.19s/it, loss=0.115, v_num=75]Epoch 0:  64%|████████████████████████████████████████████████████████████████▊                                    | 319/497 [06:20<03:32,  1.19s/it, loss=0.141, v_num=75]Epoch 0:  64%|█████████████████████████████████████████████████████████████████                                    | 320/497 [06:21<03:31,  1.19s/it, loss=0.141, v_num=75]Epoch 0:  64%|█████████████████████████████████████████████████████████████████▋                                    | 320/497 [06:21<03:31,  1.19s/it, loss=0.14, v_num=75]Epoch 0:  65%|█████████████████████████████████████████████████████████████████▉                                    | 321/497 [06:22<03:29,  1.19s/it, loss=0.14, v_num=75]Epoch 0:  65%|█████████████████████████████████████████████████████████████████▏                                   | 321/497 [06:22<03:29,  1.19s/it, loss=0.143, v_num=75]Epoch 0:  65%|█████████████████████████████████████████████████████████████████▍                                   | 322/497 [06:24<03:28,  1.19s/it, loss=0.143, v_num=75]Epoch 0:  65%|█████████████████████████████████████████████████████████████████▍                                   | 322/497 [06:24<03:28,  1.19s/it, loss=0.146, v_num=75]Epoch 0:  65%|█████████████████████████████████████████████████████████████████▋                                   | 323/497 [06:25<03:27,  1.19s/it, loss=0.146, v_num=75]Epoch 0:  65%|█████████████████████████████████████████████████████████████████▋                                   | 323/497 [06:25<03:27,  1.19s/it, loss=0.151, v_num=75]Epoch 0:  65%|█████████████████████████████████████████████████████████████████▊                                   | 324/497 [06:26<03:26,  1.19s/it, loss=0.151, v_num=75]Epoch 0:  65%|█████████████████████████████████████████████████████████████████▊                                   | 324/497 [06:26<03:26,  1.19s/it, loss=0.156, v_num=75]Epoch 0:  65%|██████████████████████████████████████████████████████████████████                                   | 325/497 [06:27<03:25,  1.19s/it, loss=0.156, v_num=75]Epoch 0:  65%|██████████████████████████████████████████████████████████████████                                   | 325/497 [06:27<03:25,  1.19s/it, loss=0.157, v_num=75]Epoch 0:  66%|██████████████████████████████████████████████████████████████████▏                                  | 326/497 [06:28<03:24,  1.19s/it, loss=0.157, v_num=75]Epoch 0:  66%|██████████████████████████████████████████████████████████████████▏                                  | 326/497 [06:28<03:24,  1.19s/it, loss=0.158, v_num=75]Epoch 0:  66%|██████████████████████████████████████████████████████████████████▍                                  | 327/497 [06:30<03:22,  1.19s/it, loss=0.158, v_num=75]Epoch 0:  66%|██████████████████████████████████████████████████████████████████▍                                  | 327/497 [06:30<03:22,  1.19s/it, loss=0.157, v_num=75]Epoch 0:  66%|██████████████████████████████████████████████████████████████████▋                                  | 328/497 [06:31<03:21,  1.19s/it, loss=0.157, v_num=75]Epoch 0:  66%|██████████████████████████████████████████████████████████████████▋                                  | 328/497 [06:31<03:21,  1.19s/it, loss=0.157, v_num=75]Epoch 0:  66%|██████████████████████████████████████████████████████████████████▊                                  | 329/497 [06:32<03:20,  1.19s/it, loss=0.157, v_num=75]Epoch 0:  66%|██████████████████████████████████████████████████████████████████▊                                  | 329/497 [06:32<03:20,  1.19s/it, loss=0.155, v_num=75]Epoch 0:  66%|███████████████████████████████████████████████████████████████████                                  | 330/497 [06:33<03:19,  1.19s/it, loss=0.155, v_num=75]Epoch 0:  66%|███████████████████████████████████████████████████████████████████                                  | 330/497 [06:33<03:19,  1.19s/it, loss=0.158, v_num=75]Epoch 0:  67%|███████████████████████████████████████████████████████████████████▎                                 | 331/497 [06:34<03:18,  1.19s/it, loss=0.158, v_num=75]Epoch 0:  67%|███████████████████████████████████████████████████████████████████▎                                 | 331/497 [06:34<03:18,  1.19s/it, loss=0.158, v_num=75]Epoch 0:  67%|███████████████████████████████████████████████████████████████████▍                                 | 332/497 [06:36<03:16,  1.19s/it, loss=0.158, v_num=75]Epoch 0:  67%|███████████████████████████████████████████████████████████████████▍                                 | 332/497 [06:36<03:16,  1.19s/it, loss=0.159, v_num=75]Epoch 0:  67%|███████████████████████████████████████████████████████████████████▋                                 | 333/497 [06:37<03:15,  1.19s/it, loss=0.159, v_num=75]Epoch 0:  67%|███████████████████████████████████████████████████████████████████▋                                 | 333/497 [06:37<03:15,  1.19s/it, loss=0.158, v_num=75]Epoch 0:  67%|███████████████████████████████████████████████████████████████████▉                                 | 334/497 [06:38<03:14,  1.19s/it, loss=0.158, v_num=75]Epoch 0:  67%|████████████████████████████████████████████████████████████████████▌                                 | 334/497 [06:38<03:14,  1.19s/it, loss=0.16, v_num=75]Epoch 0:  67%|████████████████████████████████████████████████████████████████████▊                                 | 335/497 [06:39<03:13,  1.19s/it, loss=0.16, v_num=75]Epoch 0:  67%|████████████████████████████████████████████████████████████████████                                 | 335/497 [06:39<03:13,  1.19s/it, loss=0.147, v_num=75]Epoch 0:  68%|████████████████████████████████████████████████████████████████████▎                                | 336/497 [06:40<03:12,  1.19s/it, loss=0.147, v_num=75]Epoch 0:  68%|████████████████████████████████████████████████████████████████████▎                                | 336/497 [06:40<03:12,  1.19s/it, loss=0.148, v_num=75]Epoch 0:  68%|████████████████████████████████████████████████████████████████████▍                                | 337/497 [06:42<03:10,  1.19s/it, loss=0.148, v_num=75]Epoch 0:  68%|████████████████████████████████████████████████████████████████████▍                                | 337/497 [06:42<03:10,  1.19s/it, loss=0.147, v_num=75]Epoch 0:  68%|████████████████████████████████████████████████████████████████████▋                                | 338/497 [06:43<03:09,  1.19s/it, loss=0.147, v_num=75]Epoch 0:  68%|████████████████████████████████████████████████████████████████████▋                                | 338/497 [06:43<03:09,  1.19s/it, loss=0.156, v_num=75]Epoch 0:  68%|████████████████████████████████████████████████████████████████████▉                                | 339/497 [06:44<03:08,  1.19s/it, loss=0.156, v_num=75]Epoch 0:  68%|████████████████████████████████████████████████████████████████████▉                                | 339/497 [06:44<03:08,  1.19s/it, loss=0.128, v_num=75]Epoch 0:  68%|█████████████████████████████████████████████████████████████████████                                | 340/497 [06:45<03:07,  1.19s/it, loss=0.128, v_num=75]Epoch 0:  68%|█████████████████████████████████████████████████████████████████████                                | 340/497 [06:45<03:07,  1.19s/it, loss=0.134, v_num=75]Epoch 0:  69%|█████████████████████████████████████████████████████████████████████▎                               | 341/497 [06:46<03:06,  1.19s/it, loss=0.134, v_num=75]Epoch 0:  69%|█████████████████████████████████████████████████████████████████████▎                               | 341/497 [06:46<03:06,  1.19s/it, loss=0.129, v_num=75]Epoch 0:  69%|█████████████████████████████████████████████████████████████████████▌                               | 342/497 [06:48<03:04,  1.19s/it, loss=0.129, v_num=75]Epoch 0:  69%|█████████████████████████████████████████████████████████████████████▌                               | 342/497 [06:48<03:04,  1.19s/it, loss=0.128, v_num=75]Epoch 0:  69%|█████████████████████████████████████████████████████████████████████▋                               | 343/497 [06:49<03:03,  1.19s/it, loss=0.128, v_num=75]Epoch 0:  69%|█████████████████████████████████████████████████████████████████████▋                               | 343/497 [06:49<03:03,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  69%|█████████████████████████████████████████████████████████████████████▉                               | 344/497 [06:50<03:02,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  69%|█████████████████████████████████████████████████████████████████████▉                               | 344/497 [06:50<03:02,  1.19s/it, loss=0.117, v_num=75]Epoch 0:  69%|██████████████████████████████████████████████████████████████████████                               | 345/497 [06:51<03:01,  1.19s/it, loss=0.117, v_num=75]Epoch 0:  69%|██████████████████████████████████████████████████████████████████████                               | 345/497 [06:51<03:01,  1.19s/it, loss=0.119, v_num=75]Epoch 0:  70%|██████████████████████████████████████████████████████████████████████▎                              | 346/497 [06:52<03:00,  1.19s/it, loss=0.119, v_num=75]Epoch 0:  70%|██████████████████████████████████████████████████████████████████████▎                              | 346/497 [06:52<03:00,  1.19s/it, loss=0.116, v_num=75]Epoch 0:  70%|██████████████████████████████████████████████████████████████████████▌                              | 347/497 [06:54<02:59,  1.19s/it, loss=0.116, v_num=75]Epoch 0:  70%|██████████████████████████████████████████████████████████████████████▌                              | 347/497 [06:54<02:59,  1.19s/it, loss=0.112, v_num=75]Epoch 0:  70%|██████████████████████████████████████████████████████████████████████▋                              | 348/497 [06:55<02:57,  1.19s/it, loss=0.112, v_num=75]Epoch 0:  70%|██████████████████████████████████████████████████████████████████████▋                              | 348/497 [06:55<02:57,  1.19s/it, loss=0.115, v_num=75]Epoch 0:  70%|██████████████████████████████████████████████████████████████████████▉                              | 349/497 [06:56<02:56,  1.19s/it, loss=0.115, v_num=75]Epoch 0:  70%|██████████████████████████████████████████████████████████████████████▉                              | 349/497 [06:56<02:56,  1.19s/it, loss=0.112, v_num=75]Epoch 0:  70%|███████████████████████████████████████████████████████████████████████▏                             | 350/497 [06:57<02:55,  1.19s/it, loss=0.112, v_num=75]Epoch 0:  70%|███████████████████████████████████████████████████████████████████████▏                             | 350/497 [06:57<02:55,  1.19s/it, loss=0.113, v_num=75]Epoch 0:  71%|███████████████████████████████████████████████████████████████████████▎                             | 351/497 [06:58<02:54,  1.19s/it, loss=0.113, v_num=75]Epoch 0:  71%|███████████████████████████████████████████████████████████████████████▎                             | 351/497 [06:58<02:54,  1.19s/it, loss=0.114, v_num=75]Epoch 0:  71%|███████████████████████████████████████████████████████████████████████▌                             | 352/497 [07:00<02:53,  1.19s/it, loss=0.114, v_num=75]Epoch 0:  71%|███████████████████████████████████████████████████████████████████████▌                             | 352/497 [07:00<02:53,  1.19s/it, loss=0.135, v_num=75]Epoch 0:  71%|███████████████████████████████████████████████████████████████████████▋                             | 353/497 [07:01<02:51,  1.19s/it, loss=0.135, v_num=75]Epoch 0:  71%|███████████████████████████████████████████████████████████████████████▋                             | 353/497 [07:01<02:51,  1.19s/it, loss=0.142, v_num=75]Epoch 0:  71%|███████████████████████████████████████████████████████████████████████▉                             | 354/497 [07:02<02:50,  1.19s/it, loss=0.142, v_num=75]Epoch 0:  71%|███████████████████████████████████████████████████████████████████████▉                             | 354/497 [07:02<02:50,  1.19s/it, loss=0.146, v_num=75]Epoch 0:  71%|████████████████████████████████████████████████████████████████████████▏                            | 355/497 [07:03<02:49,  1.19s/it, loss=0.146, v_num=75]Epoch 0:  71%|████████████████████████████████████████████████████████████████████████▏                            | 355/497 [07:03<02:49,  1.19s/it, loss=0.149, v_num=75]Epoch 0:  72%|████████████████████████████████████████████████████████████████████████▎                            | 356/497 [07:04<02:48,  1.19s/it, loss=0.149, v_num=75]Epoch 0:  72%|█████████████████████████████████████████████████████████████████████████                             | 356/497 [07:04<02:48,  1.19s/it, loss=0.15, v_num=75]Epoch 0:  72%|█████████████████████████████████████████████████████████████████████████▎                            | 357/497 [07:06<02:47,  1.19s/it, loss=0.15, v_num=75]Epoch 0:  72%|████████████████████████████████████████████████████████████████████████▌                            | 357/497 [07:06<02:47,  1.19s/it, loss=0.152, v_num=75]Epoch 0:  72%|████████████████████████████████████████████████████████████████████████▊                            | 358/497 [07:07<02:45,  1.19s/it, loss=0.152, v_num=75]Epoch 0:  72%|████████████████████████████████████████████████████████████████████████▊                            | 358/497 [07:07<02:45,  1.19s/it, loss=0.141, v_num=75]Epoch 0:  72%|████████████████████████████████████████████████████████████████████████▉                            | 359/497 [07:08<02:44,  1.19s/it, loss=0.141, v_num=75]Epoch 0:  72%|████████████████████████████████████████████████████████████████████████▉                            | 359/497 [07:08<02:44,  1.19s/it, loss=0.141, v_num=75]Epoch 0:  72%|█████████████████████████████████████████████████████████████████████████▏                           | 360/497 [07:09<02:43,  1.19s/it, loss=0.141, v_num=75]Epoch 0:  72%|█████████████████████████████████████████████████████████████████████████▏                           | 360/497 [07:09<02:43,  1.19s/it, loss=0.137, v_num=75]Epoch 0:  73%|█████████████████████████████████████████████████████████████████████████▎                           | 361/497 [07:10<02:42,  1.19s/it, loss=0.137, v_num=75]Epoch 0:  73%|█████████████████████████████████████████████████████████████████████████▎                           | 361/497 [07:10<02:42,  1.19s/it, loss=0.139, v_num=75]Epoch 0:  73%|█████████████████████████████████████████████████████████████████████████▌                           | 362/497 [07:12<02:41,  1.19s/it, loss=0.139, v_num=75]Epoch 0:  73%|█████████████████████████████████████████████████████████████████████████▌                           | 362/497 [07:12<02:41,  1.19s/it, loss=0.138, v_num=75]Epoch 0:  73%|█████████████████████████████████████████████████████████████████████████▊                           | 363/497 [07:13<02:39,  1.19s/it, loss=0.138, v_num=75]Epoch 0:  73%|██████████████████████████████████████████████████████████████████████████▍                           | 363/497 [07:13<02:39,  1.19s/it, loss=0.14, v_num=75]Epoch 0:  73%|██████████████████████████████████████████████████████████████████████████▋                           | 364/497 [07:14<02:38,  1.19s/it, loss=0.14, v_num=75]Epoch 0:  73%|█████████████████████████████████████████████████████████████████████████▉                           | 364/497 [07:14<02:38,  1.19s/it, loss=0.147, v_num=75]Epoch 0:  73%|██████████████████████████████████████████████████████████████████████████▏                          | 365/497 [07:15<02:37,  1.19s/it, loss=0.147, v_num=75]Epoch 0:  73%|██████████████████████████████████████████████████████████████████████████▏                          | 365/497 [07:15<02:37,  1.19s/it, loss=0.144, v_num=75]Epoch 0:  74%|██████████████████████████████████████████████████████████████████████████▍                          | 366/497 [07:17<02:36,  1.19s/it, loss=0.144, v_num=75]Epoch 0:  74%|██████████████████████████████████████████████████████████████████████████▍                          | 366/497 [07:17<02:36,  1.19s/it, loss=0.148, v_num=75]Epoch 0:  74%|██████████████████████████████████████████████████████████████████████████▌                          | 367/497 [07:18<02:35,  1.19s/it, loss=0.148, v_num=75]Epoch 0:  74%|██████████████████████████████████████████████████████████████████████████▌                          | 367/497 [07:18<02:35,  1.19s/it, loss=0.154, v_num=75]Epoch 0:  74%|██████████████████████████████████████████████████████████████████████████▊                          | 368/497 [07:19<02:34,  1.19s/it, loss=0.154, v_num=75]Epoch 0:  74%|██████████████████████████████████████████████████████████████████████████▊                          | 368/497 [07:19<02:34,  1.19s/it, loss=0.151, v_num=75]Epoch 0:  74%|██████████████████████████████████████████████████████████████████████████▉                          | 369/497 [07:20<02:32,  1.19s/it, loss=0.151, v_num=75]Epoch 0:  74%|██████████████████████████████████████████████████████████████████████████▉                          | 369/497 [07:20<02:32,  1.19s/it, loss=0.154, v_num=75]Epoch 0:  74%|███████████████████████████████████████████████████████████████████████████▏                         | 370/497 [07:21<02:31,  1.19s/it, loss=0.154, v_num=75]Epoch 0:  74%|███████████████████████████████████████████████████████████████████████████▏                         | 370/497 [07:21<02:31,  1.19s/it, loss=0.154, v_num=75]Epoch 0:  75%|███████████████████████████████████████████████████████████████████████████▍                         | 371/497 [07:23<02:30,  1.19s/it, loss=0.154, v_num=75]Epoch 0:  75%|███████████████████████████████████████████████████████████████████████████▍                         | 371/497 [07:23<02:30,  1.19s/it, loss=0.162, v_num=75]Epoch 0:  75%|███████████████████████████████████████████████████████████████████████████▌                         | 372/497 [07:24<02:29,  1.19s/it, loss=0.162, v_num=75]Epoch 0:  75%|███████████████████████████████████████████████████████████████████████████▌                         | 372/497 [07:24<02:29,  1.19s/it, loss=0.141, v_num=75]Epoch 0:  75%|███████████████████████████████████████████████████████████████████████████▊                         | 373/497 [07:25<02:28,  1.19s/it, loss=0.141, v_num=75]Epoch 0:  75%|███████████████████████████████████████████████████████████████████████████▊                         | 373/497 [07:25<02:28,  1.19s/it, loss=0.141, v_num=75]Epoch 0:  75%|████████████████████████████████████████████████████████████████████████████                         | 374/497 [07:26<02:26,  1.19s/it, loss=0.141, v_num=75]Epoch 0:  75%|████████████████████████████████████████████████████████████████████████████                         | 374/497 [07:26<02:26,  1.19s/it, loss=0.136, v_num=75]Epoch 0:  75%|████████████████████████████████████████████████████████████████████████████▏                        | 375/497 [07:27<02:25,  1.19s/it, loss=0.136, v_num=75]Epoch 0:  75%|████████████████████████████████████████████████████████████████████████████▏                        | 375/497 [07:27<02:25,  1.19s/it, loss=0.133, v_num=75]Epoch 0:  76%|████████████████████████████████████████████████████████████████████████████▍                        | 376/497 [07:29<02:24,  1.19s/it, loss=0.133, v_num=75]Epoch 0:  76%|████████████████████████████████████████████████████████████████████████████▍                        | 376/497 [07:29<02:24,  1.19s/it, loss=0.129, v_num=75]Epoch 0:  76%|████████████████████████████████████████████████████████████████████████████▌                        | 377/497 [07:30<02:23,  1.19s/it, loss=0.129, v_num=75]Epoch 0:  76%|████████████████████████████████████████████████████████████████████████████▌                        | 377/497 [07:30<02:23,  1.19s/it, loss=0.126, v_num=75]Epoch 0:  76%|████████████████████████████████████████████████████████████████████████████▊                        | 378/497 [07:31<02:22,  1.19s/it, loss=0.126, v_num=75]Epoch 0:  76%|████████████████████████████████████████████████████████████████████████████▊                        | 378/497 [07:31<02:22,  1.19s/it, loss=0.123, v_num=75]Epoch 0:  76%|█████████████████████████████████████████████████████████████████████████████                        | 379/497 [07:32<02:20,  1.19s/it, loss=0.123, v_num=75]Epoch 0:  76%|█████████████████████████████████████████████████████████████████████████████                        | 379/497 [07:32<02:20,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  76%|█████████████████████████████████████████████████████████████████████████████▏                       | 380/497 [07:33<02:19,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  76%|█████████████████████████████████████████████████████████████████████████████▏                       | 380/497 [07:33<02:19,  1.19s/it, loss=0.119, v_num=75]Epoch 0:  77%|█████████████████████████████████████████████████████████████████████████████▍                       | 381/497 [07:35<02:18,  1.19s/it, loss=0.119, v_num=75]Epoch 0:  77%|█████████████████████████████████████████████████████████████████████████████▍                       | 381/497 [07:35<02:18,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  77%|█████████████████████████████████████████████████████████████████████████████▋                       | 382/497 [07:36<02:17,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  77%|█████████████████████████████████████████████████████████████████████████████▋                       | 382/497 [07:36<02:17,  1.19s/it, loss=0.121, v_num=75]Epoch 0:  77%|█████████████████████████████████████████████████████████████████████████████▊                       | 383/497 [07:37<02:16,  1.19s/it, loss=0.121, v_num=75]Epoch 0:  77%|█████████████████████████████████████████████████████████████████████████████▊                       | 383/497 [07:37<02:16,  1.19s/it, loss=0.121, v_num=75]Epoch 0:  77%|██████████████████████████████████████████████████████████████████████████████                       | 384/497 [07:38<02:14,  1.19s/it, loss=0.121, v_num=75]Epoch 0:  77%|██████████████████████████████████████████████████████████████████████████████                       | 384/497 [07:38<02:14,  1.19s/it, loss=0.135, v_num=75]Epoch 0:  77%|██████████████████████████████████████████████████████████████████████████████▏                      | 385/497 [07:39<02:13,  1.19s/it, loss=0.135, v_num=75]Epoch 0:  77%|███████████████████████████████████████████████████████████████████████████████                       | 385/497 [07:39<02:13,  1.19s/it, loss=0.13, v_num=75]Epoch 0:  78%|███████████████████████████████████████████████████████████████████████████████▏                      | 386/497 [07:41<02:12,  1.19s/it, loss=0.13, v_num=75]Epoch 0:  78%|██████████████████████████████████████████████████████████████████████████████▍                      | 386/497 [07:41<02:12,  1.19s/it, loss=0.137, v_num=75]Epoch 0:  78%|██████████████████████████████████████████████████████████████████████████████▋                      | 387/497 [07:42<02:11,  1.19s/it, loss=0.137, v_num=75]Epoch 0:  78%|██████████████████████████████████████████████████████████████████████████████▋                      | 387/497 [07:42<02:11,  1.19s/it, loss=0.133, v_num=75]Epoch 0:  78%|██████████████████████████████████████████████████████████████████████████████▊                      | 388/497 [07:43<02:10,  1.19s/it, loss=0.133, v_num=75]Epoch 0:  78%|██████████████████████████████████████████████████████████████████████████████▊                      | 388/497 [07:43<02:10,  1.19s/it, loss=0.132, v_num=75]Epoch 0:  78%|███████████████████████████████████████████████████████████████████████████████                      | 389/497 [07:44<02:09,  1.19s/it, loss=0.132, v_num=75]Epoch 0:  78%|███████████████████████████████████████████████████████████████████████████████                      | 389/497 [07:44<02:09,  1.19s/it, loss=0.131, v_num=75]Epoch 0:  78%|███████████████████████████████████████████████████████████████████████████████▎                     | 390/497 [07:45<02:07,  1.19s/it, loss=0.131, v_num=75]Epoch 0:  78%|███████████████████████████████████████████████████████████████████████████████▎                     | 390/497 [07:45<02:07,  1.19s/it, loss=0.127, v_num=75]Epoch 0:  79%|███████████████████████████████████████████████████████████████████████████████▍                     | 391/497 [07:47<02:06,  1.19s/it, loss=0.127, v_num=75]Epoch 0:  79%|███████████████████████████████████████████████████████████████████████████████▍                     | 391/497 [07:47<02:06,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  79%|███████████████████████████████████████████████████████████████████████████████▋                     | 392/497 [07:48<02:05,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  79%|███████████████████████████████████████████████████████████████████████████████▋                     | 392/497 [07:48<02:05,  1.19s/it, loss=0.123, v_num=75]Epoch 0:  79%|███████████████████████████████████████████████████████████████████████████████▊                     | 393/497 [07:49<02:04,  1.19s/it, loss=0.123, v_num=75]Epoch 0:  79%|███████████████████████████████████████████████████████████████████████████████▊                     | 393/497 [07:49<02:04,  1.19s/it, loss=0.114, v_num=75]Epoch 0:  79%|████████████████████████████████████████████████████████████████████████████████                     | 394/497 [07:50<02:03,  1.19s/it, loss=0.114, v_num=75]Epoch 0:  79%|████████████████████████████████████████████████████████████████████████████████                     | 394/497 [07:50<02:03,  1.19s/it, loss=0.114, v_num=75]Epoch 0:  79%|████████████████████████████████████████████████████████████████████████████████▎                    | 395/497 [07:51<02:01,  1.19s/it, loss=0.114, v_num=75]Epoch 0:  79%|████████████████████████████████████████████████████████████████████████████████▎                    | 395/497 [07:51<02:01,  1.19s/it, loss=0.115, v_num=75]Epoch 0:  80%|████████████████████████████████████████████████████████████████████████████████▍                    | 396/497 [07:53<02:00,  1.19s/it, loss=0.115, v_num=75]Epoch 0:  80%|████████████████████████████████████████████████████████████████████████████████▍                    | 396/497 [07:53<02:00,  1.19s/it, loss=0.121, v_num=75]Epoch 0:  80%|████████████████████████████████████████████████████████████████████████████████▋                    | 397/497 [07:54<01:59,  1.19s/it, loss=0.121, v_num=75]Epoch 0:  80%|█████████████████████████████████████████████████████████████████████████████████▍                    | 397/497 [07:54<01:59,  1.19s/it, loss=0.12, v_num=75]Epoch 0:  80%|█████████████████████████████████████████████████████████████████████████████████▋                    | 398/497 [07:55<01:58,  1.19s/it, loss=0.12, v_num=75]Epoch 0:  80%|████████████████████████████████████████████████████████████████████████████████▉                    | 398/497 [07:55<01:58,  1.19s/it, loss=0.125, v_num=75]Epoch 0:  80%|█████████████████████████████████████████████████████████████████████████████████                    | 399/497 [07:56<01:57,  1.19s/it, loss=0.125, v_num=75]Epoch 0:  80%|█████████████████████████████████████████████████████████████████████████████████                    | 399/497 [07:56<01:57,  1.19s/it, loss=0.128, v_num=75]Epoch 0:  80%|█████████████████████████████████████████████████████████████████████████████████▎                   | 400/497 [07:57<01:55,  1.19s/it, loss=0.128, v_num=75]Epoch 0:  80%|█████████████████████████████████████████████████████████████████████████████████▎                   | 400/497 [07:57<01:55,  1.19s/it, loss=0.128, v_num=75]Epoch 0:  81%|█████████████████████████████████████████████████████████████████████████████████▍                   | 401/497 [07:59<01:54,  1.19s/it, loss=0.128, v_num=75]Epoch 0:  81%|█████████████████████████████████████████████████████████████████████████████████▍                   | 401/497 [07:59<01:54,  1.19s/it, loss=0.123, v_num=75]Epoch 0:  81%|█████████████████████████████████████████████████████████████████████████████████▋                   | 402/497 [08:00<01:53,  1.19s/it, loss=0.123, v_num=75]Epoch 0:  81%|█████████████████████████████████████████████████████████████████████████████████▋                   | 402/497 [08:00<01:53,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  81%|█████████████████████████████████████████████████████████████████████████████████▉                   | 403/497 [08:01<01:52,  1.19s/it, loss=0.122, v_num=75]Epoch 0:  81%|█████████████████████████████████████████████████████████████████████████████████▉                   | 403/497 [08:01<01:52,  1.19s/it, loss=0.121, v_num=75]Epoch 0:  81%|██████████████████████████████████████████████████████████████████████████████████                   | 404/497 [08:02<01:51,  1.19s/it, loss=0.121, v_num=75]Epoch 0:  81%|█████████████████████████████████████████████████████████████████████████████████▎                  | 404/497 [08:02<01:51,  1.19s/it, loss=0.0995, v_num=75]Epoch 0:  81%|█████████████████████████████████████████████████████████████████████████████████▍                  | 405/497 [08:03<01:49,  1.19s/it, loss=0.0995, v_num=75]Epoch 0:  81%|███████████████████████████████████████████████████████████████████████████████████▉                   | 405/497 [08:03<01:49,  1.19s/it, loss=0.1, v_num=75]Adjusting learning rate of group 0 to 9.9384e-04.
Epoch 0:  82%|████████████████████████████████████████████████████████████████████████████████████▏                  | 406/497 [08:05<01:48,  1.19s/it, loss=0.1, v_num=75]Epoch 0:  82%|█████████████████████████████████████████████████████████████████████████████████▋                  | 406/497 [08:05<01:48,  1.19s/it, loss=0.0923, v_num=75]
Validation: 0it [00:00, ?it/s][A
Validation:   0%|                                                                                                                                   | 0/91 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|                                                                                                                      | 0/91 [00:00<?, ?it/s][Atorch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:   1%|█▏                                                                                                            | 1/91 [00:00<00:23,  3.84it/s][AEpoch 0:  82%|█████████████████████████████████████████████████████████████████████████████████▉                  | 407/497 [08:05<01:47,  1.19s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:   2%|██▍                                                                                                           | 2/91 [00:00<00:24,  3.65it/s][AEpoch 0:  82%|██████████████████████████████████████████████████████████████████████████████████                  | 408/497 [08:05<01:45,  1.19s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:   3%|███▋                                                                                                          | 3/91 [00:00<00:24,  3.59it/s][AEpoch 0:  82%|██████████████████████████████████████████████████████████████████████████████████▎                 | 409/497 [08:06<01:44,  1.19s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:   4%|████▊                                                                                                         | 4/91 [00:01<00:24,  3.57it/s][AEpoch 0:  82%|██████████████████████████████████████████████████████████████████████████████████▍                 | 410/497 [08:06<01:43,  1.19s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:   5%|██████                                                                                                        | 5/91 [00:01<00:24,  3.55it/s][AEpoch 0:  83%|██████████████████████████████████████████████████████████████████████████████████▋                 | 411/497 [08:06<01:41,  1.18s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:   7%|███████▎                                                                                                      | 6/91 [00:01<00:23,  3.54it/s][AEpoch 0:  83%|██████████████████████████████████████████████████████████████████████████████████▉                 | 412/497 [08:06<01:40,  1.18s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:   8%|████████▍                                                                                                     | 7/91 [00:01<00:23,  3.54it/s][AEpoch 0:  83%|███████████████████████████████████████████████████████████████████████████████████                 | 413/497 [08:07<01:39,  1.18s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:   9%|█████████▋                                                                                                    | 8/91 [00:02<00:23,  3.52it/s][AEpoch 0:  83%|███████████████████████████████████████████████████████████████████████████████████▎                | 414/497 [08:07<01:37,  1.18s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  10%|██████████▉                                                                                                   | 9/91 [00:02<00:23,  3.52it/s][AEpoch 0:  84%|███████████████████████████████████████████████████████████████████████████████████▌                | 415/497 [08:07<01:36,  1.18s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  11%|███████████▉                                                                                                 | 10/91 [00:02<00:23,  3.52it/s][AEpoch 0:  84%|███████████████████████████████████████████████████████████████████████████████████▋                | 416/497 [08:08<01:35,  1.17s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  12%|█████████████▏                                                                                               | 11/91 [00:03<00:22,  3.52it/s][AEpoch 0:  84%|███████████████████████████████████████████████████████████████████████████████████▉                | 417/497 [08:08<01:33,  1.17s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  13%|██████████████▎                                                                                              | 12/91 [00:03<00:22,  3.51it/s][AEpoch 0:  84%|████████████████████████████████████████████████████████████████████████████████████                | 418/497 [08:08<01:32,  1.17s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  14%|███████████████▌                                                                                             | 13/91 [00:03<00:22,  3.51it/s][AEpoch 0:  84%|████████████████████████████████████████████████████████████████████████████████████▎               | 419/497 [08:08<01:31,  1.17s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  15%|████████████████▊                                                                                            | 14/91 [00:03<00:21,  3.51it/s][AEpoch 0:  85%|████████████████████████████████████████████████████████████████████████████████████▌               | 420/497 [08:09<01:29,  1.16s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  16%|█████████████████▉                                                                                           | 15/91 [00:04<00:21,  3.51it/s][AEpoch 0:  85%|████████████████████████████████████████████████████████████████████████████████████▋               | 421/497 [08:09<01:28,  1.16s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  18%|███████████████████▏                                                                                         | 16/91 [00:04<00:21,  3.51it/s][AEpoch 0:  85%|████████████████████████████████████████████████████████████████████████████████████▉               | 422/497 [08:09<01:27,  1.16s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  19%|████████████████████▎                                                                                        | 17/91 [00:04<00:21,  3.51it/s][AEpoch 0:  85%|█████████████████████████████████████████████████████████████████████████████████████               | 423/497 [08:10<01:25,  1.16s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  20%|█████████████████████▌                                                                                       | 18/91 [00:05<00:20,  3.50it/s][AEpoch 0:  85%|█████████████████████████████████████████████████████████████████████████████████████▎              | 424/497 [08:10<01:24,  1.16s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  21%|██████████████████████▊                                                                                      | 19/91 [00:05<00:20,  3.50it/s][AEpoch 0:  86%|█████████████████████████████████████████████████████████████████████████████████████▌              | 425/497 [08:10<01:23,  1.15s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  22%|███████████████████████▉                                                                                     | 20/91 [00:05<00:20,  3.50it/s][AEpoch 0:  86%|█████████████████████████████████████████████████████████████████████████████████████▋              | 426/497 [08:10<01:21,  1.15s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  23%|█████████████████████████▏                                                                                   | 21/91 [00:06<00:20,  3.50it/s][AEpoch 0:  86%|█████████████████████████████████████████████████████████████████████████████████████▉              | 427/497 [08:11<01:20,  1.15s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  24%|██████████████████████████▎                                                                                  | 22/91 [00:06<00:19,  3.50it/s][AEpoch 0:  86%|██████████████████████████████████████████████████████████████████████████████████████              | 428/497 [08:11<01:19,  1.15s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  25%|███████████████████████████▌                                                                                 | 23/91 [00:06<00:19,  3.50it/s][AEpoch 0:  86%|██████████████████████████████████████████████████████████████████████████████████████▎             | 429/497 [08:11<01:17,  1.15s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  26%|████████████████████████████▋                                                                                | 24/91 [00:06<00:19,  3.50it/s][AEpoch 0:  87%|██████████████████████████████████████████████████████████████████████████████████████▌             | 430/497 [08:12<01:16,  1.14s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  27%|█████████████████████████████▉                                                                               | 25/91 [00:07<00:18,  3.50it/s][AEpoch 0:  87%|██████████████████████████████████████████████████████████████████████████████████████▋             | 431/497 [08:12<01:15,  1.14s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  29%|███████████████████████████████▏                                                                             | 26/91 [00:07<00:18,  3.50it/s][AEpoch 0:  87%|██████████████████████████████████████████████████████████████████████████████████████▉             | 432/497 [08:12<01:14,  1.14s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  30%|████████████████████████████████▎                                                                            | 27/91 [00:07<00:18,  3.50it/s][AEpoch 0:  87%|███████████████████████████████████████████████████████████████████████████████████████             | 433/497 [08:12<01:12,  1.14s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  31%|█████████████████████████████████▌                                                                           | 28/91 [00:08<00:18,  3.50it/s][AEpoch 0:  87%|███████████████████████████████████████████████████████████████████████████████████████▎            | 434/497 [08:13<01:11,  1.14s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  32%|██████████████████████████████████▋                                                                          | 29/91 [00:08<00:17,  3.50it/s][AEpoch 0:  88%|███████████████████████████████████████████████████████████████████████████████████████▌            | 435/497 [08:13<01:10,  1.13s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  33%|███████████████████████████████████▉                                                                         | 30/91 [00:08<00:17,  3.49it/s][AEpoch 0:  88%|███████████████████████████████████████████████████████████████████████████████████████▋            | 436/497 [08:13<01:09,  1.13s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  34%|█████████████████████████████████████▏                                                                       | 31/91 [00:08<00:17,  3.49it/s][AEpoch 0:  88%|███████████████████████████████████████████████████████████████████████████████████████▉            | 437/497 [08:14<01:07,  1.13s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  35%|██████████████████████████████████████▎                                                                      | 32/91 [00:09<00:16,  3.49it/s][AEpoch 0:  88%|████████████████████████████████████████████████████████████████████████████████████████▏           | 438/497 [08:14<01:06,  1.13s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  36%|███████████████████████████████████████▌                                                                     | 33/91 [00:09<00:16,  3.49it/s][AEpoch 0:  88%|████████████████████████████████████████████████████████████████████████████████████████▎           | 439/497 [08:14<01:05,  1.13s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  37%|████████████████████████████████████████▋                                                                    | 34/91 [00:09<00:16,  3.49it/s][AEpoch 0:  89%|████████████████████████████████████████████████████████████████████████████████████████▌           | 440/497 [08:14<01:04,  1.12s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  38%|█████████████████████████████████████████▉                                                                   | 35/91 [00:10<00:16,  3.49it/s][AEpoch 0:  89%|████████████████████████████████████████████████████████████████████████████████████████▋           | 441/497 [08:15<01:02,  1.12s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  40%|███████████████████████████████████████████                                                                  | 36/91 [00:10<00:15,  3.49it/s][AEpoch 0:  89%|████████████████████████████████████████████████████████████████████████████████████████▉           | 442/497 [08:15<01:01,  1.12s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  41%|████████████████████████████████████████████▎                                                                | 37/91 [00:10<00:15,  3.49it/s][AEpoch 0:  89%|█████████████████████████████████████████████████████████████████████████████████████████▏          | 443/497 [08:15<01:00,  1.12s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  42%|█████████████████████████████████████████████▌                                                               | 38/91 [00:10<00:15,  3.49it/s][AEpoch 0:  89%|█████████████████████████████████████████████████████████████████████████████████████████▎          | 444/497 [08:16<00:59,  1.12s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  43%|██████████████████████████████████████████████▋                                                              | 39/91 [00:11<00:14,  3.49it/s][AEpoch 0:  90%|█████████████████████████████████████████████████████████████████████████████████████████▌          | 445/497 [08:16<00:57,  1.12s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  44%|███████████████████████████████████████████████▉                                                             | 40/91 [00:11<00:14,  3.49it/s][AEpoch 0:  90%|█████████████████████████████████████████████████████████████████████████████████████████▋          | 446/497 [08:16<00:56,  1.11s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  45%|█████████████████████████████████████████████████                                                            | 41/91 [00:11<00:14,  3.49it/s][AEpoch 0:  90%|█████████████████████████████████████████████████████████████████████████████████████████▉          | 447/497 [08:16<00:55,  1.11s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  46%|██████████████████████████████████████████████████▎                                                          | 42/91 [00:12<00:14,  3.49it/s][AEpoch 0:  90%|██████████████████████████████████████████████████████████████████████████████████████████▏         | 448/497 [08:17<00:54,  1.11s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  47%|███████████████████████████████████████████████████▌                                                         | 43/91 [00:12<00:13,  3.49it/s][AEpoch 0:  90%|██████████████████████████████████████████████████████████████████████████████████████████▎         | 449/497 [08:17<00:53,  1.11s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  48%|████████████████████████████████████████████████████▋                                                        | 44/91 [00:12<00:13,  3.49it/s][AEpoch 0:  91%|██████████████████████████████████████████████████████████████████████████████████████████▌         | 450/497 [08:17<00:51,  1.11s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  49%|█████████████████████████████████████████████████████▉                                                       | 45/91 [00:12<00:13,  3.49it/s][AEpoch 0:  91%|██████████████████████████████████████████████████████████████████████████████████████████▋         | 451/497 [08:18<00:50,  1.10s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  51%|███████████████████████████████████████████████████████                                                      | 46/91 [00:13<00:12,  3.49it/s][AEpoch 0:  91%|██████████████████████████████████████████████████████████████████████████████████████████▉         | 452/497 [08:18<00:49,  1.10s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  52%|████████████████████████████████████████████████████████▎                                                    | 47/91 [00:13<00:12,  3.49it/s][AEpoch 0:  91%|███████████████████████████████████████████████████████████████████████████████████████████▏        | 453/497 [08:18<00:48,  1.10s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  53%|█████████████████████████████████████████████████████████▍                                                   | 48/91 [00:13<00:12,  3.49it/s][AEpoch 0:  91%|███████████████████████████████████████████████████████████████████████████████████████████▎        | 454/497 [08:18<00:47,  1.10s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  54%|██████████████████████████████████████████████████████████▋                                                  | 49/91 [00:14<00:12,  3.49it/s][AEpoch 0:  92%|███████████████████████████████████████████████████████████████████████████████████████████▌        | 455/497 [08:19<00:46,  1.10s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  55%|███████████████████████████████████████████████████████████▉                                                 | 50/91 [00:14<00:11,  3.49it/s][AEpoch 0:  92%|███████████████████████████████████████████████████████████████████████████████████████████▊        | 456/497 [08:19<00:44,  1.10s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  56%|█████████████████████████████████████████████████████████████                                                | 51/91 [00:14<00:11,  3.49it/s][AEpoch 0:  92%|███████████████████████████████████████████████████████████████████████████████████████████▉        | 457/497 [08:19<00:43,  1.09s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  57%|██████████████████████████████████████████████████████████████▎                                              | 52/91 [00:14<00:11,  3.49it/s][AEpoch 0:  92%|████████████████████████████████████████████████████████████████████████████████████████████▏       | 458/497 [08:20<00:42,  1.09s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  58%|███████████████████████████████████████████████████████████████▍                                             | 53/91 [00:15<00:10,  3.49it/s][AEpoch 0:  92%|████████████████████████████████████████████████████████████████████████████████████████████▎       | 459/497 [08:20<00:41,  1.09s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  59%|████████████████████████████████████████████████████████████████▋                                            | 54/91 [00:15<00:10,  3.49it/s][AEpoch 0:  93%|████████████████████████████████████████████████████████████████████████████████████████████▌       | 460/497 [08:20<00:40,  1.09s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  60%|█████████████████████████████████████████████████████████████████▉                                           | 55/91 [00:15<00:10,  3.49it/s][AEpoch 0:  93%|████████████████████████████████████████████████████████████████████████████████████████████▊       | 461/497 [08:20<00:39,  1.09s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  62%|███████████████████████████████████████████████████████████████████                                          | 56/91 [00:16<00:10,  3.49it/s][AEpoch 0:  93%|████████████████████████████████████████████████████████████████████████████████████████████▉       | 462/497 [08:21<00:37,  1.08s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  63%|████████████████████████████████████████████████████████████████████▎                                        | 57/91 [00:16<00:09,  3.49it/s][AEpoch 0:  93%|█████████████████████████████████████████████████████████████████████████████████████████████▏      | 463/497 [08:21<00:36,  1.08s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  64%|█████████████████████████████████████████████████████████████████████▍                                       | 58/91 [00:16<00:09,  3.49it/s][AEpoch 0:  93%|█████████████████████████████████████████████████████████████████████████████████████████████▎      | 464/497 [08:21<00:35,  1.08s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  65%|██████████████████████████████████████████████████████████████████████▋                                      | 59/91 [00:16<00:09,  3.49it/s][AEpoch 0:  94%|█████████████████████████████████████████████████████████████████████████████████████████████▌      | 465/497 [08:22<00:34,  1.08s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  66%|███████████████████████████████████████████████████████████████████████▊                                     | 60/91 [00:17<00:08,  3.49it/s][AEpoch 0:  94%|█████████████████████████████████████████████████████████████████████████████████████████████▊      | 466/497 [08:22<00:33,  1.08s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  67%|█████████████████████████████████████████████████████████████████████████                                    | 61/91 [00:17<00:08,  3.49it/s][AEpoch 0:  94%|█████████████████████████████████████████████████████████████████████████████████████████████▉      | 467/497 [08:22<00:32,  1.08s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  68%|██████████████████████████████████████████████████████████████████████████▎                                  | 62/91 [00:17<00:08,  3.49it/s][AEpoch 0:  94%|██████████████████████████████████████████████████████████████████████████████████████████████▏     | 468/497 [08:22<00:31,  1.07s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  69%|███████████████████████████████████████████████████████████████████████████▍                                 | 63/91 [00:18<00:08,  3.49it/s][AEpoch 0:  94%|██████████████████████████████████████████████████████████████████████████████████████████████▎     | 469/497 [08:23<00:30,  1.07s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  70%|████████████████████████████████████████████████████████████████████████████▋                                | 64/91 [00:18<00:07,  3.49it/s][AEpoch 0:  95%|██████████████████████████████████████████████████████████████████████████████████████████████▌     | 470/497 [08:23<00:28,  1.07s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  71%|█████████████████████████████████████████████████████████████████████████████▊                               | 65/91 [00:18<00:07,  3.49it/s][AEpoch 0:  95%|██████████████████████████████████████████████████████████████████████████████████████████████▊     | 471/497 [08:23<00:27,  1.07s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  73%|███████████████████████████████████████████████████████████████████████████████                              | 66/91 [00:18<00:07,  3.49it/s][AEpoch 0:  95%|██████████████████████████████████████████████████████████████████████████████████████████████▉     | 472/497 [08:24<00:26,  1.07s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  74%|████████████████████████████████████████████████████████████████████████████████▎                            | 67/91 [00:19<00:06,  3.49it/s][AEpoch 0:  95%|███████████████████████████████████████████████████████████████████████████████████████████████▏    | 473/497 [08:24<00:25,  1.07s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  75%|█████████████████████████████████████████████████████████████████████████████████▍                           | 68/91 [00:19<00:06,  3.49it/s][AEpoch 0:  95%|███████████████████████████████████████████████████████████████████████████████████████████████▎    | 474/497 [08:24<00:24,  1.06s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  76%|██████████████████████████████████████████████████████████████████████████████████▋                          | 69/91 [00:19<00:06,  3.49it/s][AEpoch 0:  96%|███████████████████████████████████████████████████████████████████████████████████████████████▌    | 475/497 [08:24<00:23,  1.06s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  77%|███████████████████████████████████████████████████████████████████████████████████▊                         | 70/91 [00:20<00:06,  3.49it/s][AEpoch 0:  96%|███████████████████████████████████████████████████████████████████████████████████████████████▊    | 476/497 [08:25<00:22,  1.06s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  78%|█████████████████████████████████████████████████████████████████████████████████████                        | 71/91 [00:20<00:05,  3.49it/s][AEpoch 0:  96%|███████████████████████████████████████████████████████████████████████████████████████████████▉    | 477/497 [08:25<00:21,  1.06s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  79%|██████████████████████████████████████████████████████████████████████████████████████▏                      | 72/91 [00:20<00:05,  3.49it/s][AEpoch 0:  96%|████████████████████████████████████████████████████████████████████████████████████████████████▏   | 478/497 [08:25<00:20,  1.06s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  80%|███████████████████████████████████████████████████████████████████████████████████████▍                     | 73/91 [00:20<00:05,  3.49it/s][AEpoch 0:  96%|████████████████████████████████████████████████████████████████████████████████████████████████▍   | 479/497 [08:26<00:19,  1.06s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  81%|████████████████████████████████████████████████████████████████████████████████████████▋                    | 74/91 [00:21<00:04,  3.49it/s][AEpoch 0:  97%|████████████████████████████████████████████████████████████████████████████████████████████████▌   | 480/497 [08:26<00:17,  1.05s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  82%|█████████████████████████████████████████████████████████████████████████████████████████▊                   | 75/91 [00:21<00:04,  3.49it/s][AEpoch 0:  97%|████████████████████████████████████████████████████████████████████████████████████████████████▊   | 481/497 [08:26<00:16,  1.05s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  84%|███████████████████████████████████████████████████████████████████████████████████████████                  | 76/91 [00:21<00:04,  3.49it/s][AEpoch 0:  97%|████████████████████████████████████████████████████████████████████████████████████████████████▉   | 482/497 [08:26<00:15,  1.05s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  85%|████████████████████████████████████████████████████████████████████████████████████████████▏                | 77/91 [00:22<00:04,  3.49it/s][AEpoch 0:  97%|█████████████████████████████████████████████████████████████████████████████████████████████████▏  | 483/497 [08:27<00:14,  1.05s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  86%|█████████████████████████████████████████████████████████████████████████████████████████████▍               | 78/91 [00:22<00:03,  3.49it/s][AEpoch 0:  97%|█████████████████████████████████████████████████████████████████████████████████████████████████▍  | 484/497 [08:27<00:13,  1.05s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  87%|██████████████████████████████████████████████████████████████████████████████████████████████▋              | 79/91 [00:22<00:03,  3.49it/s][AEpoch 0:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████▌  | 485/497 [08:27<00:12,  1.05s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  88%|███████████████████████████████████████████████████████████████████████████████████████████████▊             | 80/91 [00:22<00:03,  3.49it/s][AEpoch 0:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████▊  | 486/497 [08:28<00:11,  1.05s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  89%|█████████████████████████████████████████████████████████████████████████████████████████████████            | 81/91 [00:23<00:02,  3.49it/s][AEpoch 0:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████▉  | 487/497 [08:28<00:10,  1.04s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████▏          | 82/91 [00:23<00:02,  3.49it/s][AEpoch 0:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████▏ | 488/497 [08:28<00:09,  1.04s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  91%|███████████████████████████████████████████████████████████████████████████████████████████████████▍         | 83/91 [00:23<00:02,  3.49it/s][AEpoch 0:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████▍ | 489/497 [08:28<00:08,  1.04s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  92%|████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 84/91 [00:24<00:02,  3.49it/s][AEpoch 0:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████▌ | 490/497 [08:29<00:07,  1.04s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  93%|█████████████████████████████████████████████████████████████████████████████████████████████████████▊       | 85/91 [00:24<00:01,  3.49it/s][AEpoch 0:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████▊ | 491/497 [08:29<00:06,  1.04s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████      | 86/91 [00:24<00:01,  3.49it/s][AEpoch 0:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████▉ | 492/497 [08:29<00:05,  1.04s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏    | 87/91 [00:24<00:01,  3.49it/s][AEpoch 0:  99%|███████████████████████████████████████████████████████████████████████████████████████████████████▏| 493/497 [08:30<00:04,  1.03s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 88/91 [00:25<00:00,  3.49it/s][AEpoch 0:  99%|███████████████████████████████████████████████████████████████████████████████████████████████████▍| 494/497 [08:30<00:03,  1.03s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 89/91 [00:25<00:00,  3.49it/s][AEpoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████▌| 495/497 [08:30<00:02,  1.03s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0:  99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 90/91 [00:25<00:00,  3.49it/s][AEpoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████▊| 496/497 [08:30<00:01,  1.03s/it, loss=0.0923, v_num=75]torch.Size([1, 3, 512, 512])
torch.Size([1, 512, 512])

Validation DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [00:26<00:00,  3.49it/s][AEpoch 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 497/497 [08:31<00:00,  1.03s/it, loss=0.0923, v_num=75]Epoch 0: 100%|█| 497/497 [08:31<00:00,  1.03s/it, loss=0.0923, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, Val
                                                                                                                                                                           [ATraining_loss 0.1695165798177883
Epoch 0: 100%|█| 497/497 [08:32<00:00,  1.03s/it, loss=0.0923, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 0:   0%| | 0/497 [00:00<?, ?it/s, loss=0.0923, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, Validation F2Epoch 1:   0%| | 0/497 [00:00<?, ?it/s, loss=0.0923, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, Validation F2Epoch 1:   0%| | 1/497 [00:01<10:50,  1.31s/it, loss=0.0923, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   0%| | 1/497 [00:01<10:51,  1.31s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   0%| | 2/497 [00:02<10:19,  1.25s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   0%| | 2/497 [00:02<10:20,  1.25s/it, loss=0.0984, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   1%| | 3/497 [00:03<10:09,  1.23s/it, loss=0.0984, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   1%| | 3/497 [00:03<10:09,  1.23s/it, loss=0.104, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   1%| | 4/497 [00:04<10:03,  1.22s/it, loss=0.104, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   1%| | 4/497 [00:04<10:03,  1.22s/it, loss=0.106, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   1%| | 5/497 [00:06<09:59,  1.22s/it, loss=0.106, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   1%| | 5/497 [00:06<09:59,  1.22s/it, loss=0.106, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   1%| | 6/497 [00:07<09:56,  1.22s/it, loss=0.106, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   1%| | 6/497 [00:07<09:56,  1.22s/it, loss=0.105, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   1%| | 7/497 [00:08<09:54,  1.21s/it, loss=0.105, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   1%| | 7/497 [00:08<09:54,  1.21s/it, loss=0.115, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   2%| | 8/497 [00:09<09:52,  1.21s/it, loss=0.115, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   2%| | 8/497 [00:09<09:52,  1.21s/it, loss=0.114, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   2%| | 9/497 [00:10<09:50,  1.21s/it, loss=0.114, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   2%| | 9/497 [00:10<09:50,  1.21s/it, loss=0.115, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   2%| | 10/497 [00:12<09:48,  1.21s/it, loss=0.115, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   2%| | 10/497 [00:12<09:48,  1.21s/it, loss=0.11, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   2%| | 11/497 [00:13<09:47,  1.21s/it, loss=0.11, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   2%| | 11/497 [00:13<09:47,  1.21s/it, loss=0.112, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   2%| | 12/497 [00:14<09:45,  1.21s/it, loss=0.112, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   2%| | 12/497 [00:14<09:45,  1.21s/it, loss=0.114, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   3%| | 13/497 [00:15<09:44,  1.21s/it, loss=0.114, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   3%| | 13/497 [00:15<09:44,  1.21s/it, loss=0.113, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   3%| | 14/497 [00:16<09:42,  1.21s/it, loss=0.113, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   3%| | 14/497 [00:16<09:42,  1.21s/it, loss=0.113, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   3%| | 15/497 [00:18<09:41,  1.21s/it, loss=0.113, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   3%| | 15/497 [00:18<09:41,  1.21s/it, loss=0.118, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   3%| | 16/497 [00:19<09:39,  1.21s/it, loss=0.118, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   3%| | 16/497 [00:19<09:39,  1.21s/it, loss=0.122, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   3%| | 17/497 [00:20<09:38,  1.21s/it, loss=0.122, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   3%| | 17/497 [00:20<09:38,  1.21s/it, loss=0.124, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   4%| | 18/497 [00:21<09:37,  1.21s/it, loss=0.124, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   4%| | 18/497 [00:21<09:37,  1.21s/it, loss=0.132, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   4%| | 19/497 [00:22<09:36,  1.21s/it, loss=0.132, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   4%| | 19/497 [00:22<09:36,  1.21s/it, loss=0.131, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   4%| | 20/497 [00:24<09:34,  1.21s/it, loss=0.131, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   4%| | 20/497 [00:24<09:34,  1.21s/it, loss=0.13, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   4%| | 21/497 [00:25<09:33,  1.20s/it, loss=0.13, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   4%| | 21/497 [00:25<09:33,  1.20s/it, loss=0.128, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   4%| | 22/497 [00:26<09:32,  1.20s/it, loss=0.128, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   4%| | 22/497 [00:26<09:32,  1.20s/it, loss=0.135, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   5%| | 23/497 [00:27<09:31,  1.20s/it, loss=0.135, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   5%| | 23/497 [00:27<09:31,  1.20s/it, loss=0.127, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   5%| | 24/497 [00:28<09:29,  1.20s/it, loss=0.127, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   5%| | 24/497 [00:28<09:29,  1.21s/it, loss=0.125, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   5%| | 25/497 [00:30<09:28,  1.20s/it, loss=0.125, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   5%| | 25/497 [00:30<09:28,  1.20s/it, loss=0.125, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   5%| | 26/497 [00:31<09:27,  1.20s/it, loss=0.125, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   5%| | 26/497 [00:31<09:27,  1.20s/it, loss=0.134, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   5%| | 27/497 [00:32<09:26,  1.20s/it, loss=0.134, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   5%| | 27/497 [00:32<09:26,  1.21s/it, loss=0.131, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   6%| | 28/497 [00:33<09:25,  1.20s/it, loss=0.131, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   6%| | 28/497 [00:33<09:25,  1.20s/it, loss=0.135, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   6%| | 29/497 [00:34<09:23,  1.20s/it, loss=0.135, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   6%| | 29/497 [00:34<09:23,  1.20s/it, loss=0.133, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   6%| | 30/497 [00:36<09:22,  1.20s/it, loss=0.133, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   6%| | 30/497 [00:36<09:22,  1.20s/it, loss=0.136, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   6%| | 31/497 [00:37<09:21,  1.20s/it, loss=0.136, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   6%| | 31/497 [00:37<09:21,  1.20s/it, loss=0.139, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   6%| | 32/497 [00:38<09:20,  1.20s/it, loss=0.139, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   6%| | 32/497 [00:38<09:20,  1.20s/it, loss=0.134, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   7%| | 33/497 [00:39<09:18,  1.20s/it, loss=0.134, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   7%| | 33/497 [00:39<09:18,  1.20s/it, loss=0.132, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   7%| | 34/497 [00:40<09:17,  1.20s/it, loss=0.132, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   7%| | 34/497 [00:40<09:17,  1.20s/it, loss=0.135, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   7%| | 35/497 [00:42<09:16,  1.20s/it, loss=0.135, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   7%| | 35/497 [00:42<09:16,  1.20s/it, loss=0.129, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   7%| | 36/497 [00:43<09:15,  1.20s/it, loss=0.129, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   7%| | 36/497 [00:43<09:15,  1.20s/it, loss=0.126, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   7%| | 37/497 [00:44<09:13,  1.20s/it, loss=0.126, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   7%| | 37/497 [00:44<09:13,  1.20s/it, loss=0.124, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   8%| | 38/497 [00:45<09:12,  1.20s/it, loss=0.124, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   8%| | 38/497 [00:45<09:12,  1.20s/it, loss=0.117, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   8%| | 39/497 [00:46<09:11,  1.20s/it, loss=0.117, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   8%| | 39/497 [00:46<09:11,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   8%| | 40/497 [00:48<09:10,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   8%| | 40/497 [00:48<09:10,  1.20s/it, loss=0.125, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   8%| | 41/497 [00:49<09:09,  1.20s/it, loss=0.125, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   8%| | 41/497 [00:49<09:09,  1.20s/it, loss=0.119, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   8%| | 42/497 [00:50<09:07,  1.20s/it, loss=0.119, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   8%| | 42/497 [00:50<09:07,  1.20s/it, loss=0.12, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   9%| | 43/497 [00:51<09:06,  1.20s/it, loss=0.12, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   9%| | 43/497 [00:51<09:06,  1.20s/it, loss=0.12, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   9%| | 44/497 [00:52<09:05,  1.20s/it, loss=0.12, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   9%| | 44/497 [00:52<09:05,  1.20s/it, loss=0.12, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   9%| | 45/497 [00:54<09:04,  1.20s/it, loss=0.12, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:   9%| | 45/497 [00:54<09:04,  1.20s/it, loss=0.118, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   9%| | 46/497 [00:55<09:02,  1.20s/it, loss=0.118, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   9%| | 46/497 [00:55<09:02,  1.20s/it, loss=0.106, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   9%| | 47/497 [00:56<09:01,  1.20s/it, loss=0.106, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:   9%| | 47/497 [00:56<09:01,  1.20s/it, loss=0.0989, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  10%| | 48/497 [00:57<09:00,  1.20s/it, loss=0.0989, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  10%| | 48/497 [00:57<09:00,  1.20s/it, loss=0.107, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  10%| | 49/497 [00:58<08:59,  1.20s/it, loss=0.107, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  10%| | 49/497 [00:58<08:59,  1.20s/it, loss=0.12, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:  10%| | 50/497 [01:00<08:58,  1.20s/it, loss=0.12, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:  10%| | 50/497 [01:00<08:58,  1.20s/it, loss=0.117, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  10%| | 51/497 [01:01<08:56,  1.20s/it, loss=0.117, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  10%| | 51/497 [01:01<08:56,  1.20s/it, loss=0.113, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  10%| | 52/497 [01:02<08:55,  1.20s/it, loss=0.113, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  10%| | 52/497 [01:02<08:55,  1.20s/it, loss=0.112, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  11%| | 53/497 [01:03<08:54,  1.20s/it, loss=0.112, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  11%| | 53/497 [01:03<08:54,  1.20s/it, loss=0.112, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  11%| | 54/497 [01:04<08:53,  1.20s/it, loss=0.112, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  11%| | 54/497 [01:04<08:53,  1.20s/it, loss=0.108, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  11%| | 55/497 [01:06<08:51,  1.20s/it, loss=0.108, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  11%| | 55/497 [01:06<08:51,  1.20s/it, loss=0.111, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  11%| | 56/497 [01:07<08:50,  1.20s/it, loss=0.111, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  11%| | 56/497 [01:07<08:50,  1.20s/it, loss=0.111, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  11%| | 57/497 [01:08<08:49,  1.20s/it, loss=0.111, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  11%| | 57/497 [01:08<08:49,  1.20s/it, loss=0.111, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  12%| | 58/497 [01:09<08:48,  1.20s/it, loss=0.111, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  12%| | 58/497 [01:09<08:48,  1.20s/it, loss=0.113, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  12%| | 59/497 [01:11<08:47,  1.20s/it, loss=0.113, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  12%| | 59/497 [01:11<08:47,  1.20s/it, loss=0.11, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:  12%| | 60/497 [01:12<08:45,  1.20s/it, loss=0.11, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:  12%| | 60/497 [01:12<08:45,  1.20s/it, loss=0.109, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  12%| | 61/497 [01:13<08:44,  1.20s/it, loss=0.109, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  12%| | 61/497 [01:13<08:44,  1.20s/it, loss=0.108, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  12%| | 62/497 [01:14<08:43,  1.20s/it, loss=0.108, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  12%| | 62/497 [01:14<08:43,  1.20s/it, loss=0.0995, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  13%|▏| 63/497 [01:15<08:42,  1.20s/it, loss=0.0995, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  13%|▏| 63/497 [01:15<08:42,  1.20s/it, loss=0.104, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  13%|▏| 64/497 [01:17<08:41,  1.20s/it, loss=0.104, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  13%|▏| 64/497 [01:17<08:41,  1.20s/it, loss=0.103, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  13%|▏| 65/497 [01:18<08:39,  1.20s/it, loss=0.103, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  13%|▏| 65/497 [01:18<08:39,  1.20s/it, loss=0.115, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  13%|▏| 66/497 [01:19<08:38,  1.20s/it, loss=0.115, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  13%|▏| 66/497 [01:19<08:38,  1.20s/it, loss=0.117, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  13%|▏| 67/497 [01:20<08:37,  1.20s/it, loss=0.117, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  13%|▏| 67/497 [01:20<08:37,  1.20s/it, loss=0.117, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  14%|▏| 68/497 [01:21<08:36,  1.20s/it, loss=0.117, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  14%|▏| 68/497 [01:21<08:36,  1.20s/it, loss=0.115, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  14%|▏| 69/497 [01:23<08:35,  1.20s/it, loss=0.115, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  14%|▏| 69/497 [01:23<08:35,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  14%|▏| 70/497 [01:24<08:33,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  14%|▏| 70/497 [01:24<08:33,  1.20s/it, loss=0.1, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidatEpoch 1:  14%|▏| 71/497 [01:25<08:32,  1.20s/it, loss=0.1, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidatEpoch 1:  14%|▏| 71/497 [01:25<08:32,  1.20s/it, loss=0.0992, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  14%|▏| 72/497 [01:26<08:31,  1.20s/it, loss=0.0992, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  14%|▏| 72/497 [01:26<08:31,  1.20s/it, loss=0.0974, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  15%|▏| 73/497 [01:27<08:30,  1.20s/it, loss=0.0974, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  15%|▏| 73/497 [01:27<08:30,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  15%|▏| 74/497 [01:29<08:29,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  15%|▏| 74/497 [01:29<08:29,  1.20s/it, loss=0.104, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  15%|▏| 75/497 [01:30<08:27,  1.20s/it, loss=0.104, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  15%|▏| 75/497 [01:30<08:27,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  15%|▏| 76/497 [01:31<08:26,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  15%|▏| 76/497 [01:31<08:26,  1.20s/it, loss=0.102, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  15%|▏| 77/497 [01:32<08:25,  1.20s/it, loss=0.102, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  15%|▏| 77/497 [01:32<08:25,  1.20s/it, loss=0.119, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  16%|▏| 78/497 [01:33<08:24,  1.20s/it, loss=0.119, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  16%|▏| 78/497 [01:33<08:24,  1.20s/it, loss=0.12, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:  16%|▏| 79/497 [01:35<08:23,  1.20s/it, loss=0.12, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:  16%|▏| 79/497 [01:35<08:23,  1.20s/it, loss=0.12, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:  16%|▏| 80/497 [01:36<08:21,  1.20s/it, loss=0.12, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:  16%|▏| 80/497 [01:36<08:21,  1.20s/it, loss=0.117, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  16%|▏| 81/497 [01:37<08:20,  1.20s/it, loss=0.117, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  16%|▏| 81/497 [01:37<08:20,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  16%|▏| 82/497 [01:38<08:19,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  16%|▏| 82/497 [01:38<08:19,  1.20s/it, loss=0.122, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  17%|▏| 83/497 [01:39<08:18,  1.20s/it, loss=0.122, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  17%|▏| 83/497 [01:39<08:18,  1.20s/it, loss=0.118, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  17%|▏| 84/497 [01:41<08:17,  1.20s/it, loss=0.118, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  17%|▏| 84/497 [01:41<08:17,  1.20s/it, loss=0.122, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  17%|▏| 85/497 [01:42<08:15,  1.20s/it, loss=0.122, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  17%|▏| 85/497 [01:42<08:15,  1.20s/it, loss=0.111, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  17%|▏| 86/497 [01:43<08:14,  1.20s/it, loss=0.111, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  17%|▏| 86/497 [01:43<08:14,  1.20s/it, loss=0.111, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  18%|▏| 87/497 [01:44<08:13,  1.20s/it, loss=0.111, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  18%|▏| 87/497 [01:44<08:13,  1.20s/it, loss=0.118, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  18%|▏| 88/497 [01:45<08:12,  1.20s/it, loss=0.118, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  18%|▏| 88/497 [01:45<08:12,  1.20s/it, loss=0.109, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  18%|▏| 89/497 [01:47<08:11,  1.20s/it, loss=0.109, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  18%|▏| 89/497 [01:47<08:11,  1.20s/it, loss=0.114, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  18%|▏| 90/497 [01:48<08:09,  1.20s/it, loss=0.114, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  18%|▏| 90/497 [01:48<08:09,  1.20s/it, loss=0.119, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  18%|▏| 91/497 [01:49<08:08,  1.20s/it, loss=0.119, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  18%|▏| 91/497 [01:49<08:08,  1.20s/it, loss=0.123, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  19%|▏| 92/497 [01:50<08:07,  1.20s/it, loss=0.123, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  19%|▏| 92/497 [01:50<08:07,  1.20s/it, loss=0.123, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  19%|▏| 93/497 [01:51<08:06,  1.20s/it, loss=0.123, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  19%|▏| 93/497 [01:51<08:06,  1.20s/it, loss=0.12, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:  19%|▏| 94/497 [01:53<08:05,  1.20s/it, loss=0.12, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:  19%|▏| 94/497 [01:53<08:05,  1.20s/it, loss=0.132, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  19%|▏| 95/497 [01:54<08:03,  1.20s/it, loss=0.132, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  19%|▏| 95/497 [01:54<08:03,  1.20s/it, loss=0.138, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  19%|▏| 96/497 [01:55<08:02,  1.20s/it, loss=0.138, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  19%|▏| 96/497 [01:55<08:02,  1.20s/it, loss=0.137, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  20%|▏| 97/497 [01:56<08:01,  1.20s/it, loss=0.137, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  20%|▏| 97/497 [01:56<08:01,  1.20s/it, loss=0.122, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  20%|▏| 98/497 [01:57<08:00,  1.20s/it, loss=0.122, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  20%|▏| 98/497 [01:57<08:00,  1.20s/it, loss=0.119, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  20%|▏| 99/497 [01:59<07:59,  1.20s/it, loss=0.119, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  20%|▏| 99/497 [01:59<07:59,  1.20s/it, loss=0.12, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:  20%|▏| 100/497 [02:00<07:57,  1.20s/it, loss=0.12, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  20%|▏| 100/497 [02:00<07:57,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  20%|▏| 101/497 [02:01<07:56,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  20%|▏| 101/497 [02:01<07:56,  1.20s/it, loss=0.114, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  21%|▏| 102/497 [02:02<07:55,  1.20s/it, loss=0.114, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  21%|▏| 102/497 [02:02<07:55,  1.20s/it, loss=0.114, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  21%|▏| 103/497 [02:03<07:54,  1.20s/it, loss=0.114, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  21%|▏| 103/497 [02:03<07:54,  1.20s/it, loss=0.115, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  21%|▏| 104/497 [02:05<07:53,  1.20s/it, loss=0.115, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  21%|▏| 104/497 [02:05<07:53,  1.20s/it, loss=0.111, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  21%|▏| 105/497 [02:06<07:51,  1.20s/it, loss=0.111, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  21%|▏| 105/497 [02:06<07:51,  1.20s/it, loss=0.11, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  21%|▏| 106/497 [02:07<07:50,  1.20s/it, loss=0.11, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  21%|▏| 106/497 [02:07<07:50,  1.20s/it, loss=0.108, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  22%|▏| 107/497 [02:08<07:49,  1.20s/it, loss=0.108, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  22%|▏| 107/497 [02:08<07:49,  1.20s/it, loss=0.103, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  22%|▏| 108/497 [02:09<07:48,  1.20s/it, loss=0.103, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  22%|▏| 108/497 [02:10<07:48,  1.20s/it, loss=0.106, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  22%|▏| 109/497 [02:11<07:47,  1.20s/it, loss=0.106, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  22%|▏| 109/497 [02:11<07:47,  1.20s/it, loss=0.104, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  22%|▏| 110/497 [02:12<07:45,  1.20s/it, loss=0.104, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  22%|▏| 110/497 [02:12<07:45,  1.20s/it, loss=0.0993, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  22%|▏| 111/497 [02:13<07:44,  1.20s/it, loss=0.0993, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  22%|▏| 111/497 [02:13<07:44,  1.20s/it, loss=0.108, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  23%|▏| 112/497 [02:14<07:43,  1.20s/it, loss=0.108, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  23%|▏| 112/497 [02:14<07:43,  1.20s/it, loss=0.114, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  23%|▏| 113/497 [02:16<07:42,  1.20s/it, loss=0.114, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  23%|▏| 113/497 [02:16<07:42,  1.20s/it, loss=0.118, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  23%|▏| 114/497 [02:17<07:40,  1.20s/it, loss=0.118, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  23%|▏| 114/497 [02:17<07:40,  1.20s/it, loss=0.105, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  23%|▏| 115/497 [02:18<07:39,  1.20s/it, loss=0.105, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  23%|▏| 115/497 [02:18<07:39,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  23%|▏| 116/497 [02:19<07:38,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  23%|▏| 116/497 [02:19<07:38,  1.20s/it, loss=0.102, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  24%|▏| 117/497 [02:20<07:37,  1.20s/it, loss=0.102, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  24%|▏| 117/497 [02:20<07:37,  1.20s/it, loss=0.0985, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  24%|▏| 118/497 [02:22<07:36,  1.20s/it, loss=0.0985, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  24%|▏| 118/497 [02:22<07:36,  1.20s/it, loss=0.103, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  24%|▏| 119/497 [02:23<07:34,  1.20s/it, loss=0.103, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  24%|▏| 119/497 [02:23<07:34,  1.20s/it, loss=0.104, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  24%|▏| 120/497 [02:24<07:33,  1.20s/it, loss=0.104, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  24%|▏| 120/497 [02:24<07:33,  1.20s/it, loss=0.103, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  24%|▏| 121/497 [02:25<07:32,  1.20s/it, loss=0.103, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  24%|▏| 121/497 [02:25<07:32,  1.20s/it, loss=0.108, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  25%|▏| 122/497 [02:26<07:31,  1.20s/it, loss=0.108, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  25%|▏| 122/497 [02:26<07:31,  1.20s/it, loss=0.112, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  25%|▏| 123/497 [02:28<07:30,  1.20s/it, loss=0.112, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  25%|▏| 123/497 [02:28<07:30,  1.20s/it, loss=0.119, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  25%|▏| 124/497 [02:29<07:28,  1.20s/it, loss=0.119, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  25%|▏| 124/497 [02:29<07:28,  1.20s/it, loss=0.122, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  25%|▎| 125/497 [02:30<07:27,  1.20s/it, loss=0.122, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  25%|▎| 125/497 [02:30<07:27,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  25%|▎| 126/497 [02:31<07:26,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  25%|▎| 126/497 [02:31<07:26,  1.20s/it, loss=0.122, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  26%|▎| 127/497 [02:32<07:25,  1.20s/it, loss=0.122, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  26%|▎| 127/497 [02:32<07:25,  1.20s/it, loss=0.126, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  26%|▎| 128/497 [02:34<07:24,  1.20s/it, loss=0.126, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  26%|▎| 128/497 [02:34<07:24,  1.20s/it, loss=0.123, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  26%|▎| 129/497 [02:35<07:22,  1.20s/it, loss=0.123, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  26%|▎| 129/497 [02:35<07:22,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  26%|▎| 130/497 [02:36<07:21,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  26%|▎| 130/497 [02:36<07:21,  1.20s/it, loss=0.122, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  26%|▎| 131/497 [02:37<07:20,  1.20s/it, loss=0.122, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  26%|▎| 131/497 [02:37<07:20,  1.20s/it, loss=0.115, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  27%|▎| 132/497 [02:38<07:19,  1.20s/it, loss=0.115, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  27%|▎| 132/497 [02:38<07:19,  1.20s/it, loss=0.11, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  27%|▎| 133/497 [02:40<07:18,  1.20s/it, loss=0.11, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  27%|▎| 133/497 [02:40<07:18,  1.20s/it, loss=0.11, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  27%|▎| 134/497 [02:41<07:16,  1.20s/it, loss=0.11, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  27%|▎| 134/497 [02:41<07:16,  1.20s/it, loss=0.106, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  27%|▎| 135/497 [02:42<07:15,  1.20s/it, loss=0.106, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  27%|▎| 135/497 [02:42<07:15,  1.20s/it, loss=0.106, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  27%|▎| 136/497 [02:43<07:14,  1.20s/it, loss=0.106, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  27%|▎| 136/497 [02:43<07:14,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  28%|▎| 137/497 [02:44<07:13,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  28%|▎| 137/497 [02:44<07:13,  1.20s/it, loss=0.105, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  28%|▎| 138/497 [02:46<07:12,  1.20s/it, loss=0.105, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  28%|▎| 138/497 [02:46<07:12,  1.20s/it, loss=0.103, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  28%|▎| 139/497 [02:47<07:10,  1.20s/it, loss=0.103, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  28%|▎| 139/497 [02:47<07:10,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  28%|▎| 140/497 [02:48<07:09,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  28%|▎| 140/497 [02:48<07:09,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  28%|▎| 141/497 [02:49<07:08,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  28%|▎| 141/497 [02:49<07:08,  1.20s/it, loss=0.0992, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  29%|▎| 142/497 [02:50<07:07,  1.20s/it, loss=0.0992, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  29%|▎| 142/497 [02:50<07:07,  1.20s/it, loss=0.095, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  29%|▎| 143/497 [02:52<07:06,  1.20s/it, loss=0.095, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  29%|▎| 143/497 [02:52<07:06,  1.20s/it, loss=0.086, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  29%|▎| 144/497 [02:53<07:04,  1.20s/it, loss=0.086, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  29%|▎| 144/497 [02:53<07:04,  1.20s/it, loss=0.0851, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  29%|▎| 145/497 [02:54<07:03,  1.20s/it, loss=0.0851, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  29%|▎| 145/497 [02:54<07:03,  1.20s/it, loss=0.0848, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  29%|▎| 146/497 [02:55<07:02,  1.20s/it, loss=0.0848, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  29%|▎| 146/497 [02:55<07:02,  1.20s/it, loss=0.0869, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  30%|▎| 147/497 [02:56<07:01,  1.20s/it, loss=0.0869, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  30%|▎| 147/497 [02:56<07:01,  1.20s/it, loss=0.0825, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  30%|▎| 148/497 [02:58<07:00,  1.20s/it, loss=0.0825, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  30%|▎| 148/497 [02:58<07:00,  1.20s/it, loss=0.0818, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  30%|▎| 149/497 [02:59<06:58,  1.20s/it, loss=0.0818, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  30%|▎| 149/497 [02:59<06:58,  1.20s/it, loss=0.0849, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  30%|▎| 150/497 [03:00<06:57,  1.20s/it, loss=0.0849, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  30%|▎| 150/497 [03:00<06:57,  1.20s/it, loss=0.0856, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  30%|▎| 151/497 [03:01<06:56,  1.20s/it, loss=0.0856, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  30%|▎| 151/497 [03:01<06:56,  1.20s/it, loss=0.0816, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  31%|▎| 152/497 [03:02<06:55,  1.20s/it, loss=0.0816, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  31%|▎| 152/497 [03:02<06:55,  1.20s/it, loss=0.0854, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  31%|▎| 153/497 [03:04<06:54,  1.20s/it, loss=0.0854, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  31%|▎| 153/497 [03:04<06:54,  1.20s/it, loss=0.0847, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  31%|▎| 154/497 [03:05<06:52,  1.20s/it, loss=0.0847, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  31%|▎| 154/497 [03:05<06:52,  1.20s/it, loss=0.0868, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  31%|▎| 155/497 [03:06<06:51,  1.20s/it, loss=0.0868, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  31%|▎| 155/497 [03:06<06:51,  1.20s/it, loss=0.0848, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  31%|▎| 156/497 [03:07<06:50,  1.20s/it, loss=0.0848, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  31%|▎| 156/497 [03:07<06:50,  1.20s/it, loss=0.0866, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  32%|▎| 157/497 [03:08<06:49,  1.20s/it, loss=0.0866, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  32%|▎| 157/497 [03:08<06:49,  1.20s/it, loss=0.0818, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  32%|▎| 158/497 [03:10<06:48,  1.20s/it, loss=0.0818, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  32%|▎| 158/497 [03:10<06:48,  1.20s/it, loss=0.0807, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  32%|▎| 159/497 [03:11<06:46,  1.20s/it, loss=0.0807, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  32%|▎| 159/497 [03:11<06:46,  1.20s/it, loss=0.0807, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  32%|▎| 160/497 [03:12<06:45,  1.20s/it, loss=0.0807, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  32%|▎| 160/497 [03:12<06:45,  1.20s/it, loss=0.0804, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  32%|▎| 161/497 [03:13<06:44,  1.20s/it, loss=0.0804, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  32%|▎| 161/497 [03:13<06:44,  1.20s/it, loss=0.09, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  33%|▎| 162/497 [03:14<06:43,  1.20s/it, loss=0.09, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  33%|▎| 162/497 [03:14<06:43,  1.20s/it, loss=0.0947, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  33%|▎| 163/497 [03:16<06:41,  1.20s/it, loss=0.0947, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  33%|▎| 163/497 [03:16<06:41,  1.20s/it, loss=0.0943, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  33%|▎| 164/497 [03:17<06:40,  1.20s/it, loss=0.0943, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  33%|▎| 164/497 [03:17<06:40,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  33%|▎| 165/497 [03:18<06:39,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  33%|▎| 165/497 [03:18<06:39,  1.20s/it, loss=0.0997, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  33%|▎| 166/497 [03:19<06:38,  1.20s/it, loss=0.0997, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  33%|▎| 166/497 [03:19<06:38,  1.20s/it, loss=0.109, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  34%|▎| 167/497 [03:20<06:37,  1.20s/it, loss=0.109, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  34%|▎| 167/497 [03:20<06:37,  1.20s/it, loss=0.114, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  34%|▎| 168/497 [03:22<06:35,  1.20s/it, loss=0.114, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  34%|▎| 168/497 [03:22<06:35,  1.20s/it, loss=0.115, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  34%|▎| 169/497 [03:23<06:34,  1.20s/it, loss=0.115, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  34%|▎| 169/497 [03:23<06:34,  1.20s/it, loss=0.12, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  34%|▎| 170/497 [03:24<06:33,  1.20s/it, loss=0.12, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  34%|▎| 170/497 [03:24<06:33,  1.20s/it, loss=0.122, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  34%|▎| 171/497 [03:25<06:32,  1.20s/it, loss=0.122, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  34%|▎| 171/497 [03:25<06:32,  1.20s/it, loss=0.134, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  35%|▎| 172/497 [03:27<06:31,  1.20s/it, loss=0.134, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  35%|▎| 172/497 [03:27<06:31,  1.20s/it, loss=0.135, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  35%|▎| 173/497 [03:28<06:29,  1.20s/it, loss=0.135, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  35%|▎| 173/497 [03:28<06:29,  1.20s/it, loss=0.134, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  35%|▎| 174/497 [03:29<06:28,  1.20s/it, loss=0.134, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  35%|▎| 174/497 [03:29<06:28,  1.20s/it, loss=0.135, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  35%|▎| 175/497 [03:30<06:27,  1.20s/it, loss=0.135, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  35%|▎| 175/497 [03:30<06:27,  1.20s/it, loss=0.138, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  35%|▎| 176/497 [03:31<06:26,  1.20s/it, loss=0.138, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  35%|▎| 176/497 [03:31<06:26,  1.20s/it, loss=0.139, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  36%|▎| 177/497 [03:33<06:25,  1.20s/it, loss=0.139, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  36%|▎| 177/497 [03:33<06:25,  1.20s/it, loss=0.14, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  36%|▎| 178/497 [03:34<06:23,  1.20s/it, loss=0.14, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  36%|▎| 178/497 [03:34<06:23,  1.20s/it, loss=0.137, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  36%|▎| 179/497 [03:35<06:22,  1.20s/it, loss=0.137, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  36%|▎| 179/497 [03:35<06:22,  1.20s/it, loss=0.146, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  36%|▎| 180/497 [03:36<06:21,  1.20s/it, loss=0.146, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  36%|▎| 180/497 [03:36<06:21,  1.20s/it, loss=0.146, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  36%|▎| 181/497 [03:37<06:20,  1.20s/it, loss=0.146, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  36%|▎| 181/497 [03:37<06:20,  1.20s/it, loss=0.132, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  37%|▎| 182/497 [03:39<06:19,  1.20s/it, loss=0.132, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  37%|▎| 182/497 [03:39<06:19,  1.20s/it, loss=0.135, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  37%|▎| 183/497 [03:40<06:17,  1.20s/it, loss=0.135, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  37%|▎| 183/497 [03:40<06:17,  1.20s/it, loss=0.141, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  37%|▎| 184/497 [03:41<06:16,  1.20s/it, loss=0.141, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  37%|▎| 184/497 [03:41<06:16,  1.20s/it, loss=0.14, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  37%|▎| 185/497 [03:42<06:15,  1.20s/it, loss=0.14, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  37%|▎| 185/497 [03:42<06:15,  1.20s/it, loss=0.142, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  37%|▎| 186/497 [03:43<06:14,  1.20s/it, loss=0.142, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  37%|▎| 186/497 [03:43<06:14,  1.20s/it, loss=0.133, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  38%|▍| 187/497 [03:45<06:13,  1.20s/it, loss=0.133, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  38%|▍| 187/497 [03:45<06:13,  1.20s/it, loss=0.128, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  38%|▍| 188/497 [03:46<06:11,  1.20s/it, loss=0.128, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  38%|▍| 188/497 [03:46<06:11,  1.20s/it, loss=0.136, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  38%|▍| 189/497 [03:47<06:10,  1.20s/it, loss=0.136, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  38%|▍| 189/497 [03:47<06:10,  1.20s/it, loss=0.132, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  38%|▍| 190/497 [03:48<06:09,  1.20s/it, loss=0.132, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  38%|▍| 190/497 [03:48<06:09,  1.20s/it, loss=0.127, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  38%|▍| 191/497 [03:49<06:08,  1.20s/it, loss=0.127, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  38%|▍| 191/497 [03:49<06:08,  1.20s/it, loss=0.117, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  39%|▍| 192/497 [03:51<06:07,  1.20s/it, loss=0.117, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  39%|▍| 192/497 [03:51<06:07,  1.20s/it, loss=0.112, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  39%|▍| 193/497 [03:52<06:05,  1.20s/it, loss=0.112, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  39%|▍| 193/497 [03:52<06:05,  1.20s/it, loss=0.11, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  39%|▍| 194/497 [03:53<06:04,  1.20s/it, loss=0.11, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  39%|▍| 194/497 [03:53<06:04,  1.20s/it, loss=0.11, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  39%|▍| 195/497 [03:54<06:03,  1.20s/it, loss=0.11, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  39%|▍| 195/497 [03:54<06:03,  1.20s/it, loss=0.106, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  39%|▍| 196/497 [03:55<06:02,  1.20s/it, loss=0.106, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  39%|▍| 196/497 [03:55<06:02,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  40%|▍| 197/497 [03:57<06:01,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  40%|▍| 197/497 [03:57<06:01,  1.20s/it, loss=0.123, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  40%|▍| 198/497 [03:58<05:59,  1.20s/it, loss=0.123, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  40%|▍| 198/497 [03:58<05:59,  1.20s/it, loss=0.123, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  40%|▍| 199/497 [03:59<05:58,  1.20s/it, loss=0.123, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  40%|▍| 199/497 [03:59<05:58,  1.20s/it, loss=0.117, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  40%|▍| 200/497 [04:00<05:57,  1.20s/it, loss=0.117, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  40%|▍| 200/497 [04:00<05:57,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  40%|▍| 201/497 [04:01<05:56,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  40%|▍| 201/497 [04:01<05:56,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  41%|▍| 202/497 [04:03<05:55,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  41%|▍| 202/497 [04:03<05:55,  1.20s/it, loss=0.117, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  41%|▍| 203/497 [04:04<05:53,  1.20s/it, loss=0.117, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  41%|▍| 203/497 [04:04<05:53,  1.20s/it, loss=0.112, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  41%|▍| 204/497 [04:05<05:52,  1.20s/it, loss=0.112, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  41%|▍| 204/497 [04:05<05:52,  1.20s/it, loss=0.104, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  41%|▍| 205/497 [04:06<05:51,  1.20s/it, loss=0.104, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  41%|▍| 205/497 [04:06<05:51,  1.20s/it, loss=0.103, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  41%|▍| 206/497 [04:07<05:50,  1.20s/it, loss=0.103, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  41%|▍| 206/497 [04:07<05:50,  1.20s/it, loss=0.105, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  42%|▍| 207/497 [04:09<05:48,  1.20s/it, loss=0.105, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  42%|▍| 207/497 [04:09<05:48,  1.20s/it, loss=0.105, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  42%|▍| 208/497 [04:10<05:47,  1.20s/it, loss=0.105, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  42%|▍| 208/497 [04:10<05:47,  1.20s/it, loss=0.0951, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  42%|▍| 209/497 [04:11<05:46,  1.20s/it, loss=0.0951, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  42%|▍| 209/497 [04:11<05:46,  1.20s/it, loss=0.0893, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  42%|▍| 210/497 [04:12<05:45,  1.20s/it, loss=0.0893, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  42%|▍| 210/497 [04:12<05:45,  1.20s/it, loss=0.0948, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  42%|▍| 211/497 [04:13<05:44,  1.20s/it, loss=0.0948, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  42%|▍| 211/497 [04:13<05:44,  1.20s/it, loss=0.0914, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  43%|▍| 212/497 [04:15<05:42,  1.20s/it, loss=0.0914, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  43%|▍| 212/497 [04:15<05:42,  1.20s/it, loss=0.0973, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  43%|▍| 213/497 [04:16<05:41,  1.20s/it, loss=0.0973, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  43%|▍| 213/497 [04:16<05:41,  1.20s/it, loss=0.104, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  43%|▍| 214/497 [04:17<05:40,  1.20s/it, loss=0.104, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  43%|▍| 214/497 [04:17<05:40,  1.20s/it, loss=0.111, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  43%|▍| 215/497 [04:18<05:39,  1.20s/it, loss=0.111, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  43%|▍| 215/497 [04:18<05:39,  1.20s/it, loss=0.118, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  43%|▍| 216/497 [04:19<05:38,  1.20s/it, loss=0.118, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  43%|▍| 216/497 [04:19<05:38,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  44%|▍| 217/497 [04:21<05:36,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  44%|▍| 217/497 [04:21<05:36,  1.20s/it, loss=0.1, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:  44%|▍| 218/497 [04:22<05:35,  1.20s/it, loss=0.1, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidaEpoch 1:  44%|▍| 218/497 [04:22<05:35,  1.20s/it, loss=0.105, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  44%|▍| 219/497 [04:23<05:34,  1.20s/it, loss=0.105, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  44%|▍| 219/497 [04:23<05:34,  1.20s/it, loss=0.104, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  44%|▍| 220/497 [04:24<05:33,  1.20s/it, loss=0.104, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  44%|▍| 220/497 [04:24<05:33,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  44%|▍| 221/497 [04:25<05:32,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  44%|▍| 221/497 [04:25<05:32,  1.20s/it, loss=0.119, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  45%|▍| 222/497 [04:27<05:30,  1.20s/it, loss=0.119, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  45%|▍| 222/497 [04:27<05:30,  1.20s/it, loss=0.116, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  45%|▍| 223/497 [04:28<05:29,  1.20s/it, loss=0.116, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  45%|▍| 223/497 [04:28<05:29,  1.20s/it, loss=0.117, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  45%|▍| 224/497 [04:29<05:28,  1.20s/it, loss=0.117, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  45%|▍| 224/497 [04:29<05:28,  1.20s/it, loss=0.119, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  45%|▍| 225/497 [04:30<05:27,  1.20s/it, loss=0.119, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  45%|▍| 225/497 [04:30<05:27,  1.20s/it, loss=0.123, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  45%|▍| 226/497 [04:31<05:26,  1.20s/it, loss=0.123, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  45%|▍| 226/497 [04:31<05:26,  1.20s/it, loss=0.118, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  46%|▍| 227/497 [04:33<05:24,  1.20s/it, loss=0.118, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  46%|▍| 227/497 [04:33<05:24,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  46%|▍| 228/497 [04:34<05:23,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  46%|▍| 228/497 [04:34<05:23,  1.20s/it, loss=0.122, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  46%|▍| 229/497 [04:35<05:22,  1.20s/it, loss=0.122, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  46%|▍| 229/497 [04:35<05:22,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  46%|▍| 230/497 [04:36<05:21,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  46%|▍| 230/497 [04:36<05:21,  1.20s/it, loss=0.118, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  46%|▍| 231/497 [04:37<05:20,  1.20s/it, loss=0.118, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  46%|▍| 231/497 [04:37<05:20,  1.20s/it, loss=0.118, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  47%|▍| 232/497 [04:39<05:18,  1.20s/it, loss=0.118, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  47%|▍| 232/497 [04:39<05:18,  1.20s/it, loss=0.11, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  47%|▍| 233/497 [04:40<05:17,  1.20s/it, loss=0.11, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  47%|▍| 233/497 [04:40<05:17,  1.20s/it, loss=0.102, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  47%|▍| 234/497 [04:41<05:16,  1.20s/it, loss=0.102, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  47%|▍| 234/497 [04:41<05:16,  1.20s/it, loss=0.0907, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  47%|▍| 235/497 [04:42<05:15,  1.20s/it, loss=0.0907, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  47%|▍| 235/497 [04:42<05:15,  1.20s/it, loss=0.0855, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  47%|▍| 236/497 [04:44<05:14,  1.20s/it, loss=0.0855, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  47%|▍| 236/497 [04:44<05:14,  1.20s/it, loss=0.0905, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  48%|▍| 237/497 [04:45<05:12,  1.20s/it, loss=0.0905, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  48%|▍| 237/497 [04:45<05:12,  1.20s/it, loss=0.0881, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  48%|▍| 238/497 [04:46<05:11,  1.20s/it, loss=0.0881, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  48%|▍| 238/497 [04:46<05:11,  1.20s/it, loss=0.0856, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  48%|▍| 239/497 [04:47<05:10,  1.20s/it, loss=0.0856, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  48%|▍| 239/497 [04:47<05:10,  1.20s/it, loss=0.0931, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  48%|▍| 240/497 [04:48<05:09,  1.20s/it, loss=0.0931, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  48%|▍| 240/497 [04:48<05:09,  1.20s/it, loss=0.0941, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  48%|▍| 241/497 [04:50<05:08,  1.20s/it, loss=0.0941, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  48%|▍| 241/497 [04:50<05:08,  1.20s/it, loss=0.0757, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  49%|▍| 242/497 [04:51<05:06,  1.20s/it, loss=0.0757, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  49%|▍| 242/497 [04:51<05:06,  1.20s/it, loss=0.0867, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  49%|▍| 243/497 [04:52<05:05,  1.20s/it, loss=0.0867, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  49%|▍| 243/497 [04:52<05:05,  1.20s/it, loss=0.0938, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  49%|▍| 244/497 [04:53<05:04,  1.20s/it, loss=0.0938, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  49%|▍| 244/497 [04:53<05:04,  1.20s/it, loss=0.0979, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  49%|▍| 245/497 [04:54<05:03,  1.20s/it, loss=0.0979, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  49%|▍| 245/497 [04:54<05:03,  1.20s/it, loss=0.0957, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  49%|▍| 246/497 [04:56<05:02,  1.20s/it, loss=0.0957, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  49%|▍| 246/497 [04:56<05:02,  1.20s/it, loss=0.0995, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  50%|▍| 247/497 [04:57<05:00,  1.20s/it, loss=0.0995, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValEpoch 1:  50%|▍| 247/497 [04:57<05:00,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  50%|▍| 248/497 [04:58<04:59,  1.20s/it, loss=0.101, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  50%|▍| 248/497 [04:58<04:59,  1.20s/it, loss=0.104, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  50%|▌| 249/497 [04:59<04:58,  1.20s/it, loss=0.104, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  50%|▌| 249/497 [04:59<04:58,  1.20s/it, loss=0.108, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  50%|▌| 250/497 [05:00<04:57,  1.20s/it, loss=0.108, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  50%|▌| 250/497 [05:00<04:57,  1.20s/it, loss=0.113, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  51%|▌| 251/497 [05:02<04:56,  1.20s/it, loss=0.113, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  51%|▌| 251/497 [05:02<04:56,  1.20s/it, loss=0.114, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  51%|▌| 252/497 [05:03<04:54,  1.20s/it, loss=0.114, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  51%|▌| 252/497 [05:03<04:54,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  51%|▌| 253/497 [05:04<04:53,  1.20s/it, loss=0.121, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  51%|▌| 253/497 [05:04<04:53,  1.20s/it, loss=0.123, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  51%|▌| 254/497 [05:05<04:52,  1.20s/it, loss=0.123, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  51%|▌| 254/497 [05:05<04:52,  1.20s/it, loss=0.131, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  51%|▌| 255/497 [05:06<04:51,  1.20s/it, loss=0.131, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  51%|▌| 255/497 [05:06<04:51,  1.20s/it, loss=0.144, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  52%|▌| 256/497 [05:08<04:50,  1.20s/it, loss=0.144, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  52%|▌| 256/497 [05:08<04:50,  1.20s/it, loss=0.189, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  52%|▌| 257/497 [05:09<04:48,  1.20s/it, loss=0.189, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  52%|▌| 257/497 [05:09<04:48,  1.20s/it, loss=0.193, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  52%|▌| 258/497 [05:10<04:47,  1.20s/it, loss=0.193, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  52%|▌| 258/497 [05:10<04:47,  1.20s/it, loss=0.193, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  52%|▌| 259/497 [05:11<04:46,  1.20s/it, loss=0.193, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  52%|▌| 259/497 [05:11<04:46,  1.20s/it, loss=0.191, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  52%|▌| 260/497 [05:12<04:45,  1.20s/it, loss=0.191, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  52%|▌| 260/497 [05:12<04:45,  1.20s/it, loss=0.196, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  53%|▌| 261/497 [05:14<04:44,  1.20s/it, loss=0.196, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  53%|▌| 261/497 [05:14<04:44,  1.20s/it, loss=0.202, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  53%|▌| 262/497 [05:15<04:42,  1.20s/it, loss=0.202, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  53%|▌| 262/497 [05:15<04:42,  1.20s/it, loss=0.196, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  53%|▌| 263/497 [05:16<04:41,  1.20s/it, loss=0.196, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  53%|▌| 263/497 [05:16<04:41,  1.20s/it, loss=0.193, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  53%|▌| 264/497 [05:17<04:40,  1.20s/it, loss=0.193, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  53%|▌| 264/497 [05:17<04:40,  1.20s/it, loss=0.192, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  53%|▌| 265/497 [05:18<04:39,  1.20s/it, loss=0.192, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  53%|▌| 265/497 [05:18<04:39,  1.20s/it, loss=0.195, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  54%|▌| 266/497 [05:20<04:38,  1.20s/it, loss=0.195, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  54%|▌| 266/497 [05:20<04:38,  1.20s/it, loss=0.194, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  54%|▌| 267/497 [05:21<04:36,  1.20s/it, loss=0.194, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  54%|▌| 267/497 [05:21<04:36,  1.20s/it, loss=0.192, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  54%|▌| 268/497 [05:22<04:35,  1.20s/it, loss=0.192, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  54%|▌| 268/497 [05:22<04:35,  1.20s/it, loss=0.195, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  54%|▌| 269/497 [05:23<04:34,  1.20s/it, loss=0.195, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  54%|▌| 269/497 [05:23<04:34,  1.20s/it, loss=0.193, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  54%|▌| 270/497 [05:24<04:33,  1.20s/it, loss=0.193, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  54%|▌| 270/497 [05:24<04:33,  1.20s/it, loss=0.185, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  55%|▌| 271/497 [05:26<04:31,  1.20s/it, loss=0.185, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  55%|▌| 271/497 [05:26<04:31,  1.20s/it, loss=0.191, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  55%|▌| 272/497 [05:27<04:30,  1.20s/it, loss=0.191, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  55%|▌| 272/497 [05:27<04:30,  1.20s/it, loss=0.187, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  55%|▌| 273/497 [05:28<04:29,  1.20s/it, loss=0.187, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  55%|▌| 273/497 [05:28<04:29,  1.20s/it, loss=0.187, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  55%|▌| 274/497 [05:29<04:28,  1.20s/it, loss=0.187, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  55%|▌| 274/497 [05:29<04:28,  1.20s/it, loss=0.181, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  55%|▌| 275/497 [05:30<04:27,  1.20s/it, loss=0.181, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  55%|▌| 275/497 [05:30<04:27,  1.20s/it, loss=0.168, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  56%|▌| 276/497 [05:32<04:25,  1.20s/it, loss=0.168, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  56%|▌| 276/497 [05:32<04:25,  1.20s/it, loss=0.123, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  56%|▌| 277/497 [05:33<04:24,  1.20s/it, loss=0.123, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  56%|▌| 277/497 [05:33<04:24,  1.20s/it, loss=0.127, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  56%|▌| 278/497 [05:34<04:23,  1.20s/it, loss=0.127, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  56%|▌| 278/497 [05:34<04:23,  1.20s/it, loss=0.136, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  56%|▌| 279/497 [05:35<04:22,  1.20s/it, loss=0.136, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  56%|▌| 279/497 [05:35<04:22,  1.20s/it, loss=0.141, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  56%|▌| 280/497 [05:36<04:21,  1.20s/it, loss=0.141, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  56%|▌| 280/497 [05:36<04:21,  1.20s/it, loss=0.136, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  57%|▌| 281/497 [05:38<04:19,  1.20s/it, loss=0.136, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  57%|▌| 281/497 [05:38<04:19,  1.20s/it, loss=0.13, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  57%|▌| 282/497 [05:39<04:18,  1.20s/it, loss=0.13, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValidEpoch 1:  57%|▌| 282/497 [05:39<04:18,  1.20s/it, loss=0.135, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  57%|▌| 283/497 [05:40<04:17,  1.20s/it, loss=0.135, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, ValiEpoch 1:  57%|▌| 283/497 [05:40<04:17,  1.20s/it, loss=0.129, v_num=75, Validation Loss=0.131, Validation IoU=nan.0, Validation Dice=0.941, Validation F1 score=nan.0, Vali